{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50559d19-ab40-4880-bae7-44a3dbe180a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Others\n",
    "import random\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "import optuna\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ff5d38-60c9-476d-a509-9e1419bea2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing packages\n",
    "#Sklearn\n",
    "from sklearn import model_selection, linear_model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import RFECV,SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,RepeatedStratifiedKFold,cross_validate\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,auc,roc_auc_score,roc_curve,classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6baca9e-4a5b-4e5f-9cd3-2db48c543dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bc75945b-ce4f-4650-8380-f7bfb5ff1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\A\\\\Desktop\\\\Paper_0_Hydrogel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12865361-291e-449c-8ca4-53ed95b92f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71, 4175)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sv</th>\n",
       "      <th>Se</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mv</th>\n",
       "      <th>Me</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>...</th>\n",
       "      <th>s1_numAroBonds</th>\n",
       "      <th>s2_numAroBonds</th>\n",
       "      <th>s3_numAroBonds</th>\n",
       "      <th>s4_numAroBonds</th>\n",
       "      <th>s34_size</th>\n",
       "      <th>s34_relSize</th>\n",
       "      <th>s34_phSize</th>\n",
       "      <th>s34_phRelSize</th>\n",
       "      <th>chiralMoment</th>\n",
       "      <th>chiralPhMoment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ma_2019_A</th>\n",
       "      <td>267.28</td>\n",
       "      <td>8.352500</td>\n",
       "      <td>20.0734</td>\n",
       "      <td>33.3526</td>\n",
       "      <td>19.8921</td>\n",
       "      <td>36.9899</td>\n",
       "      <td>0.627294</td>\n",
       "      <td>1.042269</td>\n",
       "      <td>0.621628</td>\n",
       "      <td>1.155934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>17.966397</td>\n",
       "      <td>8.618182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_U</th>\n",
       "      <td>244.23</td>\n",
       "      <td>8.421724</td>\n",
       "      <td>17.9656</td>\n",
       "      <td>30.5854</td>\n",
       "      <td>17.5454</td>\n",
       "      <td>33.3290</td>\n",
       "      <td>0.619503</td>\n",
       "      <td>1.054669</td>\n",
       "      <td>0.605014</td>\n",
       "      <td>1.149276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>14.808251</td>\n",
       "      <td>7.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_C</th>\n",
       "      <td>243.25</td>\n",
       "      <td>8.108333</td>\n",
       "      <td>18.2722</td>\n",
       "      <td>31.3599</td>\n",
       "      <td>18.0966</td>\n",
       "      <td>34.6179</td>\n",
       "      <td>0.609073</td>\n",
       "      <td>1.045330</td>\n",
       "      <td>0.603220</td>\n",
       "      <td>1.153930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>14.808251</td>\n",
       "      <td>7.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_G</th>\n",
       "      <td>283.28</td>\n",
       "      <td>8.584242</td>\n",
       "      <td>20.7882</td>\n",
       "      <td>34.6799</td>\n",
       "      <td>20.3466</td>\n",
       "      <td>38.1993</td>\n",
       "      <td>0.629945</td>\n",
       "      <td>1.050906</td>\n",
       "      <td>0.616564</td>\n",
       "      <td>1.157555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>19.586399</td>\n",
       "      <td>10.257197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_dA</th>\n",
       "      <td>251.28</td>\n",
       "      <td>8.105806</td>\n",
       "      <td>19.3586</td>\n",
       "      <td>32.0253</td>\n",
       "      <td>19.4376</td>\n",
       "      <td>35.7805</td>\n",
       "      <td>0.624471</td>\n",
       "      <td>1.033074</td>\n",
       "      <td>0.627019</td>\n",
       "      <td>1.154210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>17.259745</td>\n",
       "      <td>8.115820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                MW       AMW       Sv       Se       Sp       Si        Mv  \\\n",
       "ID                                                                           \n",
       "Ma_2019_A   267.28  8.352500  20.0734  33.3526  19.8921  36.9899  0.627294   \n",
       "Ma_2019_U   244.23  8.421724  17.9656  30.5854  17.5454  33.3290  0.619503   \n",
       "Ma_2019_C   243.25  8.108333  18.2722  31.3599  18.0966  34.6179  0.609073   \n",
       "Ma_2019_G   283.28  8.584242  20.7882  34.6799  20.3466  38.1993  0.629945   \n",
       "Ma_2019_dA  251.28  8.105806  19.3586  32.0253  19.4376  35.7805  0.624471   \n",
       "\n",
       "                  Me        Mp        Mi  ...  s1_numAroBonds  s2_numAroBonds  \\\n",
       "ID                                        ...                                   \n",
       "Ma_2019_A   1.042269  0.621628  1.155934  ...             0.0             0.0   \n",
       "Ma_2019_U   1.054669  0.605014  1.149276  ...             0.0             0.0   \n",
       "Ma_2019_C   1.045330  0.603220  1.153930  ...             0.0             0.0   \n",
       "Ma_2019_G   1.050906  0.616564  1.157555  ...             0.0             0.0   \n",
       "Ma_2019_dA  1.033074  0.627019  1.154210  ...             0.0             0.0   \n",
       "\n",
       "            s3_numAroBonds  s4_numAroBonds  s34_size  s34_relSize  s34_phSize  \\\n",
       "ID                                                                              \n",
       "Ma_2019_A              0.0            10.0      16.0     0.842105        7.75   \n",
       "Ma_2019_U              0.0             0.0      14.0     0.823529        6.75   \n",
       "Ma_2019_C              0.0             0.0      14.0     0.823529        6.75   \n",
       "Ma_2019_G              0.0             5.0      17.0     0.850000        8.75   \n",
       "Ma_2019_dA             0.0            10.0      15.0     0.833333        7.00   \n",
       "\n",
       "            s34_phRelSize  chiralMoment  chiralPhMoment  \n",
       "ID                                                       \n",
       "Ma_2019_A        0.407895     17.966397        8.618182  \n",
       "Ma_2019_U        0.397059     14.808251        7.026700  \n",
       "Ma_2019_C        0.397059     14.808251        7.026700  \n",
       "Ma_2019_G        0.437500     19.586399       10.257197  \n",
       "Ma_2019_dA       0.388889     17.259745        8.115820  \n",
       "\n",
       "[5 rows x 4175 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the data\n",
    "ML_data= pd.read_csv(\"./Original data/ML_data.csv\",header=0,index_col=0)\n",
    "X_NAomit_data= pd.read_csv(\"./Original data/X_NAomit_data.csv\",header=0,index_col=0)\n",
    "Raw_data = pd.read_csv('./Original data/raw_data.csv',index_col=0)\n",
    "Raw_data['Hydrogel-forming ability']=np.where(Raw_data['Hydrogel-forming ability']=='Gelator', 1, 0)\n",
    "#original data(descriptors= 4175）\n",
    "print(X_NAomit_data.shape)\n",
    "X_NAomit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a68cc910-dca6-4b63-ae90-8fac100e1ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sv</th>\n",
       "      <th>Se</th>\n",
       "      <th>Sp</th>\n",
       "      <th>Si</th>\n",
       "      <th>Mv</th>\n",
       "      <th>Me</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>...</th>\n",
       "      <th>s1_numAroBonds</th>\n",
       "      <th>s2_numAroBonds</th>\n",
       "      <th>s3_numAroBonds</th>\n",
       "      <th>s4_numAroBonds</th>\n",
       "      <th>s34_size</th>\n",
       "      <th>s34_relSize</th>\n",
       "      <th>s34_phSize</th>\n",
       "      <th>s34_phRelSize</th>\n",
       "      <th>chiralMoment</th>\n",
       "      <th>chiralPhMoment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ma_2019_A</th>\n",
       "      <td>267.28</td>\n",
       "      <td>8.352500</td>\n",
       "      <td>20.0734</td>\n",
       "      <td>33.3526</td>\n",
       "      <td>19.8921</td>\n",
       "      <td>36.9899</td>\n",
       "      <td>0.627294</td>\n",
       "      <td>1.042269</td>\n",
       "      <td>0.621628</td>\n",
       "      <td>1.155934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>17.966397</td>\n",
       "      <td>8.618182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_U</th>\n",
       "      <td>244.23</td>\n",
       "      <td>8.421724</td>\n",
       "      <td>17.9656</td>\n",
       "      <td>30.5854</td>\n",
       "      <td>17.5454</td>\n",
       "      <td>33.3290</td>\n",
       "      <td>0.619503</td>\n",
       "      <td>1.054669</td>\n",
       "      <td>0.605014</td>\n",
       "      <td>1.149276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>14.808251</td>\n",
       "      <td>7.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_C</th>\n",
       "      <td>243.25</td>\n",
       "      <td>8.108333</td>\n",
       "      <td>18.2722</td>\n",
       "      <td>31.3599</td>\n",
       "      <td>18.0966</td>\n",
       "      <td>34.6179</td>\n",
       "      <td>0.609073</td>\n",
       "      <td>1.045330</td>\n",
       "      <td>0.603220</td>\n",
       "      <td>1.153930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>14.808251</td>\n",
       "      <td>7.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_G</th>\n",
       "      <td>283.28</td>\n",
       "      <td>8.584242</td>\n",
       "      <td>20.7882</td>\n",
       "      <td>34.6799</td>\n",
       "      <td>20.3466</td>\n",
       "      <td>38.1993</td>\n",
       "      <td>0.629945</td>\n",
       "      <td>1.050906</td>\n",
       "      <td>0.616564</td>\n",
       "      <td>1.157555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>19.586399</td>\n",
       "      <td>10.257197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_dA</th>\n",
       "      <td>251.28</td>\n",
       "      <td>8.105806</td>\n",
       "      <td>19.3586</td>\n",
       "      <td>32.0253</td>\n",
       "      <td>19.4376</td>\n",
       "      <td>35.7805</td>\n",
       "      <td>0.624471</td>\n",
       "      <td>1.033074</td>\n",
       "      <td>0.627019</td>\n",
       "      <td>1.154210</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>17.259745</td>\n",
       "      <td>8.115820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_ArabinoC</th>\n",
       "      <td>243.25</td>\n",
       "      <td>8.108333</td>\n",
       "      <td>18.2722</td>\n",
       "      <td>31.3599</td>\n",
       "      <td>18.0966</td>\n",
       "      <td>34.6179</td>\n",
       "      <td>0.609073</td>\n",
       "      <td>1.045330</td>\n",
       "      <td>0.603220</td>\n",
       "      <td>1.153930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>6.75</td>\n",
       "      <td>0.397059</td>\n",
       "      <td>14.808251</td>\n",
       "      <td>7.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_DideoxyC</th>\n",
       "      <td>211.25</td>\n",
       "      <td>7.544643</td>\n",
       "      <td>16.8426</td>\n",
       "      <td>28.7053</td>\n",
       "      <td>17.1876</td>\n",
       "      <td>32.1991</td>\n",
       "      <td>0.601521</td>\n",
       "      <td>1.025189</td>\n",
       "      <td>0.613843</td>\n",
       "      <td>1.149968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>13.597226</td>\n",
       "      <td>7.439730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peters_2014_3</th>\n",
       "      <td>268.26</td>\n",
       "      <td>8.653548</td>\n",
       "      <td>19.7668</td>\n",
       "      <td>32.5781</td>\n",
       "      <td>19.3409</td>\n",
       "      <td>35.7010</td>\n",
       "      <td>0.637639</td>\n",
       "      <td>1.050906</td>\n",
       "      <td>0.623900</td>\n",
       "      <td>1.151645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>17.966397</td>\n",
       "      <td>8.618182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plank_2016_2</th>\n",
       "      <td>393.17</td>\n",
       "      <td>12.286563</td>\n",
       "      <td>21.3902</td>\n",
       "      <td>33.4217</td>\n",
       "      <td>22.5512</td>\n",
       "      <td>36.7105</td>\n",
       "      <td>0.668444</td>\n",
       "      <td>1.044428</td>\n",
       "      <td>0.704725</td>\n",
       "      <td>1.147203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>8.25</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>19.586399</td>\n",
       "      <td>10.684257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Du2021_L_G</th>\n",
       "      <td>283.28</td>\n",
       "      <td>8.584242</td>\n",
       "      <td>20.7882</td>\n",
       "      <td>34.6799</td>\n",
       "      <td>20.3466</td>\n",
       "      <td>38.1993</td>\n",
       "      <td>0.629945</td>\n",
       "      <td>1.050906</td>\n",
       "      <td>0.616564</td>\n",
       "      <td>1.157555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>19.586399</td>\n",
       "      <td>10.257197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 4175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MW        AMW       Sv       Se       Sp       Si  \\\n",
       "ID                                                                          \n",
       "Ma_2019_A           267.28   8.352500  20.0734  33.3526  19.8921  36.9899   \n",
       "Ma_2019_U           244.23   8.421724  17.9656  30.5854  17.5454  33.3290   \n",
       "Ma_2019_C           243.25   8.108333  18.2722  31.3599  18.0966  34.6179   \n",
       "Ma_2019_G           283.28   8.584242  20.7882  34.6799  20.3466  38.1993   \n",
       "Ma_2019_dA          251.28   8.105806  19.3586  32.0253  19.4376  35.7805   \n",
       "...                    ...        ...      ...      ...      ...      ...   \n",
       "Tang_2019_ArabinoC  243.25   8.108333  18.2722  31.3599  18.0966  34.6179   \n",
       "Tang_2019_DideoxyC  211.25   7.544643  16.8426  28.7053  17.1876  32.1991   \n",
       "Peters_2014_3       268.26   8.653548  19.7668  32.5781  19.3409  35.7010   \n",
       "Plank_2016_2        393.17  12.286563  21.3902  33.4217  22.5512  36.7105   \n",
       "Du2021_L_G          283.28   8.584242  20.7882  34.6799  20.3466  38.1993   \n",
       "\n",
       "                          Mv        Me        Mp        Mi  ...  \\\n",
       "ID                                                          ...   \n",
       "Ma_2019_A           0.627294  1.042269  0.621628  1.155934  ...   \n",
       "Ma_2019_U           0.619503  1.054669  0.605014  1.149276  ...   \n",
       "Ma_2019_C           0.609073  1.045330  0.603220  1.153930  ...   \n",
       "Ma_2019_G           0.629945  1.050906  0.616564  1.157555  ...   \n",
       "Ma_2019_dA          0.624471  1.033074  0.627019  1.154210  ...   \n",
       "...                      ...       ...       ...       ...  ...   \n",
       "Tang_2019_ArabinoC  0.609073  1.045330  0.603220  1.153930  ...   \n",
       "Tang_2019_DideoxyC  0.601521  1.025189  0.613843  1.149968  ...   \n",
       "Peters_2014_3       0.637639  1.050906  0.623900  1.151645  ...   \n",
       "Plank_2016_2        0.668444  1.044428  0.704725  1.147203  ...   \n",
       "Du2021_L_G          0.629945  1.050906  0.616564  1.157555  ...   \n",
       "\n",
       "                    s1_numAroBonds  s2_numAroBonds  s3_numAroBonds  \\\n",
       "ID                                                                   \n",
       "Ma_2019_A                      0.0             0.0             0.0   \n",
       "Ma_2019_U                      0.0             0.0             0.0   \n",
       "Ma_2019_C                      0.0             0.0             0.0   \n",
       "Ma_2019_G                      0.0             0.0             0.0   \n",
       "Ma_2019_dA                     0.0             0.0             0.0   \n",
       "...                            ...             ...             ...   \n",
       "Tang_2019_ArabinoC             0.0             0.0             0.0   \n",
       "Tang_2019_DideoxyC             0.0             0.0             0.0   \n",
       "Peters_2014_3                  0.0             0.0             0.0   \n",
       "Plank_2016_2                   0.0             0.0             0.0   \n",
       "Du2021_L_G                     0.0             0.0             0.0   \n",
       "\n",
       "                    s4_numAroBonds  s34_size  s34_relSize  s34_phSize  \\\n",
       "ID                                                                      \n",
       "Ma_2019_A                     10.0      16.0     0.842105        7.75   \n",
       "Ma_2019_U                      0.0      14.0     0.823529        6.75   \n",
       "Ma_2019_C                      0.0      14.0     0.823529        6.75   \n",
       "Ma_2019_G                      5.0      17.0     0.850000        8.75   \n",
       "Ma_2019_dA                    10.0      15.0     0.833333        7.00   \n",
       "...                            ...       ...          ...         ...   \n",
       "Tang_2019_ArabinoC             0.0      14.0     0.823529        6.75   \n",
       "Tang_2019_DideoxyC             0.0      12.0     0.800000        6.00   \n",
       "Peters_2014_3                  5.0      16.0     0.842105        7.75   \n",
       "Plank_2016_2                   5.0      17.0     0.850000        8.25   \n",
       "Du2021_L_G                     5.0      17.0     0.850000        8.75   \n",
       "\n",
       "                    s34_phRelSize  chiralMoment  chiralPhMoment  \n",
       "ID                                                               \n",
       "Ma_2019_A                0.407895     17.966397        8.618182  \n",
       "Ma_2019_U                0.397059     14.808251        7.026700  \n",
       "Ma_2019_C                0.397059     14.808251        7.026700  \n",
       "Ma_2019_G                0.437500     19.586399       10.257197  \n",
       "Ma_2019_dA               0.388889     17.259745        8.115820  \n",
       "...                           ...           ...             ...  \n",
       "Tang_2019_ArabinoC       0.397059     14.808251        7.026700  \n",
       "Tang_2019_DideoxyC       0.400000     13.597226        7.439730  \n",
       "Peters_2014_3            0.407895     17.966397        8.618182  \n",
       "Plank_2016_2             0.412500     19.586399       10.684257  \n",
       "Du2021_L_G               0.437500     19.586399       10.257197  \n",
       "\n",
       "[71 rows x 4175 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_NAomit_data= pd.read_csv(\"./Original data/X_NAomit_data.csv\",header=0,index_col=0)\n",
    "X_NAomit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1c89e32-9d35-4229-a7fd-6dc4bc6e3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data= pd.read_csv(\"./Original data/X_NAomit_data.csv\",header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c075c2f5-c839-4ab6-8f1d-ee4e808c3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(X_NAomit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "066559ff-aefe-4de1-98aa-04e6e2d7d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=Raw_data['Hydrogel-forming ability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f04d536-af36-4f80-8c82-b43e30cbf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cv_model= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf835fc-df29-4f95-ac06-6cc88fa619f2",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "07038cde-c52d-4ba2-b851-445a650f8dc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.491e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.820e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.078e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.396e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.960e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.032e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.783e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.841e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.112e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.075e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.780e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.859e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.412e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.970e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.931e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.071e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.448e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.909e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.083e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.833e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.857e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.947e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.490e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.624e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.720e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.477e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.986e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.108e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.925e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.142e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.346e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.644e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.928e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.640e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.806e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.693e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.467e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.704e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.687e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.589e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.476e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.982e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.891e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.462e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.627e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.710e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.640e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.244e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.143e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.040e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.758e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.575e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.912e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.256e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.393e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.869e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.585e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.015e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.381e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.047e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.257e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.284e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.714e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.504e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.224e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.035e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.951e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.167e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.929e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.751e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.548e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.466e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.424e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.481e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.748e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.690e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.922e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.457e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.240e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.604e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.530e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.711e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.020e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.443e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.469e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.748e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.479e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.518e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.949e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.261e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.642e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.301e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.685e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.460e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.101e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.455e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.454e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.569e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.293e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.709e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.756e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.391e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.262e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.039e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.821e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.681e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.148e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.391e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.453e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.920e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.258e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.655e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.552e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.097e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.627e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.639e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.605e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.479e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.575e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.647e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.280e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.546e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.074e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.507e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.763e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.927e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.930e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.432e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.651e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.898e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.273e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.652e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.023e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.971e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.551e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.063e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.574e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.557e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.114e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.621e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.819e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.286e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.603e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.878e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.787e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.856e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.608e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.474e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.791e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.695e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.586e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.868e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.845e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.437e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.560e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.037e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.622e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.787e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.339e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.935e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.748e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.671e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.492e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.952e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.424e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.998e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.995e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.595e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.754e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.628e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.621e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.806e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.207e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.707e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.629e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.319e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.988e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.057e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.868e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.707e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.736e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.875e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.749e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.409e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.305e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.782e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.662e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.664e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.851e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.398e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.826e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.974e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.555e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.508e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.671e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.760e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.075e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.267e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.077e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.490e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.743e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.561e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.506e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.434e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.316e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.225e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.808e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.950e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.948e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.114e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.810e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.894e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.747e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.301e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.107e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.426e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.592e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.039e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.736e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.272e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.250e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.429e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.237e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.711e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.645e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.688e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.685e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.674e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.966e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.625e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.430e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.835e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.579e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.712e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.011e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.538e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.581e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.509e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.100e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.563e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.205e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.489e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.222e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.900e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.625e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.034e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.572e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.664e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.043e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.660e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.917e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.147e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.378e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.851e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.885e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.931e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.879e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.771e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.125e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.842e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.443e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.514e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.852e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.722e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.742e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.002e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.469e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.869e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.825e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.686e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.516e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.213e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.941e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.097e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.262e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.288e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.054e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.050e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.081e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.982e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.660e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.033e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.058e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.587e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.714e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.914e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.349e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.966e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.060e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.478e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.970e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.779e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.307e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.745e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.634e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.109e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.498e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.723e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.138e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.318e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.112e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.706e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.613e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.513e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.033e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.120e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.386e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.556e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.546e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.648e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.537e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.227e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.124e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.635e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.430e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.132e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.065e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.888e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.817e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.515e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.681e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.856e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.745e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.600e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.458e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.491e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.057e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.525e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.095e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.030e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.869e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.713e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.127e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.594e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.551e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.453e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.074e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.091e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.942e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.852e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.787e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.063e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.388e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.666e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.869e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.420e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.498e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.425e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.626e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.491e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.435e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.783e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.769e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.109e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.942e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.704e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.198e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.014e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.425e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.728e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.800e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.069e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.350e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.238e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.211e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.829e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.349e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.605e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.415e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.960e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.847e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.736e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.496e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.738e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.979e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.499e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.465e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.508e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.602e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.878e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.046e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.201e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.550e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.581e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.777e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.153e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.368e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.401e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.375e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.604e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.532e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.576e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.472e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.927e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.831e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.000e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.628e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.615e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.847e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.653e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.935e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.914e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.919e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.867e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.157e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.277e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.716e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.688e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.196e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.671e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.706e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.708e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.744e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.947e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.951e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.232e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.390e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.983e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.389e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.604e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.381e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.168e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.606e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.147e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.855e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.838e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.437e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.268e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.559e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.822e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.283e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.578e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.854e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.838e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.672e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.581e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.494e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.416e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.957e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.788e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.506e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.810e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.938e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.253e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.304e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.913e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.513e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.930e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.342e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.504e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.994e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.591e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.358e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.811e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.863e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.164e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.480e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.992e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.604e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.513e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.184e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.713e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.073e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.787e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.564e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.625e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.785e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.845e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.069e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.602e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.473e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.048e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.638e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.034e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.640e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.442e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.845e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.693e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.464e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.782e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.737e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.639e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.747e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.930e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.792e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.415e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.098e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.291e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.165e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.736e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.287e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.046e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.829e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.633e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.451e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.943e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.808e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.538e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.048e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.609e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.273e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.649e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.672e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.811e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.845e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.789e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.544e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.601e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.097e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.532e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.797e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.621e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.048e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.914e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.606e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.520e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.903e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.316e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.216e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.489e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.588e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.848e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.006e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.490e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.542e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.992e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.488e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.583e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.875e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.316e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.664e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.871e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.656e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.011e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.414e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.147e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.504e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.794e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.156e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.425e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.657e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.689e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.515e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.300e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.409e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.612e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.844e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.429e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.164e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.858e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.240e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.510e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.733e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.121e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.697e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.747e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.889e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.189e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.762e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.191e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.399e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.979e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.747e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.604e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.415e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.957e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.478e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.397e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.316e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.577e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.890e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.839e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.581e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.683e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.859e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.001e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.093e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.947e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.415e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.418e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.594e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.347e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.594e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.815e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.980e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.827e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.839e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.348e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.454e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.007e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.903e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.570e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.998e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.645e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.258e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.133e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.913e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.575e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.615e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.800e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.510e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.244e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.392e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.036e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.111e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.615e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.132e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.101e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.584e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.449e-03, tolerance: 1.393e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.092e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.123e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.008e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.625e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.401e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.492e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.083e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.933e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.869e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.207e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.892e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.399e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.477e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.229e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.622e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.494e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.288e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.149e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.035e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.935e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.355e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.466e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.191e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.467e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.623e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.466e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.049e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.157e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.773e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.253e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.378e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.143e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.922e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.858e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.022e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.600e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.904e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.679e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.648e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.421e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.098e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.983e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.246e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.806e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.762e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.041e-03, tolerance: 1.421e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.699e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.532e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.479e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.901e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.496e-03, tolerance: 1.414e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "model = make_pipeline(StandardScaler(), LassoCV(cv=Cv_model)).fit(X, y)\n",
    "\n",
    "warnings.simplefilter(\"always\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "402b4353-a400-45cc-ba23-bddd8b9e5d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x25984b89880>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ymin, ymax =0, 1\n",
    "lasso = model[-1]\n",
    "plt.semilogx(lasso.alphas_, lasso.mse_path_, linestyle=\":\")\n",
    "plt.plot(\n",
    "    lasso.alphas_,\n",
    "    lasso.mse_path_.mean(axis=-1),\n",
    "    color=\"black\",\n",
    "    label=\"Average across the folds\",\n",
    "    linewidth=2,\n",
    ")\n",
    "plt.axvline(lasso.alpha_, linestyle=\"--\", color=\"black\", label=\"alpha: CV estimate\")\n",
    "\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.xlabel(r\"$\\alpha$\")\n",
    "plt.ylabel(\"Mean square error\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6d8f6c6-6af3-489b-ac9c-21b7ea220538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03623607861465868"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0be9143b-967c-41fc-8e33-04f34afa9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cv_model= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d10f1531-da2b-4bb6-b83b-3558bdac2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.920e+00, tolerance: 1.766e-03\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(71, 70)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc =  Lasso(alpha=lasso.alpha_).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c39feec4-a989-406f-b6a2-1c3428dc3b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D/Dtr05</th>\n",
       "      <th>D/Dtr06</th>\n",
       "      <th>D/Dtr09</th>\n",
       "      <th>ZM1Mad</th>\n",
       "      <th>ZM1Per</th>\n",
       "      <th>ZM2Per</th>\n",
       "      <th>CENT</th>\n",
       "      <th>SMTI</th>\n",
       "      <th>GMTIV</th>\n",
       "      <th>CSI</th>\n",
       "      <th>...</th>\n",
       "      <th>P_VSA_charge_6</th>\n",
       "      <th>P_VSA_charge_7</th>\n",
       "      <th>P_VSA_charge_8</th>\n",
       "      <th>P_VSA_charge_9</th>\n",
       "      <th>P_VSA_charge_10</th>\n",
       "      <th>T(N..O)</th>\n",
       "      <th>T(O..O)</th>\n",
       "      <th>T(O..F)</th>\n",
       "      <th>F05[C-N]</th>\n",
       "      <th>SAtot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ma_2019_A</th>\n",
       "      <td>79.738154</td>\n",
       "      <td>44.396318</td>\n",
       "      <td>66.871159</td>\n",
       "      <td>123.123376</td>\n",
       "      <td>465.73</td>\n",
       "      <td>492.95</td>\n",
       "      <td>383.0</td>\n",
       "      <td>2886.0</td>\n",
       "      <td>10547.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.626612</td>\n",
       "      <td>42.923526</td>\n",
       "      <td>118.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>341.796154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_U</th>\n",
       "      <td>45.292352</td>\n",
       "      <td>53.643579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>107.361029</td>\n",
       "      <td>431.84</td>\n",
       "      <td>414.09</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>8734.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.523668</td>\n",
       "      <td>94.168816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>332.147792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_C</th>\n",
       "      <td>45.292352</td>\n",
       "      <td>53.643579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.229031</td>\n",
       "      <td>412.07</td>\n",
       "      <td>408.23</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>7972.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.476247</td>\n",
       "      <td>9.047420</td>\n",
       "      <td>94.168816</td>\n",
       "      <td>2.327920</td>\n",
       "      <td>74.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>336.298102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_G</th>\n",
       "      <td>85.493024</td>\n",
       "      <td>47.162191</td>\n",
       "      <td>71.547747</td>\n",
       "      <td>132.059311</td>\n",
       "      <td>511.55</td>\n",
       "      <td>534.91</td>\n",
       "      <td>446.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>12768.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.626612</td>\n",
       "      <td>20.389695</td>\n",
       "      <td>135.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>353.277651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_dA</th>\n",
       "      <td>74.565443</td>\n",
       "      <td>41.842288</td>\n",
       "      <td>63.146800</td>\n",
       "      <td>115.019665</td>\n",
       "      <td>425.91</td>\n",
       "      <td>455.75</td>\n",
       "      <td>336.0</td>\n",
       "      <td>2584.0</td>\n",
       "      <td>8812.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.076736</td>\n",
       "      <td>58.855510</td>\n",
       "      <td>42.923526</td>\n",
       "      <td>91.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>313.418445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_ArabinoC</th>\n",
       "      <td>45.292352</td>\n",
       "      <td>53.643579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>106.229031</td>\n",
       "      <td>412.07</td>\n",
       "      <td>408.23</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>7972.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.476247</td>\n",
       "      <td>9.047420</td>\n",
       "      <td>94.168816</td>\n",
       "      <td>2.327920</td>\n",
       "      <td>74.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>336.298102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_DideoxyC</th>\n",
       "      <td>38.692352</td>\n",
       "      <td>46.741306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>90.021608</td>\n",
       "      <td>333.09</td>\n",
       "      <td>337.91</td>\n",
       "      <td>213.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>5062.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.545311</td>\n",
       "      <td>56.131828</td>\n",
       "      <td>70.626612</td>\n",
       "      <td>2.327920</td>\n",
       "      <td>41.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>279.542683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peters_2014_3</th>\n",
       "      <td>79.738154</td>\n",
       "      <td>44.396318</td>\n",
       "      <td>66.871159</td>\n",
       "      <td>124.255374</td>\n",
       "      <td>486.06</td>\n",
       "      <td>501.59</td>\n",
       "      <td>383.0</td>\n",
       "      <td>2886.0</td>\n",
       "      <td>11275.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.245205</td>\n",
       "      <td>32.160797</td>\n",
       "      <td>101.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>338.871066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plank_2016_2</th>\n",
       "      <td>85.493024</td>\n",
       "      <td>47.162191</td>\n",
       "      <td>71.547747</td>\n",
       "      <td>260.396909</td>\n",
       "      <td>537.51</td>\n",
       "      <td>542.77</td>\n",
       "      <td>446.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>13668.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.782008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.550804</td>\n",
       "      <td>47.084408</td>\n",
       "      <td>20.389695</td>\n",
       "      <td>98.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>368.384917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Du2021_L_G</th>\n",
       "      <td>85.493024</td>\n",
       "      <td>47.162191</td>\n",
       "      <td>71.547747</td>\n",
       "      <td>132.059311</td>\n",
       "      <td>511.55</td>\n",
       "      <td>534.91</td>\n",
       "      <td>446.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>12768.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.626612</td>\n",
       "      <td>20.389695</td>\n",
       "      <td>135.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>353.277651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      D/Dtr05    D/Dtr06    D/Dtr09      ZM1Mad  ZM1Per  \\\n",
       "ID                                                                        \n",
       "Ma_2019_A           79.738154  44.396318  66.871159  123.123376  465.73   \n",
       "Ma_2019_U           45.292352  53.643579   0.000000  107.361029  431.84   \n",
       "Ma_2019_C           45.292352  53.643579   0.000000  106.229031  412.07   \n",
       "Ma_2019_G           85.493024  47.162191  71.547747  132.059311  511.55   \n",
       "Ma_2019_dA          74.565443  41.842288  63.146800  115.019665  425.91   \n",
       "...                       ...        ...        ...         ...     ...   \n",
       "Tang_2019_ArabinoC  45.292352  53.643579   0.000000  106.229031  412.07   \n",
       "Tang_2019_DideoxyC  38.692352  46.741306   0.000000   90.021608  333.09   \n",
       "Peters_2014_3       79.738154  44.396318  66.871159  124.255374  486.06   \n",
       "Plank_2016_2        85.493024  47.162191  71.547747  260.396909  537.51   \n",
       "Du2021_L_G          85.493024  47.162191  71.547747  132.059311  511.55   \n",
       "\n",
       "                    ZM2Per   CENT    SMTI    GMTIV    CSI  ...  \\\n",
       "ID                                                         ...   \n",
       "Ma_2019_A           492.95  383.0  2886.0  10547.0  288.0  ...   \n",
       "Ma_2019_U           414.09  295.0  2084.0   8734.0  240.0  ...   \n",
       "Ma_2019_C           408.23  295.0  2084.0   7972.0  240.0  ...   \n",
       "Ma_2019_G           534.91  446.0  3270.0  12768.0  305.0  ...   \n",
       "Ma_2019_dA          455.75  336.0  2584.0   8812.0  275.0  ...   \n",
       "...                    ...    ...     ...      ...    ...  ...   \n",
       "Tang_2019_ArabinoC  408.23  295.0  2084.0   7972.0  240.0  ...   \n",
       "Tang_2019_DideoxyC  337.91  213.0  1585.0   5062.0  212.0  ...   \n",
       "Peters_2014_3       501.59  383.0  2886.0  11275.0  288.0  ...   \n",
       "Plank_2016_2        542.77  446.0  3270.0  13668.0  305.0  ...   \n",
       "Du2021_L_G          534.91  446.0  3270.0  12768.0  305.0  ...   \n",
       "\n",
       "                    P_VSA_charge_6  P_VSA_charge_7  P_VSA_charge_8  \\\n",
       "ID                                                                   \n",
       "Ma_2019_A                 0.000000        0.000000        0.000000   \n",
       "Ma_2019_U                 0.000000        0.000000       18.523668   \n",
       "Ma_2019_C                 0.000000        9.476247        9.047420   \n",
       "Ma_2019_G                 0.000000        0.000000        0.000000   \n",
       "Ma_2019_dA                0.000000        0.000000       26.076736   \n",
       "...                            ...             ...             ...   \n",
       "Tang_2019_ArabinoC        0.000000        9.476247        9.047420   \n",
       "Tang_2019_DideoxyC        0.000000       14.545311       56.131828   \n",
       "Peters_2014_3             0.000000        0.000000        0.000000   \n",
       "Plank_2016_2             56.782008        0.000000       24.550804   \n",
       "Du2021_L_G                0.000000        0.000000        0.000000   \n",
       "\n",
       "                    P_VSA_charge_9  P_VSA_charge_10  T(N..O)  T(O..O)  \\\n",
       "ID                                                                      \n",
       "Ma_2019_A                70.626612        42.923526    118.0     21.0   \n",
       "Ma_2019_U                94.168816         0.000000     46.0     77.0   \n",
       "Ma_2019_C                94.168816         2.327920     74.0     43.0   \n",
       "Ma_2019_G                70.626612        20.389695    135.0     51.0   \n",
       "Ma_2019_dA               58.855510        42.923526     91.0     10.0   \n",
       "...                            ...              ...      ...      ...   \n",
       "Tang_2019_ArabinoC       94.168816         2.327920     74.0     43.0   \n",
       "Tang_2019_DideoxyC       70.626612         2.327920     41.0     14.0   \n",
       "Peters_2014_3            79.245205        32.160797    101.0     51.0   \n",
       "Plank_2016_2             47.084408        20.389695     98.0     30.0   \n",
       "Du2021_L_G               70.626612        20.389695    135.0     51.0   \n",
       "\n",
       "                    T(O..F)  F05[C-N]       SAtot  \n",
       "ID                                                 \n",
       "Ma_2019_A               0.0       6.0  341.796154  \n",
       "Ma_2019_U               0.0       2.0  332.147792  \n",
       "Ma_2019_C               0.0       3.0  336.298102  \n",
       "Ma_2019_G               0.0       7.0  353.277651  \n",
       "Ma_2019_dA              0.0       6.0  313.418445  \n",
       "...                     ...       ...         ...  \n",
       "Tang_2019_ArabinoC      0.0       3.0  336.298102  \n",
       "Tang_2019_DideoxyC      0.0       3.0  279.542683  \n",
       "Peters_2014_3           0.0       5.0  338.871066  \n",
       "Plank_2016_2            0.0       7.0  368.384917  \n",
       "Du2021_L_G              0.0       7.0  353.277651  \n",
       "\n",
       "[71 rows x 70 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lasso_data=X_NAomit_data[X_NAomit_data.columns[model.get_support()]]\n",
    "Lasso_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c9c9a6da-b735-4689-897c-972cd30e5f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D/Dtr05</th>\n",
       "      <th>D/Dtr06</th>\n",
       "      <th>D/Dtr09</th>\n",
       "      <th>ZM1Mad</th>\n",
       "      <th>ZM1Per</th>\n",
       "      <th>ZM2Per</th>\n",
       "      <th>CENT</th>\n",
       "      <th>SMTI</th>\n",
       "      <th>GMTIV</th>\n",
       "      <th>CSI</th>\n",
       "      <th>...</th>\n",
       "      <th>P_VSA_charge_6</th>\n",
       "      <th>P_VSA_charge_7</th>\n",
       "      <th>P_VSA_charge_8</th>\n",
       "      <th>P_VSA_charge_9</th>\n",
       "      <th>P_VSA_charge_10</th>\n",
       "      <th>T(N..O)</th>\n",
       "      <th>T(O..O)</th>\n",
       "      <th>T(O..F)</th>\n",
       "      <th>F05[C-N]</th>\n",
       "      <th>SAtot</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ma_2019_A</th>\n",
       "      <td>0.079122</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>0.181294</td>\n",
       "      <td>0.166748</td>\n",
       "      <td>0.189792</td>\n",
       "      <td>0.193293</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.044265</td>\n",
       "      <td>0.040042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091908</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250784</td>\n",
       "      <td>0.054187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_U</th>\n",
       "      <td>0.012722</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087346</td>\n",
       "      <td>0.141300</td>\n",
       "      <td>0.094976</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.029633</td>\n",
       "      <td>0.014752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044637</td>\n",
       "      <td>0.183816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025078</td>\n",
       "      <td>0.330049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.083150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_C</th>\n",
       "      <td>0.012722</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081644</td>\n",
       "      <td>0.113011</td>\n",
       "      <td>0.087670</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.014752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206211</td>\n",
       "      <td>0.021802</td>\n",
       "      <td>0.183816</td>\n",
       "      <td>0.054234</td>\n",
       "      <td>0.112853</td>\n",
       "      <td>0.162562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.089710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_G</th>\n",
       "      <td>0.090215</td>\n",
       "      <td>0.011012</td>\n",
       "      <td>0.193972</td>\n",
       "      <td>0.211762</td>\n",
       "      <td>0.255355</td>\n",
       "      <td>0.245605</td>\n",
       "      <td>0.036549</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>0.062188</td>\n",
       "      <td>0.048999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091908</td>\n",
       "      <td>0.475024</td>\n",
       "      <td>0.304075</td>\n",
       "      <td>0.201970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.116549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_dA</th>\n",
       "      <td>0.069151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171197</td>\n",
       "      <td>0.125926</td>\n",
       "      <td>0.132814</td>\n",
       "      <td>0.146914</td>\n",
       "      <td>0.019294</td>\n",
       "      <td>0.023662</td>\n",
       "      <td>0.030263</td>\n",
       "      <td>0.033193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062838</td>\n",
       "      <td>0.045954</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12500</td>\n",
       "      <td>0.053545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_ArabinoC</th>\n",
       "      <td>0.012722</td>\n",
       "      <td>0.024427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081644</td>\n",
       "      <td>0.113011</td>\n",
       "      <td>0.087670</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.014752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206211</td>\n",
       "      <td>0.021802</td>\n",
       "      <td>0.183816</td>\n",
       "      <td>0.054234</td>\n",
       "      <td>0.112853</td>\n",
       "      <td>0.162562</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.089710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_DideoxyC</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316518</td>\n",
       "      <td>0.135262</td>\n",
       "      <td>0.091908</td>\n",
       "      <td>0.054234</td>\n",
       "      <td>0.009404</td>\n",
       "      <td>0.019704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peters_2014_3</th>\n",
       "      <td>0.079122</td>\n",
       "      <td>0.005287</td>\n",
       "      <td>0.181294</td>\n",
       "      <td>0.172450</td>\n",
       "      <td>0.218882</td>\n",
       "      <td>0.204064</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.050140</td>\n",
       "      <td>0.040042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125555</td>\n",
       "      <td>0.749258</td>\n",
       "      <td>0.197492</td>\n",
       "      <td>0.201970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09375</td>\n",
       "      <td>0.093777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plank_2016_2</th>\n",
       "      <td>0.090215</td>\n",
       "      <td>0.011012</td>\n",
       "      <td>0.193972</td>\n",
       "      <td>0.858253</td>\n",
       "      <td>0.292501</td>\n",
       "      <td>0.255405</td>\n",
       "      <td>0.036549</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>0.069451</td>\n",
       "      <td>0.048999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059161</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475024</td>\n",
       "      <td>0.188088</td>\n",
       "      <td>0.098522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.140428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Du2021_L_G</th>\n",
       "      <td>0.090215</td>\n",
       "      <td>0.011012</td>\n",
       "      <td>0.193972</td>\n",
       "      <td>0.211762</td>\n",
       "      <td>0.255355</td>\n",
       "      <td>0.245605</td>\n",
       "      <td>0.036549</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>0.062188</td>\n",
       "      <td>0.048999</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091908</td>\n",
       "      <td>0.475024</td>\n",
       "      <td>0.304075</td>\n",
       "      <td>0.201970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15625</td>\n",
       "      <td>0.116549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     D/Dtr05   D/Dtr06   D/Dtr09    ZM1Mad    ZM1Per  \\\n",
       "ID                                                                     \n",
       "Ma_2019_A           0.079122  0.005287  0.181294  0.166748  0.189792   \n",
       "Ma_2019_U           0.012722  0.024427  0.000000  0.087346  0.141300   \n",
       "Ma_2019_C           0.012722  0.024427  0.000000  0.081644  0.113011   \n",
       "Ma_2019_G           0.090215  0.011012  0.193972  0.211762  0.255355   \n",
       "Ma_2019_dA          0.069151  0.000000  0.171197  0.125926  0.132814   \n",
       "...                      ...       ...       ...       ...       ...   \n",
       "Tang_2019_ArabinoC  0.012722  0.024427  0.000000  0.081644  0.113011   \n",
       "Tang_2019_DideoxyC  0.000000  0.010140  0.000000  0.000000  0.000000   \n",
       "Peters_2014_3       0.079122  0.005287  0.181294  0.172450  0.218882   \n",
       "Plank_2016_2        0.090215  0.011012  0.193972  0.858253  0.292501   \n",
       "Du2021_L_G          0.090215  0.011012  0.193972  0.211762  0.255355   \n",
       "\n",
       "                      ZM2Per      CENT      SMTI     GMTIV       CSI  ...  \\\n",
       "ID                                                                    ...   \n",
       "Ma_2019_A           0.193293  0.026667  0.030815  0.044265  0.040042  ...   \n",
       "Ma_2019_U           0.094976  0.012863  0.011819  0.029633  0.014752  ...   \n",
       "Ma_2019_C           0.087670  0.012863  0.011819  0.023484  0.014752  ...   \n",
       "Ma_2019_G           0.245605  0.036549  0.039910  0.062188  0.048999  ...   \n",
       "Ma_2019_dA          0.146914  0.019294  0.023662  0.030263  0.033193  ...   \n",
       "...                      ...       ...       ...       ...       ...  ...   \n",
       "Tang_2019_ArabinoC  0.087670  0.012863  0.011819  0.023484  0.014752  ...   \n",
       "Tang_2019_DideoxyC  0.000000  0.000000  0.000000  0.000000  0.000000  ...   \n",
       "Peters_2014_3       0.204064  0.026667  0.030815  0.050140  0.040042  ...   \n",
       "Plank_2016_2        0.255405  0.036549  0.039910  0.069451  0.048999  ...   \n",
       "Du2021_L_G          0.245605  0.036549  0.039910  0.062188  0.048999  ...   \n",
       "\n",
       "                    P_VSA_charge_6  P_VSA_charge_7  P_VSA_charge_8  \\\n",
       "ID                                                                   \n",
       "Ma_2019_A                 0.000000        0.000000        0.000000   \n",
       "Ma_2019_U                 0.000000        0.000000        0.044637   \n",
       "Ma_2019_C                 0.000000        0.206211        0.021802   \n",
       "Ma_2019_G                 0.000000        0.000000        0.000000   \n",
       "Ma_2019_dA                0.000000        0.000000        0.062838   \n",
       "...                            ...             ...             ...   \n",
       "Tang_2019_ArabinoC        0.000000        0.206211        0.021802   \n",
       "Tang_2019_DideoxyC        0.000000        0.316518        0.135262   \n",
       "Peters_2014_3             0.000000        0.000000        0.000000   \n",
       "Plank_2016_2              0.391941        0.000000        0.059161   \n",
       "Du2021_L_G                0.000000        0.000000        0.000000   \n",
       "\n",
       "                    P_VSA_charge_9  P_VSA_charge_10   T(N..O)   T(O..O)  \\\n",
       "ID                                                                        \n",
       "Ma_2019_A                 0.091908         1.000000  0.250784  0.054187   \n",
       "Ma_2019_U                 0.183816         0.000000  0.025078  0.330049   \n",
       "Ma_2019_C                 0.183816         0.054234  0.112853  0.162562   \n",
       "Ma_2019_G                 0.091908         0.475024  0.304075  0.201970   \n",
       "Ma_2019_dA                0.045954         1.000000  0.166144  0.000000   \n",
       "...                            ...              ...       ...       ...   \n",
       "Tang_2019_ArabinoC        0.183816         0.054234  0.112853  0.162562   \n",
       "Tang_2019_DideoxyC        0.091908         0.054234  0.009404  0.019704   \n",
       "Peters_2014_3             0.125555         0.749258  0.197492  0.201970   \n",
       "Plank_2016_2              0.000000         0.475024  0.188088  0.098522   \n",
       "Du2021_L_G                0.091908         0.475024  0.304075  0.201970   \n",
       "\n",
       "                    T(O..F)  F05[C-N]     SAtot  \n",
       "ID                                               \n",
       "Ma_2019_A               0.0   0.12500  0.098400  \n",
       "Ma_2019_U               0.0   0.00000  0.083150  \n",
       "Ma_2019_C               0.0   0.03125  0.089710  \n",
       "Ma_2019_G               0.0   0.15625  0.116549  \n",
       "Ma_2019_dA              0.0   0.12500  0.053545  \n",
       "...                     ...       ...       ...  \n",
       "Tang_2019_ArabinoC      0.0   0.03125  0.089710  \n",
       "Tang_2019_DideoxyC      0.0   0.03125  0.000000  \n",
       "Peters_2014_3           0.0   0.09375  0.093777  \n",
       "Plank_2016_2            0.0   0.15625  0.140428  \n",
       "Du2021_L_G              0.0   0.15625  0.116549  \n",
       "\n",
       "[71 rows x 70 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scale data\n",
    "Scaler = preprocessing.MinMaxScaler() #StandardScaler\n",
    "Transformer =Scaler.fit(Lasso_data)\n",
    "X_scaled_data=Transformer.transform(Lasso_data)\n",
    "X_scaled_data =pd.DataFrame(X_scaled_data)\n",
    "X_scaled_data.columns=Lasso_data.columns\n",
    "X_scaled_data.index=Raw_data.index\n",
    "\n",
    "joblib.dump(Transformer, './Models/Lasso_Scaler_transformer.pkl')\n",
    "\n",
    "X_scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c0643c9-e6ee-441f-a6e6-f8951935a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_results(Model_clf,X_test,y,Cv_model):\n",
    "    Model_scores= cross_validate(estimator=Model_clf, X=X_test, y=y, cv=Cv_model,scoring=( 'accuracy','f1','precision','recall','roc_auc'), return_train_score=True)\n",
    "    Model_score= cross_validate(estimator=Model_clf, X=X_test, y=y, cv=Cv_model,scoring=( 'accuracy','f1','precision','recall','roc_auc'), return_train_score=False)\n",
    "#Accuracy\n",
    "    Model_Accuracy_test_mean=Model_scores['test_accuracy'].mean()\n",
    "    Model_Accuracy_test_se=(Model_scores['test_accuracy'].std()/math.sqrt(len(Model_scores['test_accuracy']))) \n",
    "    Model_Accuracy_train_mean=Model_scores['train_accuracy'].mean()\n",
    "    Model_Accuracy_train_se=(Model_scores['train_accuracy'].std()/math.sqrt(len(Model_scores['train_accuracy']))) \n",
    "#f1\n",
    "    Model_f1_mean=Model_score['test_f1'].mean()\n",
    "    Model_f1_se=(Model_score['test_f1'].std()/math.sqrt(len(Model_score['test_f1']))) \n",
    "#precision\n",
    "    Model_precision_mean=Model_score['test_precision'].mean()\n",
    "    Model_precision_se=(Model_score['test_precision'].std()/math.sqrt(len(Model_score['test_precision']))) \n",
    "#recall\n",
    "    Model_recall_mean=Model_score['test_recall'].mean()\n",
    "    Model_recall_se=(Model_score['test_recall'].std()/math.sqrt(len(Model_score['test_recall']))) \n",
    "#roc_auc\n",
    "    Model_roc_auc_mean=Model_score['test_roc_auc'].mean()\n",
    "    Model_roc_auc_se=(Model_score['test_roc_auc'].std()/math.sqrt(len(Model_score['test_roc_auc']))) \n",
    "    Model = {'Mean':[Model_Accuracy_test_mean,Model_Accuracy_train_mean,Model_f1_mean,Model_precision_mean,Model_recall_mean,Model_roc_auc_mean],\n",
    "        'Se':[Model_Accuracy_test_se,Model_Accuracy_train_se,Model_f1_se,Model_precision_se,Model_recall_se,Model_roc_auc_se]}\n",
    "    Model = pd.DataFrame(Model, index=['Accuracy_test','Accuracy_train','F1 Score','Precision','Recall','Roc_auc']) # 这里设定了 index 个数要和列表长度一致\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e49d1bb-d5f4-4e7f-a620-a943c578fba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the KFold \n",
    "Cv_optuna= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "Cv_model= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "Cv_RFECV= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b74394b-d874-4115-b66e-0bac0d514965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data pre-processing of models\n",
    "X=np.array(X_scaled_data)\n",
    "y=Raw_data['Hydrogel-forming ability'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047485d-89ab-471b-b393-08d893b317bc",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7e67efc5-4154-43ea-ad89-57f6ba68ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LogisticRegression(solver='liblinear',random_state=0,dual=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4e72f778-0bc0-4da2-b17d-d66357ab161c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.650667</td>\n",
       "      <td>0.013512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.780627</td>\n",
       "      <td>0.004924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.717158</td>\n",
       "      <td>0.012260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.636606</td>\n",
       "      <td>0.011612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.020861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.674107</td>\n",
       "      <td>0.019188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.650667  0.013512\n",
       "Accuracy_train  0.780627  0.004924\n",
       "F1 Score        0.717158  0.012260\n",
       "Precision       0.636606  0.011612\n",
       "Recall          0.840000  0.020861\n",
       "Roc_auc         0.674107  0.019188"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b9338635-62a3-4fe9-912b-6acc77ef994a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-07-16 15:52:57,634]\u001b[0m A new study created in memory with name: no-name-326cd61c-d045-4060-aa5b-c2efe9b72f3f\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:58,904]\u001b[0m Trial 0 finished with value: 0.6422857142857142 and parameters: {'logreg_c': 0.3177840006884068, 'l1_ratio': 0.7482920440979423, 'max_iter': 100}. Best is trial 0 with value: 0.6422857142857142.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:58,929]\u001b[0m Trial 1 finished with value: 0.6323809523809524 and parameters: {'logreg_c': 0.0651621545821569, 'l1_ratio': 0.23208030173540176, 'max_iter': 275}. Best is trial 0 with value: 0.6422857142857142.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:58,951]\u001b[0m Trial 2 finished with value: 0.5856190476190476 and parameters: {'logreg_c': 0.013108749615263334, 'l1_ratio': 0.411004654338743, 'max_iter': 854}. Best is trial 0 with value: 0.6422857142857142.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:58,974]\u001b[0m Trial 3 finished with value: 0.658952380952381 and parameters: {'logreg_c': 1.7096232052870346, 'l1_ratio': 0.4772750629629653, 'max_iter': 1402}. Best is trial 3 with value: 0.658952380952381.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:58,996]\u001b[0m Trial 4 finished with value: 0.5955238095238096 and parameters: {'logreg_c': 0.016854407828169382, 'l1_ratio': 0.8903056927518509, 'max_iter': 152}. Best is trial 3 with value: 0.658952380952381.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,018]\u001b[0m Trial 5 finished with value: 0.6846666666666666 and parameters: {'logreg_c': 10.539137268289434, 'l1_ratio': 0.4755743221304143, 'max_iter': 1162}. Best is trial 5 with value: 0.6846666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,040]\u001b[0m Trial 6 finished with value: 0.557142857142857 and parameters: {'logreg_c': 0.006955392321661603, 'l1_ratio': 0.2782913401763909, 'max_iter': 1622}. Best is trial 5 with value: 0.6846666666666666.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,062]\u001b[0m Trial 7 finished with value: 0.7016190476190477 and parameters: {'logreg_c': 645.0144652189372, 'l1_ratio': 0.38208176034331853, 'max_iter': 1416}. Best is trial 7 with value: 0.7016190476190477.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,086]\u001b[0m Trial 8 finished with value: 0.6917142857142856 and parameters: {'logreg_c': 181.27374779133626, 'l1_ratio': 0.9051459971534626, 'max_iter': 261}. Best is trial 7 with value: 0.7016190476190477.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,109]\u001b[0m Trial 9 finished with value: 0.5202857142857142 and parameters: {'logreg_c': 0.001715255021408637, 'l1_ratio': 0.25284737760811204, 'max_iter': 1769}. Best is trial 7 with value: 0.7016190476190477.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,136]\u001b[0m Trial 10 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 912.2207863473513, 'l1_ratio': 0.6623431782063935, 'max_iter': 845}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,163]\u001b[0m Trial 11 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 843.2126062012233, 'l1_ratio': 0.6695176814297372, 'max_iter': 776}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,189]\u001b[0m Trial 12 finished with value: 0.6890476190476191 and parameters: {'logreg_c': 53.42250642297136, 'l1_ratio': 0.6683480033773077, 'max_iter': 704}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,215]\u001b[0m Trial 13 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 864.9174342698016, 'l1_ratio': 0.6610161473279021, 'max_iter': 662}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,242]\u001b[0m Trial 14 finished with value: 0.6876190476190476 and parameters: {'logreg_c': 16.80539776862483, 'l1_ratio': 0.770493267181325, 'max_iter': 972}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,269]\u001b[0m Trial 15 finished with value: 0.6888571428571431 and parameters: {'logreg_c': 77.51797518020979, 'l1_ratio': 0.6069499392552298, 'max_iter': 593}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,297]\u001b[0m Trial 16 finished with value: 0.678857142857143 and parameters: {'logreg_c': 6.812685015040122, 'l1_ratio': 0.7970837068885077, 'max_iter': 1181}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,323]\u001b[0m Trial 17 finished with value: 0.6888571428571427 and parameters: {'logreg_c': 268.69501319250026, 'l1_ratio': 0.5218186467521022, 'max_iter': 480}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,348]\u001b[0m Trial 18 finished with value: 0.6577142857142857 and parameters: {'logreg_c': 1.3786253842892806, 'l1_ratio': 0.10853917129565638, 'max_iter': 457}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,376]\u001b[0m Trial 19 finished with value: 0.6862857142857145 and parameters: {'logreg_c': 49.52608566949532, 'l1_ratio': 0.9976109927145953, 'max_iter': 1008}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,402]\u001b[0m Trial 20 finished with value: 0.6888571428571428 and parameters: {'logreg_c': 261.39765080282456, 'l1_ratio': 0.5944802472863524, 'max_iter': 814}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,429]\u001b[0m Trial 21 finished with value: 0.7031428571428572 and parameters: {'logreg_c': 759.1478575995593, 'l1_ratio': 0.6940757004876918, 'max_iter': 693}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,457]\u001b[0m Trial 22 finished with value: 0.7016190476190477 and parameters: {'logreg_c': 691.9798172917368, 'l1_ratio': 0.649961791348065, 'max_iter': 485}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,484]\u001b[0m Trial 23 finished with value: 0.7018095238095238 and parameters: {'logreg_c': 994.284783831103, 'l1_ratio': 0.8420511141698004, 'max_iter': 851}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,510]\u001b[0m Trial 24 finished with value: 0.6874285714285713 and parameters: {'logreg_c': 116.99026685380682, 'l1_ratio': 0.7072856219234651, 'max_iter': 1230}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,537]\u001b[0m Trial 25 finished with value: 0.6892380952380954 and parameters: {'logreg_c': 35.75986379041051, 'l1_ratio': 0.5671530919384115, 'max_iter': 730}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,563]\u001b[0m Trial 26 finished with value: 0.6744761904761905 and parameters: {'logreg_c': 3.6100999722324016, 'l1_ratio': 0.641416286831473, 'max_iter': 1003}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,589]\u001b[0m Trial 27 finished with value: 0.6365714285714286 and parameters: {'logreg_c': 0.21372768992091656, 'l1_ratio': 0.5402112320586119, 'max_iter': 576}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,616]\u001b[0m Trial 28 finished with value: 0.6902857142857143 and parameters: {'logreg_c': 289.4376833631736, 'l1_ratio': 0.7480100274050329, 'max_iter': 357}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,642]\u001b[0m Trial 29 finished with value: 0.6821904761904762 and parameters: {'logreg_c': 26.631905824468024, 'l1_ratio': 0.723138468169708, 'max_iter': 1338}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,670]\u001b[0m Trial 30 finished with value: 0.6420952380952382 and parameters: {'logreg_c': 0.3873858111080394, 'l1_ratio': 0.8302692636235095, 'max_iter': 909}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,699]\u001b[0m Trial 31 finished with value: 0.696 and parameters: {'logreg_c': 433.6029563234908, 'l1_ratio': 0.703764969900987, 'max_iter': 674}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,726]\u001b[0m Trial 32 finished with value: 0.6845714285714286 and parameters: {'logreg_c': 125.31560564851311, 'l1_ratio': 0.6645045800258389, 'max_iter': 743}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,756]\u001b[0m Trial 33 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 902.4620309875683, 'l1_ratio': 0.6074249751646598, 'max_iter': 607}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,784]\u001b[0m Trial 34 finished with value: 0.6873333333333334 and parameters: {'logreg_c': 149.26411314707147, 'l1_ratio': 0.4814099677039107, 'max_iter': 568}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,811]\u001b[0m Trial 35 finished with value: 0.7018095238095238 and parameters: {'logreg_c': 991.430048149724, 'l1_ratio': 0.4086937528627142, 'max_iter': 366}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,838]\u001b[0m Trial 36 finished with value: 0.696 and parameters: {'logreg_c': 397.94747906081244, 'l1_ratio': 0.6165237726820284, 'max_iter': 1072}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,865]\u001b[0m Trial 37 finished with value: 0.6874285714285715 and parameters: {'logreg_c': 85.03729049337146, 'l1_ratio': 0.5051856166850125, 'max_iter': 176}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,892]\u001b[0m Trial 38 finished with value: 0.6338095238095238 and parameters: {'logreg_c': 0.04710532421976166, 'l1_ratio': 0.44210998102809645, 'max_iter': 807}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,919]\u001b[0m Trial 39 finished with value: 0.6931428571428572 and parameters: {'logreg_c': 359.98816269209834, 'l1_ratio': 0.5614465788930126, 'max_iter': 1085}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,945]\u001b[0m Trial 40 finished with value: 0.6890476190476191 and parameters: {'logreg_c': 19.605915253680823, 'l1_ratio': 0.36998429957648815, 'max_iter': 909}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:52:59,972]\u001b[0m Trial 41 finished with value: 0.6974285714285715 and parameters: {'logreg_c': 586.7309576241138, 'l1_ratio': 0.6876302689244391, 'max_iter': 650}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,001]\u001b[0m Trial 42 finished with value: 0.7018095238095238 and parameters: {'logreg_c': 960.9336854620046, 'l1_ratio': 0.7589385189093267, 'max_iter': 805}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,031]\u001b[0m Trial 43 finished with value: 0.6917142857142856 and parameters: {'logreg_c': 203.7108516823381, 'l1_ratio': 0.6050298110131368, 'max_iter': 372}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,060]\u001b[0m Trial 44 finished with value: 0.696 and parameters: {'logreg_c': 404.4484945006352, 'l1_ratio': 0.8001535334101337, 'max_iter': 602}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,089]\u001b[0m Trial 45 finished with value: 0.6974285714285715 and parameters: {'logreg_c': 586.0513989709891, 'l1_ratio': 0.9115413611515424, 'max_iter': 1996}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,118]\u001b[0m Trial 46 finished with value: 0.6917142857142856 and parameters: {'logreg_c': 176.57588489896972, 'l1_ratio': 0.7332825120755947, 'max_iter': 924}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,147]\u001b[0m Trial 47 finished with value: 0.516095238095238 and parameters: {'logreg_c': 0.0012787292031029157, 'l1_ratio': 0.639060012376742, 'max_iter': 510}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,175]\u001b[0m Trial 48 finished with value: 0.6877142857142858 and parameters: {'logreg_c': 63.96390529174601, 'l1_ratio': 0.57662949733279, 'max_iter': 739}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,203]\u001b[0m Trial 49 finished with value: 0.6974285714285715 and parameters: {'logreg_c': 578.4820507133139, 'l1_ratio': 0.6865196305023439, 'max_iter': 267}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,230]\u001b[0m Trial 50 finished with value: 0.6860952380952381 and parameters: {'logreg_c': 8.385187950453478, 'l1_ratio': 0.862295721180618, 'max_iter': 670}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,257]\u001b[0m Trial 51 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 832.8699047979767, 'l1_ratio': 0.9443722913211522, 'max_iter': 843}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,286]\u001b[0m Trial 52 finished with value: 0.6874285714285713 and parameters: {'logreg_c': 271.77805273377345, 'l1_ratio': 0.9231718759892552, 'max_iter': 799}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,315]\u001b[0m Trial 53 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 870.7978283867163, 'l1_ratio': 0.6285148676217605, 'max_iter': 892}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,341]\u001b[0m Trial 54 finished with value: 0.5359999999999999 and parameters: {'logreg_c': 0.0036568829011891252, 'l1_ratio': 0.9512396006975474, 'max_iter': 1142}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,369]\u001b[0m Trial 55 finished with value: 0.6859999999999999 and parameters: {'logreg_c': 119.2383830651957, 'l1_ratio': 0.1686719489986555, 'max_iter': 944}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,396]\u001b[0m Trial 56 finished with value: 0.6945714285714285 and parameters: {'logreg_c': 480.9784545357148, 'l1_ratio': 0.7897634693581976, 'max_iter': 854}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,425]\u001b[0m Trial 57 finished with value: 0.6917142857142856 and parameters: {'logreg_c': 226.29288309520166, 'l1_ratio': 0.5176841405039894, 'max_iter': 534}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,453]\u001b[0m Trial 58 finished with value: 0.7018095238095238 and parameters: {'logreg_c': 962.5926128913575, 'l1_ratio': 0.2976045845188322, 'max_iter': 439}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,480]\u001b[0m Trial 59 finished with value: 0.6877142857142857 and parameters: {'logreg_c': 42.043042070305546, 'l1_ratio': 0.6208608766805833, 'max_iter': 1012}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,508]\u001b[0m Trial 60 finished with value: 0.6902857142857143 and parameters: {'logreg_c': 293.0067886429266, 'l1_ratio': 0.9972823858168096, 'max_iter': 1262}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,537]\u001b[0m Trial 61 finished with value: 0.7016190476190477 and parameters: {'logreg_c': 699.0168432198744, 'l1_ratio': 0.5837183772011929, 'max_iter': 615}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,565]\u001b[0m Trial 62 finished with value: 0.7018095238095238 and parameters: {'logreg_c': 978.2016233007297, 'l1_ratio': 0.6620858620791422, 'max_iter': 743}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,592]\u001b[0m Trial 63 finished with value: 0.6873333333333334 and parameters: {'logreg_c': 90.54486710680634, 'l1_ratio': 0.5450598988794523, 'max_iter': 865}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,623]\u001b[0m Trial 64 finished with value: 0.6974285714285715 and parameters: {'logreg_c': 583.2149571795027, 'l1_ratio': 0.7160756887870361, 'max_iter': 763}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,651]\u001b[0m Trial 65 finished with value: 0.6945714285714285 and parameters: {'logreg_c': 375.8017906086745, 'l1_ratio': 0.6787919838425219, 'max_iter': 631}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,679]\u001b[0m Trial 66 finished with value: 0.6917142857142856 and parameters: {'logreg_c': 188.12922408836891, 'l1_ratio': 0.6465142704647608, 'max_iter': 689}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,707]\u001b[0m Trial 67 finished with value: 0.675809523809524 and parameters: {'logreg_c': 2.9496605099585125, 'l1_ratio': 0.7645996006944501, 'max_iter': 860}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,735]\u001b[0m Trial 68 finished with value: 0.7016190476190477 and parameters: {'logreg_c': 644.1560284129099, 'l1_ratio': 0.6353596268202752, 'max_iter': 1013}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,762]\u001b[0m Trial 69 finished with value: 0.647904761904762 and parameters: {'logreg_c': 0.5245673265812799, 'l1_ratio': 0.8291424203118494, 'max_iter': 431}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,794]\u001b[0m Trial 70 finished with value: 0.6902857142857143 and parameters: {'logreg_c': 329.4070188023448, 'l1_ratio': 0.739349763775759, 'max_iter': 1546}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,823]\u001b[0m Trial 71 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 870.0885145749858, 'l1_ratio': 0.7675084244569488, 'max_iter': 796}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,855]\u001b[0m Trial 72 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 778.2575311165746, 'l1_ratio': 0.6951629503494239, 'max_iter': 954}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,882]\u001b[0m Trial 73 finished with value: 0.696 and parameters: {'logreg_c': 442.63308761299, 'l1_ratio': 0.5935568612134046, 'max_iter': 1089}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,912]\u001b[0m Trial 74 finished with value: 0.7001904761904763 and parameters: {'logreg_c': 665.6593146708336, 'l1_ratio': 0.7119372256889996, 'max_iter': 951}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,941]\u001b[0m Trial 75 finished with value: 0.6887619047619047 and parameters: {'logreg_c': 152.5428645897848, 'l1_ratio': 0.8766417471385834, 'max_iter': 791}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,968]\u001b[0m Trial 76 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 0.10664554801941503, 'l1_ratio': 0.6665869786260287, 'max_iter': 888}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:00,996]\u001b[0m Trial 77 finished with value: 0.7018095238095238 and parameters: {'logreg_c': 976.170601596124, 'l1_ratio': 0.7971269986574467, 'max_iter': 970}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,022]\u001b[0m Trial 78 finished with value: 0.6902857142857143 and parameters: {'logreg_c': 240.27470454754823, 'l1_ratio': 0.622992742567599, 'max_iter': 702}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,052]\u001b[0m Trial 79 finished with value: 0.696 and parameters: {'logreg_c': 440.2572839585082, 'l1_ratio': 0.77790507217502, 'max_iter': 1137}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,081]\u001b[0m Trial 80 finished with value: 0.6901904761904762 and parameters: {'logreg_c': 96.74586763896534, 'l1_ratio': 0.5453987994778594, 'max_iter': 821}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,110]\u001b[0m Trial 81 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 852.1542633843692, 'l1_ratio': 0.6857426322411954, 'max_iter': 535}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,139]\u001b[0m Trial 82 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 832.4634503432111, 'l1_ratio': 0.6968282509520956, 'max_iter': 584}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,168]\u001b[0m Trial 83 finished with value: 0.696 and parameters: {'logreg_c': 515.4469891597857, 'l1_ratio': 0.7356845168380834, 'max_iter': 560}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,198]\u001b[0m Trial 84 finished with value: 0.7031428571428572 and parameters: {'logreg_c': 756.9555170471222, 'l1_ratio': 0.7026102072543227, 'max_iter': 494}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,226]\u001b[0m Trial 85 finished with value: 0.6931428571428572 and parameters: {'logreg_c': 362.8544767790542, 'l1_ratio': 0.6797266675040227, 'max_iter': 395}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,254]\u001b[0m Trial 86 finished with value: 0.7016190476190477 and parameters: {'logreg_c': 679.529063634052, 'l1_ratio': 0.610697159194145, 'max_iter': 635}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,284]\u001b[0m Trial 87 finished with value: 0.6902857142857143 and parameters: {'logreg_c': 292.4297337026197, 'l1_ratio': 0.6368621224190697, 'max_iter': 713}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,313]\u001b[0m Trial 88 finished with value: 0.6917142857142856 and parameters: {'logreg_c': 204.11397334759508, 'l1_ratio': 0.5701303494369576, 'max_iter': 327}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,341]\u001b[0m Trial 89 finished with value: 0.6945714285714285 and parameters: {'logreg_c': 473.79012243229494, 'l1_ratio': 0.6589628791416318, 'max_iter': 531}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,370]\u001b[0m Trial 90 finished with value: 0.6874285714285713 and parameters: {'logreg_c': 146.6592536900826, 'l1_ratio': 0.7154333563590555, 'max_iter': 592}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,399]\u001b[0m Trial 91 finished with value: 0.7031428571428572 and parameters: {'logreg_c': 749.8086897758585, 'l1_ratio': 0.6941884169400266, 'max_iter': 777}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,426]\u001b[0m Trial 92 finished with value: 0.7018095238095238 and parameters: {'logreg_c': 992.2957896376967, 'l1_ratio': 0.5947556869588314, 'max_iter': 834}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,454]\u001b[0m Trial 93 finished with value: 0.7031428571428572 and parameters: {'logreg_c': 739.4580011831634, 'l1_ratio': 0.7558318446481275, 'max_iter': 675}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,482]\u001b[0m Trial 94 finished with value: 0.6945714285714285 and parameters: {'logreg_c': 497.95430807240075, 'l1_ratio': 0.6590171584394634, 'max_iter': 880}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,512]\u001b[0m Trial 95 finished with value: 0.6888571428571428 and parameters: {'logreg_c': 258.4102597263621, 'l1_ratio': 0.6283233924316362, 'max_iter': 750}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,552]\u001b[0m Trial 96 finished with value: 0.696 and parameters: {'logreg_c': 387.4750266064402, 'l1_ratio': 0.8222565240534406, 'max_iter': 1055}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,581]\u001b[0m Trial 97 finished with value: 0.6112380952380952 and parameters: {'logreg_c': 0.02559567892770348, 'l1_ratio': 0.6838629505744083, 'max_iter': 912}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,611]\u001b[0m Trial 98 finished with value: 0.7045714285714287 and parameters: {'logreg_c': 785.5302994357671, 'l1_ratio': 0.7340704897756487, 'max_iter': 969}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:53:01,643]\u001b[0m Trial 99 finished with value: 0.696 and parameters: {'logreg_c': 549.7506503565595, 'l1_ratio': 0.7281083551484597, 'max_iter': 993}. Best is trial 10 with value: 0.7045714285714287.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    logreg_c = trial.suggest_float(\"logreg_c\", 1e-3,  1e3, log=True)\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\",0.1,1,log=False) \n",
    "    #penalty = trial.suggest_categorical(\"penalty\",['l1','l2'])\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100,2000)\n",
    "    model =LogisticRegression(C=logreg_c,\n",
    "                              max_iter=max_iter,\n",
    "                              l1_ratio=l1_ratio,\n",
    "                              solver='liblinear',random_state=1)\n",
    "    \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd09afa2-c7ae-4f28-acb3-9a0c06e89504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'logreg_c': 912.2207863473513, 'l1_ratio': 0.6623431782063935, 'max_iter': 845}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=LogisticRegression(C=study.best_params['logreg_c'],\n",
    "                              max_iter=study.best_params['max_iter'],\n",
    "                              l1_ratio=study.best_params['l1_ratio'],\n",
    "                              solver='liblinear',\n",
    "                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "18f98263-5b44-4a58-ad76-bd2288347abf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.704571</td>\n",
       "      <td>0.017062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.977462</td>\n",
       "      <td>0.001576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.732877</td>\n",
       "      <td>0.017040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.717844</td>\n",
       "      <td>0.019672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.772857</td>\n",
       "      <td>0.023079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.741633</td>\n",
       "      <td>0.019542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.704571  0.017062\n",
       "Accuracy_train  0.977462  0.001576\n",
       "F1 Score        0.732877  0.017040\n",
       "Precision       0.717844  0.019672\n",
       "Recall          0.772857  0.023079\n",
       "Roc_auc         0.741633  0.019542"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model3\n",
    "Model3=Model_results(clf,X,y,Cv_model)\n",
    "Model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0844c7c8-f6b1-410a-b253-879aedd14e57",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d9c4731c-bbfe-47eb-bb7f-7d8b2909ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ffdd2f35-ec29-4ef8-8309-6a84646f4539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.648000</td>\n",
       "      <td>0.014535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.675897</td>\n",
       "      <td>0.015271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.667234</td>\n",
       "      <td>0.013517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.705714</td>\n",
       "      <td>0.022288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.750370</td>\n",
       "      <td>0.016641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.648000  0.014535\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.675897  0.015271\n",
       "Precision       0.667234  0.013517\n",
       "Recall          0.705714  0.022288\n",
       "Roc_auc         0.750370  0.016641"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6f4b6d2b-6cf0-4f44-8bd5-02b5d1377add",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-07-16 15:56:54,231]\u001b[0m A new study created in memory with name: no-name-397aace5-9070-4ba3-9f87-e83b04a36515\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:56:57,299]\u001b[0m Trial 0 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 594, 'max_depth': 16, 'max_features': 20, 'min_impurity_decrease': 2.724415914984484}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:56:59,796]\u001b[0m Trial 1 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 481, 'max_depth': 15, 'max_features': 16, 'min_impurity_decrease': 4.4588650039103985}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:04,677]\u001b[0m Trial 2 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 968, 'max_depth': 11, 'max_features': 25, 'min_impurity_decrease': 2.644474598764522}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:07,668]\u001b[0m Trial 3 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 611, 'max_depth': 19, 'max_features': 6, 'min_impurity_decrease': 0.43564649850770354}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:08,298]\u001b[0m Trial 4 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 118, 'max_depth': 18, 'max_features': 25, 'min_impurity_decrease': 4.3500607412340955}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:13,143]\u001b[0m Trial 5 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 981, 'max_depth': 17, 'max_features': 16, 'min_impurity_decrease': 3.902645881432277}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:14,187]\u001b[0m Trial 6 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 206, 'max_depth': 15, 'max_features': 8, 'min_impurity_decrease': 4.7233445852479194}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:17,018]\u001b[0m Trial 7 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 570, 'max_depth': 11, 'max_features': 11, 'min_impurity_decrease': 3.871168447171083}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:19,518]\u001b[0m Trial 8 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 510, 'max_depth': 14, 'max_features': 5, 'min_impurity_decrease': 3.0881774853793855}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:22,772]\u001b[0m Trial 9 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 651, 'max_depth': 14, 'max_features': 29, 'min_impurity_decrease': 3.409101495517417}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:26,692]\u001b[0m Trial 10 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 790, 'max_depth': 7, 'max_features': 21, 'min_impurity_decrease': 1.5046577569438309}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:28,672]\u001b[0m Trial 11 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 397, 'max_depth': 20, 'max_features': 16, 'min_impurity_decrease': 1.9887909510549393}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:30,444]\u001b[0m Trial 12 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 353, 'max_depth': 16, 'max_features': 20, 'min_impurity_decrease': 0.7029374955194359}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:32,505]\u001b[0m Trial 13 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 411, 'max_depth': 12, 'max_features': 12, 'min_impurity_decrease': 4.931175650908394}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:36,201]\u001b[0m Trial 14 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 743, 'max_depth': 7, 'max_features': 19, 'min_impurity_decrease': 1.5253121430442067}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:39,899]\u001b[0m Trial 15 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 757, 'max_depth': 13, 'max_features': 12, 'min_impurity_decrease': 2.668111844473928}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:41,373]\u001b[0m Trial 16 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 286, 'max_depth': 9, 'max_features': 24, 'min_impurity_decrease': 3.4814210246729473}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:45,376]\u001b[0m Trial 17 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 807, 'max_depth': 5, 'max_features': 19, 'min_impurity_decrease': 1.4307627258174422}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:48,913]\u001b[0m Trial 18 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 714, 'max_depth': 13, 'max_features': 12, 'min_impurity_decrease': 2.574069984147719}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:50,422]\u001b[0m Trial 19 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 291, 'max_depth': 8, 'max_features': 24, 'min_impurity_decrease': 3.3144569973486893}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:54,865]\u001b[0m Trial 20 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 871, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 1.0528796896599464}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:57:58,322]\u001b[0m Trial 21 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 670, 'max_depth': 10, 'max_features': 14, 'min_impurity_decrease': 2.1272690676619246}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:00,897]\u001b[0m Trial 22 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 502, 'max_depth': 9, 'max_features': 23, 'min_impurity_decrease': 3.0212243946132653}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:05,254]\u001b[0m Trial 23 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 871, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.8924229302747374}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:08,691]\u001b[0m Trial 24 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 673, 'max_depth': 11, 'max_features': 28, 'min_impurity_decrease': 2.183958261339378}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:11,482]\u001b[0m Trial 25 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 548, 'max_depth': 9, 'max_features': 22, 'min_impurity_decrease': 2.0762642939740097}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:16,577]\u001b[0m Trial 26 finished with value: 0.6297142857142858 and parameters: {'n_estimators': 859, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.05016303754931206}. Best is trial 26 with value: 0.6297142857142858.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:21,911]\u001b[0m Trial 27 finished with value: 0.6408571428571429 and parameters: {'n_estimators': 889, 'max_depth': 6, 'max_features': 27, 'min_impurity_decrease': 0.008573433661244079}. Best is trial 27 with value: 0.6408571428571429.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:27,290]\u001b[0m Trial 28 finished with value: 0.6409523809523808 and parameters: {'n_estimators': 901, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.04449057244021093}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:32,628]\u001b[0m Trial 29 finished with value: 0.623904761904762 and parameters: {'n_estimators': 918, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.06568686852538762}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:36,920]\u001b[0m Trial 30 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 855, 'max_depth': 6, 'max_features': 26, 'min_impurity_decrease': 0.3179551086113602}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:41,694]\u001b[0m Trial 31 finished with value: 0.582952380952381 and parameters: {'n_estimators': 937, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.18490386200494244}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:47,254]\u001b[0m Trial 32 finished with value: 0.6408571428571429 and parameters: {'n_estimators': 913, 'max_depth': 8, 'max_features': 27, 'min_impurity_decrease': 0.004905793398417582}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:52,817]\u001b[0m Trial 33 finished with value: 0.6394285714285715 and parameters: {'n_estimators': 917, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.0020653183224317627}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:58:57,837]\u001b[0m Trial 34 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 989, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 0.6299600959488392}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:02,490]\u001b[0m Trial 35 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 921, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.46338942427376184}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:06,508]\u001b[0m Trial 36 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 808, 'max_depth': 6, 'max_features': 25, 'min_impurity_decrease': 1.0293121320437253}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:11,185]\u001b[0m Trial 37 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 936, 'max_depth': 8, 'max_features': 26, 'min_impurity_decrease': 0.46004002169673}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:16,185]\u001b[0m Trial 38 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 998, 'max_depth': 10, 'max_features': 23, 'min_impurity_decrease': 0.6726799043596892}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:20,679]\u001b[0m Trial 39 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 890, 'max_depth': 6, 'max_features': 28, 'min_impurity_decrease': 1.2782349340929975}. Best is trial 28 with value: 0.6409523809523808.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:25,571]\u001b[0m Trial 40 finished with value: 0.6423809523809524 and parameters: {'n_estimators': 831, 'max_depth': 5, 'max_features': 25, 'min_impurity_decrease': 0.024123964900357054}. Best is trial 40 with value: 0.6423809523809524.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:29,764]\u001b[0m Trial 41 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 830, 'max_depth': 5, 'max_features': 26, 'min_impurity_decrease': 0.3173934212231931}. Best is trial 40 with value: 0.6423809523809524.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:35,354]\u001b[0m Trial 42 finished with value: 0.6437142857142857 and parameters: {'n_estimators': 946, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.021986609957610637}. Best is trial 42 with value: 0.6437142857142857.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:40,245]\u001b[0m Trial 43 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 950, 'max_depth': 8, 'max_features': 28, 'min_impurity_decrease': 0.26560344494046556}. Best is trial 42 with value: 0.6437142857142857.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:44,113]\u001b[0m Trial 44 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 760, 'max_depth': 5, 'max_features': 24, 'min_impurity_decrease': 0.8244094328287189}. Best is trial 42 with value: 0.6437142857142857.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:49,131]\u001b[0m Trial 45 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 966, 'max_depth': 6, 'max_features': 26, 'min_impurity_decrease': 0.5478322323285495}. Best is trial 42 with value: 0.6437142857142857.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:53,542]\u001b[0m Trial 46 finished with value: 0.5422857142857143 and parameters: {'n_estimators': 832, 'max_depth': 7, 'max_features': 28, 'min_impurity_decrease': 0.20917372146842544}. Best is trial 42 with value: 0.6437142857142857.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 15:59:58,072]\u001b[0m Trial 47 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 906, 'max_depth': 10, 'max_features': 21, 'min_impurity_decrease': 1.2183260072780886}. Best is trial 42 with value: 0.6437142857142857.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:01,128]\u001b[0m Trial 48 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 610, 'max_depth': 8, 'max_features': 25, 'min_impurity_decrease': 1.7554372932180566}. Best is trial 42 with value: 0.6437142857142857.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:01,721]\u001b[0m Trial 49 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 107, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 0.40928959633065826}. Best is trial 42 with value: 0.6437142857142857.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:05,977]\u001b[0m Trial 50 finished with value: 0.6464761904761904 and parameters: {'n_estimators': 718, 'max_depth': 9, 'max_features': 23, 'min_impurity_decrease': 0.001610466494046827}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:10,206]\u001b[0m Trial 51 finished with value: 0.6424761904761904 and parameters: {'n_estimators': 718, 'max_depth': 9, 'max_features': 27, 'min_impurity_decrease': 0.021600304156371312}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:13,798]\u001b[0m Trial 52 finished with value: 0.5366666666666667 and parameters: {'n_estimators': 705, 'max_depth': 7, 'max_features': 23, 'min_impurity_decrease': 0.21718286729614245}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:17,815]\u001b[0m Trial 53 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 787, 'max_depth': 9, 'max_features': 17, 'min_impurity_decrease': 0.794390338977914}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:21,553]\u001b[0m Trial 54 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 727, 'max_depth': 11, 'max_features': 25, 'min_impurity_decrease': 0.4761630397774994}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:24,855]\u001b[0m Trial 55 finished with value: 0.5492380952380952 and parameters: {'n_estimators': 642, 'max_depth': 10, 'max_features': 9, 'min_impurity_decrease': 0.16993016747151118}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:28,797]\u001b[0m Trial 56 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 771, 'max_depth': 12, 'max_features': 24, 'min_impurity_decrease': 4.063447563238093}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:32,958]\u001b[0m Trial 57 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 824, 'max_depth': 9, 'max_features': 22, 'min_impurity_decrease': 0.9243590958239454}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:35,919]\u001b[0m Trial 58 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 574, 'max_depth': 6, 'max_features': 29, 'min_impurity_decrease': 0.7260708810977635}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:39,494]\u001b[0m Trial 59 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 685, 'max_depth': 18, 'max_features': 30, 'min_impurity_decrease': 0.39012979924955926}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:44,015]\u001b[0m Trial 60 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 885, 'max_depth': 5, 'max_features': 20, 'min_impurity_decrease': 0.5651290611433561}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:49,855]\u001b[0m Trial 61 finished with value: 0.6422857142857142 and parameters: {'n_estimators': 966, 'max_depth': 8, 'max_features': 27, 'min_impurity_decrease': 0.008295143745487604}. Best is trial 50 with value: 0.6464761904761904.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:00:55,527]\u001b[0m Trial 62 finished with value: 0.647904761904762 and parameters: {'n_estimators': 964, 'max_depth': 7, 'max_features': 26, 'min_impurity_decrease': 0.021851274789025533}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:00,666]\u001b[0m Trial 63 finished with value: 0.6053333333333333 and parameters: {'n_estimators': 965, 'max_depth': 7, 'max_features': 26, 'min_impurity_decrease': 0.1672439571027447}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:05,797]\u001b[0m Trial 64 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 1000, 'max_depth': 8, 'max_features': 28, 'min_impurity_decrease': 0.3336824046151028}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:10,329]\u001b[0m Trial 65 finished with value: 0.5867619047619047 and parameters: {'n_estimators': 844, 'max_depth': 9, 'max_features': 24, 'min_impurity_decrease': 0.14229839029148225}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:16,157]\u001b[0m Trial 66 finished with value: 0.6451428571428572 and parameters: {'n_estimators': 962, 'max_depth': 7, 'max_features': 25, 'min_impurity_decrease': 0.017377079970615474}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:21,036]\u001b[0m Trial 67 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 974, 'max_depth': 9, 'max_features': 22, 'min_impurity_decrease': 0.3151574869388898}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:25,817]\u001b[0m Trial 68 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 952, 'max_depth': 15, 'max_features': 25, 'min_impurity_decrease': 0.5612712122477459}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:30,304]\u001b[0m Trial 69 finished with value: 0.5858095238095238 and parameters: {'n_estimators': 864, 'max_depth': 10, 'max_features': 26, 'min_impurity_decrease': 0.1832870377976945}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:33,612]\u001b[0m Trial 70 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 644, 'max_depth': 8, 'max_features': 23, 'min_impurity_decrease': 0.6649747564097053}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:39,001]\u001b[0m Trial 71 finished with value: 0.6062857142857143 and parameters: {'n_estimators': 937, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.08842468301846634}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:44,360]\u001b[0m Trial 72 finished with value: 0.6366666666666667 and parameters: {'n_estimators': 904, 'max_depth': 7, 'max_features': 25, 'min_impurity_decrease': 0.031033180901609704}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:49,337]\u001b[0m Trial 73 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 970, 'max_depth': 6, 'max_features': 28, 'min_impurity_decrease': 0.29418012025376244}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:53,459]\u001b[0m Trial 74 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 809, 'max_depth': 6, 'max_features': 27, 'min_impurity_decrease': 0.4414048006058832}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:57,930]\u001b[0m Trial 75 finished with value: 0.6422857142857142 and parameters: {'n_estimators': 738, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.0033439550181828892}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:01:58,855]\u001b[0m Trial 76 finished with value: 0.5838095238095238 and parameters: {'n_estimators': 159, 'max_depth': 8, 'max_features': 29, 'min_impurity_decrease': 0.13096740093988923}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:03,317]\u001b[0m Trial 77 finished with value: 0.6380952380952382 and parameters: {'n_estimators': 736, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.01386683942391737}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:05,692]\u001b[0m Trial 78 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 457, 'max_depth': 7, 'max_features': 14, 'min_impurity_decrease': 0.30955539999212284}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:09,780]\u001b[0m Trial 79 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 785, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 0.49895092045118317}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:13,459]\u001b[0m Trial 80 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 707, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 3.0026029089989303}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:19,091]\u001b[0m Trial 81 finished with value: 0.6438095238095239 and parameters: {'n_estimators': 930, 'max_depth': 7, 'max_features': 26, 'min_impurity_decrease': 0.006541662218678964}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:23,991]\u001b[0m Trial 82 finished with value: 0.5745714285714285 and parameters: {'n_estimators': 940, 'max_depth': 8, 'max_features': 26, 'min_impurity_decrease': 0.1887247003163086}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:28,523]\u001b[0m Trial 83 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 877, 'max_depth': 7, 'max_features': 25, 'min_impurity_decrease': 0.36442546779785145}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:34,421]\u001b[0m Trial 84 finished with value: 0.640952380952381 and parameters: {'n_estimators': 984, 'max_depth': 6, 'max_features': 24, 'min_impurity_decrease': 0.003980085758029522}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:38,246]\u001b[0m Trial 85 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 752, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.2402042606068197}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:42,997]\u001b[0m Trial 86 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 923, 'max_depth': 5, 'max_features': 30, 'min_impurity_decrease': 0.5966509778040109}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:46,832]\u001b[0m Trial 87 finished with value: 0.5979047619047619 and parameters: {'n_estimators': 682, 'max_depth': 8, 'max_features': 26, 'min_impurity_decrease': 0.10857274018274565}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:51,212]\u001b[0m Trial 88 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 845, 'max_depth': 8, 'max_features': 28, 'min_impurity_decrease': 0.3857124590792891}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:02:55,982]\u001b[0m Trial 89 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 956, 'max_depth': 20, 'max_features': 29, 'min_impurity_decrease': 2.3494285961951196}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:00,434]\u001b[0m Trial 90 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 894, 'max_depth': 6, 'max_features': 25, 'min_impurity_decrease': 4.493469463689619}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:06,222]\u001b[0m Trial 91 finished with value: 0.6408571428571429 and parameters: {'n_estimators': 981, 'max_depth': 6, 'max_features': 23, 'min_impurity_decrease': 0.00784502876272759}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:11,233]\u001b[0m Trial 92 finished with value: 0.5937142857142857 and parameters: {'n_estimators': 929, 'max_depth': 7, 'max_features': 24, 'min_impurity_decrease': 0.11537163507846038}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:16,365]\u001b[0m Trial 93 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 999, 'max_depth': 5, 'max_features': 27, 'min_impurity_decrease': 0.2609562932238807}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:21,861]\u001b[0m Trial 94 finished with value: 0.5978095238095238 and parameters: {'n_estimators': 980, 'max_depth': 6, 'max_features': 21, 'min_impurity_decrease': 0.09542344968767952}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:27,672]\u001b[0m Trial 95 finished with value: 0.6422857142857145 and parameters: {'n_estimators': 954, 'max_depth': 7, 'max_features': 24, 'min_impurity_decrease': 0.005989315541029012}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:32,536]\u001b[0m Trial 96 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 944, 'max_depth': 7, 'max_features': 22, 'min_impurity_decrease': 0.44203533666600137}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:36,925]\u001b[0m Trial 97 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 862, 'max_depth': 9, 'max_features': 26, 'min_impurity_decrease': 0.2502919239720993}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:41,046]\u001b[0m Trial 98 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 814, 'max_depth': 8, 'max_features': 24, 'min_impurity_decrease': 0.7538441699059766}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:03:45,859]\u001b[0m Trial 99 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 959, 'max_depth': 7, 'max_features': 6, 'min_impurity_decrease': 3.742156249512944}. Best is trial 62 with value: 0.647904761904762.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\",100,1000,1) \n",
    "    max_depth = trial.suggest_int(\"max_depth\",5,20,1)\n",
    "    max_features = trial.suggest_int(\"max_features\",5,30,1)\n",
    "    min_impurity_decrease = trial.suggest_float(\"min_impurity_decrease\",0,5,log=False) \n",
    "    model = RandomForestClassifier(n_estimators = n_estimators\n",
    "              ,max_depth = max_depth\n",
    "              ,max_features = max_features\n",
    "              ,min_impurity_decrease = min_impurity_decrease\n",
    "              ,random_state=1\n",
    "              ,verbose=False\n",
    "              ,n_jobs=8)\n",
    "\n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    " \n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3ea7fc26-11ae-497b-ab4a-949cd12a43a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'n_estimators': 964, 'max_depth': 7, 'max_features': 26, 'min_impurity_decrease': 0.021851274789025533}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=RandomForestClassifier(n_estimators = study.best_params['n_estimators']\n",
    "              ,max_depth = study.best_params['max_depth']\n",
    "              ,max_features = study.best_params['max_features']\n",
    "              ,min_impurity_decrease = study.best_params['min_impurity_decrease']\n",
    "              ,random_state=0\n",
    "              ,verbose=False\n",
    "              ,n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "90cca67e-3df8-43ff-8b96-2c9bec87ee43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.642286</td>\n",
       "      <td>0.014344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.977112</td>\n",
       "      <td>0.001592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.676622</td>\n",
       "      <td>0.015551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.656327</td>\n",
       "      <td>0.013317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.722143</td>\n",
       "      <td>0.023702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.726709</td>\n",
       "      <td>0.016915</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.642286  0.014344\n",
       "Accuracy_train  0.977112  0.001592\n",
       "F1 Score        0.676622  0.015551\n",
       "Precision       0.656327  0.013317\n",
       "Recall          0.722143  0.023702\n",
       "Roc_auc         0.726709  0.016915"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model3\n",
    "Model3=Model_results(clf,X,y,Cv_model)\n",
    "Model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115be659-b066-4abf-85ac-59fccd101ad4",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3ff2324c-d316-43ae-96dc-e58f404273a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=xgb.XGBClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "03979633-d8bb-44e7-a9d7-b7050c4c32c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.655429</td>\n",
       "      <td>0.015002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.686321</td>\n",
       "      <td>0.015577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.668029</td>\n",
       "      <td>0.013729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.725357</td>\n",
       "      <td>0.023386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.729966</td>\n",
       "      <td>0.016739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.655429  0.015002\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.686321  0.015577\n",
       "Precision       0.668029  0.013729\n",
       "Recall          0.725357  0.023386\n",
       "Roc_auc         0.729966  0.016739"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c6e5e42b-3688-4c6f-aa87-cf0fe5b1ff05",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-07-16 16:10:42,300]\u001b[0m A new study created in memory with name: no-name-29e377fc-0da8-4377-82fe-a6dcb88e12fa\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:45,400]\u001b[0m Trial 0 finished with value: 0.6860952380952381 and parameters: {'lambda': 0.15676677195506075, 'alpha': 0.7257005721594281, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.0801, 'n_estimators': 664}. Best is trial 0 with value: 0.6860952380952381.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:46,328]\u001b[0m Trial 1 finished with value: 0.6321904761904762 and parameters: {'lambda': 0.0562793204741517, 'alpha': 3.6905577292137624, 'colsample_bytree': 1.0, 'subsample': 0.6000000000000001, 'learning_rate': 0.1551, 'n_estimators': 552}. Best is trial 0 with value: 0.6860952380952381.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:47,501]\u001b[0m Trial 2 finished with value: 0.5508571428571428 and parameters: {'lambda': 0.18714500686240676, 'alpha': 5.039489598671215, 'colsample_bytree': 0.3, 'subsample': 0.4, 'learning_rate': 0.0001, 'n_estimators': 841}. Best is trial 0 with value: 0.6860952380952381.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:48,845]\u001b[0m Trial 3 finished with value: 0.6127619047619047 and parameters: {'lambda': 1.2960656597279736, 'alpha': 3.0202896401586674, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.0901, 'n_estimators': 792}. Best is trial 0 with value: 0.6860952380952381.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:49,674]\u001b[0m Trial 4 finished with value: 0.6494285714285714 and parameters: {'lambda': 0.0029723346443356552, 'alpha': 0.3628140404024381, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.10010000000000001, 'n_estimators': 444}. Best is trial 0 with value: 0.6860952380952381.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:51,641]\u001b[0m Trial 5 finished with value: 0.6096190476190477 and parameters: {'lambda': 0.011434638743472197, 'alpha': 1.2500712230836255, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.0001, 'n_estimators': 637}. Best is trial 0 with value: 0.6860952380952381.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:52,723]\u001b[0m Trial 6 finished with value: 0.6718095238095237 and parameters: {'lambda': 0.2807908107885728, 'alpha': 0.2935864364395359, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.07010000000000001, 'n_estimators': 465}. Best is trial 0 with value: 0.6860952380952381.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:53,276]\u001b[0m Trial 7 finished with value: 0.6536190476190477 and parameters: {'lambda': 0.6173405204074314, 'alpha': 0.0017414134181586202, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 172}. Best is trial 0 with value: 0.6860952380952381.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:53,617]\u001b[0m Trial 8 finished with value: 0.6888571428571428 and parameters: {'lambda': 0.01826894228153233, 'alpha': 0.028499883436971588, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 147}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:54,198]\u001b[0m Trial 9 finished with value: 0.674952380952381 and parameters: {'lambda': 0.006847105576684045, 'alpha': 0.004418125737902547, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.0901, 'n_estimators': 282}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:54,402]\u001b[0m Trial 10 finished with value: 0.6401904761904761 and parameters: {'lambda': 5.790132527437195, 'alpha': 0.025043968115100592, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.1901, 'n_estimators': 57}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:55,953]\u001b[0m Trial 11 finished with value: 0.6876190476190476 and parameters: {'lambda': 0.03849595149411625, 'alpha': 0.043812245346487144, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1401, 'n_estimators': 981}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:57,410]\u001b[0m Trial 12 finished with value: 0.6719999999999999 and parameters: {'lambda': 0.02491353701899208, 'alpha': 0.03404799435343261, 'colsample_bytree': 0.7, 'subsample': 0.6000000000000001, 'learning_rate': 0.14509999999999998, 'n_estimators': 955}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:58,063]\u001b[0m Trial 13 finished with value: 0.6763809523809523 and parameters: {'lambda': 0.0017532782487939581, 'alpha': 0.038990832725213885, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.1951, 'n_estimators': 320}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:59,634]\u001b[0m Trial 14 finished with value: 0.6648571428571428 and parameters: {'lambda': 0.040970272286798115, 'alpha': 0.00873369887896242, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.6000000000000001, 'learning_rate': 0.1551, 'n_estimators': 999}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:10:59,845]\u001b[0m Trial 15 finished with value: 0.6663809523809523 and parameters: {'lambda': 0.013898680104378393, 'alpha': 0.12035495424746019, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.1301, 'n_estimators': 53}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:00,481]\u001b[0m Trial 16 finished with value: 0.6775238095238096 and parameters: {'lambda': 0.004931808890264009, 'alpha': 0.10224323740750124, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.1701, 'n_estimators': 325}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:01,761]\u001b[0m Trial 17 finished with value: 0.6803809523809524 and parameters: {'lambda': 0.0010007385532741818, 'alpha': 0.010465517693177323, 'colsample_bytree': 0.7, 'subsample': 0.5, 'learning_rate': 0.1251, 'n_estimators': 790}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:02,209]\u001b[0m Trial 18 finished with value: 0.6706666666666667 and parameters: {'lambda': 0.08876422204232874, 'alpha': 0.0033125271935794923, 'colsample_bytree': 0.5, 'subsample': 0.9, 'learning_rate': 0.1751, 'n_estimators': 189}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:03,308]\u001b[0m Trial 19 finished with value: 0.6658095238095236 and parameters: {'lambda': 0.03335097345021328, 'alpha': 0.015347938492279205, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.6000000000000001, 'learning_rate': 0.11510000000000001, 'n_estimators': 625}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:04,853]\u001b[0m Trial 20 finished with value: 0.6864761904761905 and parameters: {'lambda': 1.9921501245794788, 'alpha': 0.08978642156944139, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.1751, 'n_estimators': 882}. Best is trial 8 with value: 0.6888571428571428.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:06,762]\u001b[0m Trial 21 finished with value: 0.6890476190476191 and parameters: {'lambda': 7.477510087767987, 'alpha': 0.08131403452109104, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.1701, 'n_estimators': 887}. Best is trial 21 with value: 0.6890476190476191.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:08,655]\u001b[0m Trial 22 finished with value: 0.6916190476190475 and parameters: {'lambda': 7.1170005989437515, 'alpha': 0.06000828352878482, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 931}. Best is trial 22 with value: 0.6916190476190475.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:10,211]\u001b[0m Trial 23 finished with value: 0.6904761904761907 and parameters: {'lambda': 8.923747309195527, 'alpha': 0.20695580789945744, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.1951, 'n_estimators': 720}. Best is trial 22 with value: 0.6916190476190475.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:12,011]\u001b[0m Trial 24 finished with value: 0.664952380952381 and parameters: {'lambda': 9.101588677764802, 'alpha': 0.23860036755178277, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1701, 'n_estimators': 721}. Best is trial 22 with value: 0.6916190476190475.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:13,834]\u001b[0m Trial 25 finished with value: 0.6750476190476189 and parameters: {'lambda': 3.081695262982042, 'alpha': 0.16092969851094782, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.9, 'learning_rate': 0.18009999999999998, 'n_estimators': 897}. Best is trial 22 with value: 0.6916190476190475.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:15,434]\u001b[0m Trial 26 finished with value: 0.6748571428571428 and parameters: {'lambda': 4.382762538433666, 'alpha': 0.68162943225459, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.1551, 'n_estimators': 739}. Best is trial 22 with value: 0.6916190476190475.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:17,031]\u001b[0m Trial 27 finished with value: 0.6936190476190476 and parameters: {'lambda': 1.131718667956299, 'alpha': 0.057767161258496784, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.1851, 'n_estimators': 904}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:18,514]\u001b[0m Trial 28 finished with value: 0.6906666666666667 and parameters: {'lambda': 0.9094563684685327, 'alpha': 0.05776009577974223, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1851, 'n_estimators': 791}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:20,287]\u001b[0m Trial 29 finished with value: 0.6635238095238095 and parameters: {'lambda': 0.6903325225331443, 'alpha': 0.6060297428867552, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.040100000000000004, 'n_estimators': 927}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:21,801]\u001b[0m Trial 30 finished with value: 0.6862857142857142 and parameters: {'lambda': 0.8499140794520063, 'alpha': 0.06533334275930723, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.1601, 'n_estimators': 830}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:23,218]\u001b[0m Trial 31 finished with value: 0.6877142857142857 and parameters: {'lambda': 3.2526754397405266, 'alpha': 0.22686536843182395, 'colsample_bytree': 0.3, 'subsample': 0.8, 'learning_rate': 0.1851, 'n_estimators': 712}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:24,287]\u001b[0m Trial 32 finished with value: 0.6676190476190476 and parameters: {'lambda': 1.6701263908544497, 'alpha': 1.5642267664603327, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 572}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:25,720]\u001b[0m Trial 33 finished with value: 0.6877142857142857 and parameters: {'lambda': 0.3785100556469342, 'alpha': 0.06040749180175992, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1651, 'n_estimators': 814}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:26,793]\u001b[0m Trial 34 finished with value: 0.5985714285714285 and parameters: {'lambda': 2.4516991579592506, 'alpha': 8.388507692348886, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.1851, 'n_estimators': 748}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:28,242]\u001b[0m Trial 35 finished with value: 0.684952380952381 and parameters: {'lambda': 0.11805255774975507, 'alpha': 0.013249260777934811, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.14509999999999998, 'n_estimators': 855}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:30,339]\u001b[0m Trial 36 finished with value: 0.6650476190476189 and parameters: {'lambda': 9.97588520282575, 'alpha': 0.3890427422183178, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.1851, 'n_estimators': 934}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:31,743]\u001b[0m Trial 37 finished with value: 0.6833333333333332 and parameters: {'lambda': 5.044372827497579, 'alpha': 0.13534786529042558, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1601, 'n_estimators': 676}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:32,862]\u001b[0m Trial 38 finished with value: 0.6835238095238095 and parameters: {'lambda': 1.157786350789831, 'alpha': 0.017914450971484315, 'colsample_bytree': 0.3, 'subsample': 0.8, 'learning_rate': 0.18009999999999998, 'n_estimators': 588}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:33,956]\u001b[0m Trial 39 finished with value: 0.6548571428571429 and parameters: {'lambda': 0.3044846568314907, 'alpha': 1.1461969081287056, 'colsample_bytree': 0.8, 'subsample': 0.7000000000000001, 'learning_rate': 0.0751, 'n_estimators': 517}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:35,338]\u001b[0m Trial 40 finished with value: 0.6731428571428572 and parameters: {'lambda': 0.4364559577785702, 'alpha': 0.006359606665092805, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.6000000000000001, 'learning_rate': 0.1101, 'n_estimators': 779}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:37,124]\u001b[0m Trial 41 finished with value: 0.6876190476190477 and parameters: {'lambda': 6.330987596129652, 'alpha': 0.06745255773739045, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.1951, 'n_estimators': 873}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:38,781]\u001b[0m Trial 42 finished with value: 0.6764761904761905 and parameters: {'lambda': 1.2352096300732605, 'alpha': 0.17918320596738224, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.1701, 'n_estimators': 897}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:40,648]\u001b[0m Trial 43 finished with value: 0.6805714285714285 and parameters: {'lambda': 7.41433091163272, 'alpha': 0.42494949487944755, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.1851, 'n_estimators': 835}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:42,267]\u001b[0m Trial 44 finished with value: 0.6762857142857142 and parameters: {'lambda': 3.9678939308620604, 'alpha': 0.0492145816250789, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.0601, 'n_estimators': 673}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:43,856]\u001b[0m Trial 45 finished with value: 0.6852380952380953 and parameters: {'lambda': 0.19255624030406235, 'alpha': 0.08335122425357532, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.15009999999999998, 'n_estimators': 946}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:45,493]\u001b[0m Trial 46 finished with value: 0.6835238095238095 and parameters: {'lambda': 2.303030642249525, 'alpha': 0.04101469461981874, 'colsample_bytree': 0.8, 'subsample': 0.9, 'learning_rate': 0.1351, 'n_estimators': 769}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:47,467]\u001b[0m Trial 47 finished with value: 0.6891428571428573 and parameters: {'lambda': 5.826630611794677, 'alpha': 0.02302323628607111, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.1951, 'n_estimators': 982}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:49,071]\u001b[0m Trial 48 finished with value: 0.6902857142857144 and parameters: {'lambda': 0.8463937635169668, 'alpha': 0.024325309221026378, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 987}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:50,568]\u001b[0m Trial 49 finished with value: 0.6776190476190476 and parameters: {'lambda': 0.9225185145214695, 'alpha': 0.02652698575934916, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.1851, 'n_estimators': 931}. Best is trial 27 with value: 0.6936190476190476.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:52,111]\u001b[0m Trial 50 finished with value: 0.6947619047619047 and parameters: {'lambda': 0.4990987074228772, 'alpha': 0.03830112719939052, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 967}. Best is trial 50 with value: 0.6947619047619047.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:53,733]\u001b[0m Trial 51 finished with value: 0.6945714285714285 and parameters: {'lambda': 0.4963669593327326, 'alpha': 0.020289335659759034, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 995}. Best is trial 50 with value: 0.6947619047619047.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:55,266]\u001b[0m Trial 52 finished with value: 0.6760952380952381 and parameters: {'lambda': 0.5633069005688511, 'alpha': 0.03596698506688077, 'colsample_bytree': 0.7, 'subsample': 0.6000000000000001, 'learning_rate': 0.18009999999999998, 'n_estimators': 959}. Best is trial 50 with value: 0.6947619047619047.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:56,705]\u001b[0m Trial 53 finished with value: 0.671904761904762 and parameters: {'lambda': 0.19651690268082508, 'alpha': 0.05244814893320909, 'colsample_bytree': 0.8, 'subsample': 0.7000000000000001, 'learning_rate': 0.1901, 'n_estimators': 909}. Best is trial 50 with value: 0.6947619047619047.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:11:59,227]\u001b[0m Trial 54 finished with value: 0.6661904761904762 and parameters: {'lambda': 0.06396243093917878, 'alpha': 0.10801589092281892, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.0101, 'n_estimators': 998}. Best is trial 50 with value: 0.6947619047619047.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:00,678]\u001b[0m Trial 55 finished with value: 0.6833333333333332 and parameters: {'lambda': 1.6364115768228884, 'alpha': 0.0010430520155247799, 'colsample_bytree': 0.8, 'subsample': 0.7000000000000001, 'learning_rate': 0.1751, 'n_estimators': 819}. Best is trial 50 with value: 0.6947619047619047.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:02,037]\u001b[0m Trial 56 finished with value: 0.6775238095238093 and parameters: {'lambda': 0.4715802004541249, 'alpha': 0.009996646921175292, 'colsample_bytree': 0.7, 'subsample': 0.6000000000000001, 'learning_rate': 0.1651, 'n_estimators': 860}. Best is trial 50 with value: 0.6947619047619047.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:02,820]\u001b[0m Trial 57 finished with value: 0.7003809523809525 and parameters: {'lambda': 0.262599937791131, 'alpha': 0.003699202202542825, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1901, 'n_estimators': 433}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:03,553]\u001b[0m Trial 58 finished with value: 0.6735238095238095 and parameters: {'lambda': 0.3024568329709052, 'alpha': 0.002986975347273344, 'colsample_bytree': 0.3, 'subsample': 0.7000000000000001, 'learning_rate': 0.1751, 'n_estimators': 389}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:04,065]\u001b[0m Trial 59 finished with value: 0.6872380952380952 and parameters: {'lambda': 0.24579394170544847, 'alpha': 0.007193582926557286, 'colsample_bytree': 0.4, 'subsample': 0.6000000000000001, 'learning_rate': 0.1901, 'n_estimators': 247}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:04,852]\u001b[0m Trial 60 finished with value: 0.6775238095238096 and parameters: {'lambda': 0.12107782352334365, 'alpha': 0.005461440782184388, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.1651, 'n_estimators': 441}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:05,764]\u001b[0m Trial 61 finished with value: 0.6847619047619048 and parameters: {'lambda': 0.6560545627872607, 'alpha': 0.1798794122586093, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.1901, 'n_estimators': 479}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:07,260]\u001b[0m Trial 62 finished with value: 0.6932380952380953 and parameters: {'lambda': 0.15006556246586417, 'alpha': 0.002135220462981736, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.18009999999999998, 'n_estimators': 964}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:07,931]\u001b[0m Trial 63 finished with value: 0.6789523809523811 and parameters: {'lambda': 0.0693744547585457, 'alpha': 0.0023101004896810507, 'colsample_bytree': 0.4, 'subsample': 0.6000000000000001, 'learning_rate': 0.18009999999999998, 'n_estimators': 390}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:09,438]\u001b[0m Trial 64 finished with value: 0.6890476190476191 and parameters: {'lambda': 0.15136796205300812, 'alpha': 0.0013634883516294072, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.18009999999999998, 'n_estimators': 942}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:10,906]\u001b[0m Trial 65 finished with value: 0.6976190476190476 and parameters: {'lambda': 0.3615746164168785, 'alpha': 0.0035732548408648015, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1901, 'n_estimators': 963}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:12,394]\u001b[0m Trial 66 finished with value: 0.6819047619047619 and parameters: {'lambda': 0.08856211303721895, 'alpha': 0.0045689445682281365, 'colsample_bytree': 0.5, 'subsample': 0.6000000000000001, 'learning_rate': 0.1951, 'n_estimators': 966}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:13,832]\u001b[0m Trial 67 finished with value: 0.6947619047619048 and parameters: {'lambda': 0.23096687760448822, 'alpha': 0.0019490631031809174, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1751, 'n_estimators': 922}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:15,314]\u001b[0m Trial 68 finished with value: 0.687904761904762 and parameters: {'lambda': 0.23042194563416393, 'alpha': 0.001983584949342901, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1701, 'n_estimators': 910}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:16,879]\u001b[0m Trial 69 finished with value: 0.6922857142857143 and parameters: {'lambda': 0.3610561360471142, 'alpha': 0.003104691033794679, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1601, 'n_estimators': 969}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:18,414]\u001b[0m Trial 70 finished with value: 0.6863809523809523 and parameters: {'lambda': 0.15905875179586212, 'alpha': 0.0038577939947848664, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.0951, 'n_estimators': 879}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:19,961]\u001b[0m Trial 71 finished with value: 0.6978095238095239 and parameters: {'lambda': 0.3160337547524513, 'alpha': 0.0015505932964089992, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1601, 'n_estimators': 962}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:21,555]\u001b[0m Trial 72 finished with value: 0.682190476190476 and parameters: {'lambda': 0.5192661529568264, 'alpha': 0.0014885612105456996, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1751, 'n_estimators': 1000}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:22,997]\u001b[0m Trial 73 finished with value: 0.6875238095238096 and parameters: {'lambda': 0.3564727421133158, 'alpha': 0.002733605599764328, 'colsample_bytree': 0.4, 'subsample': 0.6000000000000001, 'learning_rate': 0.15009999999999998, 'n_estimators': 912}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:24,467]\u001b[0m Trial 74 finished with value: 0.6946666666666667 and parameters: {'lambda': 0.2537065290359778, 'alpha': 0.0017254799058472256, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1901, 'n_estimators': 964}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:25,827]\u001b[0m Trial 75 finished with value: 0.6848571428571428 and parameters: {'lambda': 0.6812288869803005, 'alpha': 0.0010135621271614892, 'colsample_bytree': 0.3, 'subsample': 0.7000000000000001, 'learning_rate': 0.1901, 'n_estimators': 857}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:27,292]\u001b[0m Trial 76 finished with value: 0.6976190476190477 and parameters: {'lambda': 0.2245677162726368, 'alpha': 0.001336496224345571, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.1851, 'n_estimators': 946}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:28,776]\u001b[0m Trial 77 finished with value: 0.6836190476190477 and parameters: {'lambda': 0.262059400289009, 'alpha': 0.0016868794201915286, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.1751, 'n_estimators': 948}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:29,525]\u001b[0m Trial 78 finished with value: 0.683047619047619 and parameters: {'lambda': 0.3920991480082741, 'alpha': 0.0014062667631798541, 'colsample_bytree': 0.4, 'subsample': 0.6000000000000001, 'learning_rate': 0.1901, 'n_estimators': 407}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:31,001]\u001b[0m Trial 79 finished with value: 0.6821904761904762 and parameters: {'lambda': 0.10854706502408418, 'alpha': 0.004493542746504537, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.1651, 'n_estimators': 926}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:31,252]\u001b[0m Trial 80 finished with value: 0.6687619047619048 and parameters: {'lambda': 0.04988891483021678, 'alpha': 0.002664791499236946, 'colsample_bytree': 0.3, 'subsample': 0.6000000000000001, 'learning_rate': 0.1901, 'n_estimators': 85}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:32,683]\u001b[0m Trial 81 finished with value: 0.6905714285714286 and parameters: {'lambda': 0.31789430593571527, 'alpha': 0.0011079949252558155, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.18009999999999998, 'n_estimators': 890}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:34,183]\u001b[0m Trial 82 finished with value: 0.6832380952380953 and parameters: {'lambda': 0.195546777528461, 'alpha': 0.0019278930208085025, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1851, 'n_estimators': 976}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:35,735]\u001b[0m Trial 83 finished with value: 0.692095238095238 and parameters: {'lambda': 0.49404987418180585, 'alpha': 0.00351654009146966, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.1701, 'n_estimators': 921}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:37,261]\u001b[0m Trial 84 finished with value: 0.6892380952380952 and parameters: {'lambda': 0.23060275217306994, 'alpha': 0.014212503640297042, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.1851, 'n_estimators': 954}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:38,904]\u001b[0m Trial 85 finished with value: 0.6836190476190476 and parameters: {'lambda': 1.0951798416452485, 'alpha': 0.007628315828377501, 'colsample_bytree': 0.3, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 998}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:39,839]\u001b[0m Trial 86 finished with value: 0.6878095238095239 and parameters: {'lambda': 0.7586481710391817, 'alpha': 0.005440269956726898, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1901, 'n_estimators': 520}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:40,598]\u001b[0m Trial 87 finished with value: 0.6808571428571427 and parameters: {'lambda': 0.46055917607259317, 'alpha': 0.019590504361946464, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.1201, 'n_estimators': 325}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:42,143]\u001b[0m Trial 88 finished with value: 0.6836190476190476 and parameters: {'lambda': 1.6792886396399889, 'alpha': 0.001223105128341601, 'colsample_bytree': 0.3, 'subsample': 0.7000000000000001, 'learning_rate': 0.1601, 'n_estimators': 878}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:43,539]\u001b[0m Trial 89 finished with value: 0.694857142857143 and parameters: {'lambda': 0.28396151258648933, 'alpha': 0.0016801596944289526, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.1751, 'n_estimators': 843}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:44,943]\u001b[0m Trial 90 finished with value: 0.691904761904762 and parameters: {'lambda': 0.1327177841270286, 'alpha': 0.0015695776146960802, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.1751, 'n_estimators': 846}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:46,399]\u001b[0m Trial 91 finished with value: 0.6849523809523811 and parameters: {'lambda': 0.5908712136300067, 'alpha': 0.0024234161422423415, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.18009999999999998, 'n_estimators': 902}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:47,957]\u001b[0m Trial 92 finished with value: 0.6832380952380952 and parameters: {'lambda': 0.28293686869467694, 'alpha': 0.031804093947294484, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.1851, 'n_estimators': 935}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:49,265]\u001b[0m Trial 93 finished with value: 0.6893333333333334 and parameters: {'lambda': 0.17932836217125914, 'alpha': 0.0019157569119970922, 'colsample_bytree': 0.5, 'subsample': 0.8, 'learning_rate': 0.1701, 'n_estimators': 806}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:50,860]\u001b[0m Trial 94 finished with value: 0.6877142857142857 and parameters: {'lambda': 0.4194483692778804, 'alpha': 0.0035723116611271984, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1551, 'n_estimators': 966}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:52,432]\u001b[0m Trial 95 finished with value: 0.6776190476190476 and parameters: {'lambda': 0.09580239047829117, 'alpha': 0.0013011850089699243, 'colsample_bytree': 0.7, 'subsample': 0.8, 'learning_rate': 0.1901, 'n_estimators': 981}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:53,785]\u001b[0m Trial 96 finished with value: 0.689047619047619 and parameters: {'lambda': 0.2177951248222654, 'alpha': 0.0017666795571469195, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 868}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:55,212]\u001b[0m Trial 97 finished with value: 0.6904761904761905 and parameters: {'lambda': 0.2804000566575748, 'alpha': 0.00226765980204561, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.1851, 'n_estimators': 895}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:56,652]\u001b[0m Trial 98 finished with value: 0.6759999999999999 and parameters: {'lambda': 0.32443015975289125, 'alpha': 0.011028615907895056, 'colsample_bytree': 0.4, 'subsample': 0.6000000000000001, 'learning_rate': 0.1751, 'n_estimators': 947}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 16:12:57,657]\u001b[0m Trial 99 finished with value: 0.6903809523809525 and parameters: {'lambda': 1.0236051230929204, 'alpha': 0.005427345855484707, 'colsample_bytree': 1.0, 'subsample': 0.7000000000000001, 'learning_rate': 0.1051, 'n_estimators': 494}. Best is trial 57 with value: 0.7003809523809525.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3,1.0,step=0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0, step=0.1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.2, step=0.005),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\",50,1000,1)\n",
    "        #'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param,random_state=1,n_jobs=8)\n",
    "\n",
    " \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b2be4542-3f0e-438f-8e41-c67d96410fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'lambda': 0.262599937791131, 'alpha': 0.003699202202542825, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1901, 'n_estimators': 433}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=xgb.XGBClassifier(alpha = study.best_params['alpha']\n",
    "              ,colsample_bytree = study.best_params['colsample_bytree']\n",
    "              ,subsample = study.best_params['subsample']\n",
    "              ,n_estimators = study.best_params['n_estimators']\n",
    "              ,learning_rate= study.best_params['learning_rate'], n_jobs=8\n",
    "              ,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d0687ace-1f1a-44a0-9329-fc50b681ff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.672095</td>\n",
       "      <td>0.016234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.707779</td>\n",
       "      <td>0.014983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.682626</td>\n",
       "      <td>0.015459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.753571</td>\n",
       "      <td>0.021375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.742781</td>\n",
       "      <td>0.016832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.672095  0.016234\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.707779  0.014983\n",
       "Precision       0.682626  0.015459\n",
       "Recall          0.753571  0.021375\n",
       "Roc_auc         0.742781  0.016832"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model3\n",
    "Model3=Model_results(clf,X,y,Cv_model)\n",
    "Model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76792ca-81e9-46cc-92a5-67a45a48325b",
   "metadata": {},
   "source": [
    "## DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d5432c97-68f0-438c-8542-f94169d397d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7582336-4b65-43e9-ae74-b92f34625039",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.676190</td>\n",
       "      <td>0.013877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.695797</td>\n",
       "      <td>0.014569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.704834</td>\n",
       "      <td>0.014202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.711429</td>\n",
       "      <td>0.022511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.679413</td>\n",
       "      <td>0.013723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.676190  0.013877\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.695797  0.014569\n",
       "Precision       0.704834  0.014202\n",
       "Recall          0.711429  0.022511\n",
       "Roc_auc         0.679413  0.013723"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f476645a-fbde-4223-9f82-6e7e9c2e15c3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-07-16 16:10:34,257]\u001b[0m A new study created in memory with name: no-name-666cf9ef-4cd3-41cd-ab9e-038eb13e1623\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:35,977]\u001b[0m Trial 0 finished with value: 0.637047619047619 and parameters: {'max_depth': 4, 'max_features': 17, 'min_samples_split': 16}. Best is trial 0 with value: 0.637047619047619.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,013]\u001b[0m Trial 1 finished with value: 0.6478095238095238 and parameters: {'max_depth': 4, 'max_features': 14, 'min_samples_split': 17}. Best is trial 1 with value: 0.6478095238095238.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,038]\u001b[0m Trial 2 finished with value: 0.5834285714285714 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 25}. Best is trial 1 with value: 0.6478095238095238.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,063]\u001b[0m Trial 3 finished with value: 0.6131428571428572 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 14}. Best is trial 1 with value: 0.6478095238095238.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,088]\u001b[0m Trial 4 finished with value: 0.6483809523809524 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 4 with value: 0.6483809523809524.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,111]\u001b[0m Trial 5 finished with value: 0.6574285714285715 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 21}. Best is trial 5 with value: 0.6574285714285715.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,134]\u001b[0m Trial 6 finished with value: 0.5862857142857143 and parameters: {'max_depth': 5, 'max_features': 19, 'min_samples_split': 25}. Best is trial 5 with value: 0.6574285714285715.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,158]\u001b[0m Trial 7 finished with value: 0.6168571428571429 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 20}. Best is trial 5 with value: 0.6574285714285715.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,181]\u001b[0m Trial 8 finished with value: 0.6484761904761904 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 5}. Best is trial 5 with value: 0.6574285714285715.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,203]\u001b[0m Trial 9 finished with value: 0.6384761904761905 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 11}. Best is trial 5 with value: 0.6574285714285715.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,231]\u001b[0m Trial 10 finished with value: 0.6735238095238094 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 9}. Best is trial 10 with value: 0.6735238095238094.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,259]\u001b[0m Trial 11 finished with value: 0.6734285714285713 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 10}. Best is trial 10 with value: 0.6735238095238094.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,288]\u001b[0m Trial 12 finished with value: 0.6735238095238094 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 9}. Best is trial 10 with value: 0.6735238095238094.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,317]\u001b[0m Trial 13 finished with value: 0.6461904761904762 and parameters: {'max_depth': 3, 'max_features': 12, 'min_samples_split': 7}. Best is trial 10 with value: 0.6735238095238094.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,346]\u001b[0m Trial 14 finished with value: 0.6419047619047619 and parameters: {'max_depth': 3, 'max_features': 12, 'min_samples_split': 9}. Best is trial 10 with value: 0.6735238095238094.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,373]\u001b[0m Trial 15 finished with value: 0.6476190476190478 and parameters: {'max_depth': 3, 'max_features': 12, 'min_samples_split': 2}. Best is trial 10 with value: 0.6735238095238094.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,403]\u001b[0m Trial 16 finished with value: 0.6216190476190476 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 12}. Best is trial 10 with value: 0.6735238095238094.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,434]\u001b[0m Trial 17 finished with value: 0.6349523809523809 and parameters: {'max_depth': 3, 'max_features': 13, 'min_samples_split': 7}. Best is trial 10 with value: 0.6735238095238094.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,463]\u001b[0m Trial 18 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 7}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,491]\u001b[0m Trial 19 finished with value: 0.6561904761904762 and parameters: {'max_depth': 4, 'max_features': 14, 'min_samples_split': 5}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,519]\u001b[0m Trial 20 finished with value: 0.6196190476190476 and parameters: {'max_depth': 4, 'max_features': 11, 'min_samples_split': 14}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,547]\u001b[0m Trial 21 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,574]\u001b[0m Trial 22 finished with value: 0.6242857142857143 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 7}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,603]\u001b[0m Trial 23 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 5}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,630]\u001b[0m Trial 24 finished with value: 0.6335238095238095 and parameters: {'max_depth': 3, 'max_features': 13, 'min_samples_split': 5}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,659]\u001b[0m Trial 25 finished with value: 0.6185714285714285 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 2}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,686]\u001b[0m Trial 26 finished with value: 0.6335238095238095 and parameters: {'max_depth': 3, 'max_features': 13, 'min_samples_split': 4}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,714]\u001b[0m Trial 27 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 7}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,743]\u001b[0m Trial 28 finished with value: 0.6452380952380952 and parameters: {'max_depth': 4, 'max_features': 12, 'min_samples_split': 12}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,772]\u001b[0m Trial 29 finished with value: 0.6172380952380953 and parameters: {'max_depth': 4, 'max_features': 11, 'min_samples_split': 6}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,799]\u001b[0m Trial 30 finished with value: 0.6285714285714286 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 16}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,823]\u001b[0m Trial 31 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,851]\u001b[0m Trial 32 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,877]\u001b[0m Trial 33 finished with value: 0.6242857142857143 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 9}. Best is trial 18 with value: 0.6778095238095239.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,905]\u001b[0m Trial 34 finished with value: 0.6795238095238095 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 4}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,932]\u001b[0m Trial 35 finished with value: 0.6679999999999999 and parameters: {'max_depth': 4, 'max_features': 16, 'min_samples_split': 3}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,962]\u001b[0m Trial 36 finished with value: 0.6444761904761905 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 11}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:36,990]\u001b[0m Trial 37 finished with value: 0.6637142857142857 and parameters: {'max_depth': 4, 'max_features': 16, 'min_samples_split': 4}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,017]\u001b[0m Trial 38 finished with value: 0.6404761904761905 and parameters: {'max_depth': 3, 'max_features': 14, 'min_samples_split': 15}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,044]\u001b[0m Trial 39 finished with value: 0.6059047619047618 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 18}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,071]\u001b[0m Trial 40 finished with value: 0.6620952380952381 and parameters: {'max_depth': 5, 'max_features': 20, 'min_samples_split': 4}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,100]\u001b[0m Trial 41 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 7}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,130]\u001b[0m Trial 42 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,158]\u001b[0m Trial 43 finished with value: 0.6465714285714285 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,185]\u001b[0m Trial 44 finished with value: 0.6185714285714285 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 3}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,212]\u001b[0m Trial 45 finished with value: 0.6422857142857142 and parameters: {'max_depth': 3, 'max_features': 15, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,239]\u001b[0m Trial 46 finished with value: 0.6734285714285713 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 10}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,265]\u001b[0m Trial 47 finished with value: 0.6461904761904762 and parameters: {'max_depth': 3, 'max_features': 12, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,293]\u001b[0m Trial 48 finished with value: 0.647904761904762 and parameters: {'max_depth': 3, 'max_features': 15, 'min_samples_split': 5}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,321]\u001b[0m Trial 49 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,348]\u001b[0m Trial 50 finished with value: 0.6743809523809523 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 3}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,375]\u001b[0m Trial 51 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,403]\u001b[0m Trial 52 finished with value: 0.6476190476190475 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 23}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,431]\u001b[0m Trial 53 finished with value: 0.6271428571428571 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 10}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,458]\u001b[0m Trial 54 finished with value: 0.6200000000000001 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,486]\u001b[0m Trial 55 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,515]\u001b[0m Trial 56 finished with value: 0.6734285714285713 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 11}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,543]\u001b[0m Trial 57 finished with value: 0.6342857142857142 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 12}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,571]\u001b[0m Trial 58 finished with value: 0.6171428571428571 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 4}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,600]\u001b[0m Trial 59 finished with value: 0.6747619047619048 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 13}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,629]\u001b[0m Trial 60 finished with value: 0.6335238095238095 and parameters: {'max_depth': 3, 'max_features': 13, 'min_samples_split': 5}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,656]\u001b[0m Trial 61 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 7}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,686]\u001b[0m Trial 62 finished with value: 0.6242857142857143 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 7}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,715]\u001b[0m Trial 63 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,745]\u001b[0m Trial 64 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,774]\u001b[0m Trial 65 finished with value: 0.6735238095238094 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 9}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,803]\u001b[0m Trial 66 finished with value: 0.6476190476190478 and parameters: {'max_depth': 3, 'max_features': 12, 'min_samples_split': 2}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,831]\u001b[0m Trial 67 finished with value: 0.6271428571428571 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 10}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,859]\u001b[0m Trial 68 finished with value: 0.6461904761904762 and parameters: {'max_depth': 3, 'max_features': 12, 'min_samples_split': 5}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,888]\u001b[0m Trial 69 finished with value: 0.6475238095238095 and parameters: {'max_depth': 3, 'max_features': 14, 'min_samples_split': 9}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,916]\u001b[0m Trial 70 finished with value: 0.6698095238095237 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,945]\u001b[0m Trial 71 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 7}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:37,973]\u001b[0m Trial 72 finished with value: 0.6242857142857143 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 7}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,001]\u001b[0m Trial 73 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,028]\u001b[0m Trial 74 finished with value: 0.6735238095238094 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 9}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,056]\u001b[0m Trial 75 finished with value: 0.6171428571428571 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 4}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,085]\u001b[0m Trial 76 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,114]\u001b[0m Trial 77 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,142]\u001b[0m Trial 78 finished with value: 0.6200000000000001 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,170]\u001b[0m Trial 79 finished with value: 0.62 and parameters: {'max_depth': 4, 'max_features': 11, 'min_samples_split': 3}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,198]\u001b[0m Trial 80 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 5}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,228]\u001b[0m Trial 81 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 5}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,258]\u001b[0m Trial 82 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 5}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,287]\u001b[0m Trial 83 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,316]\u001b[0m Trial 84 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 7}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,342]\u001b[0m Trial 85 finished with value: 0.6242857142857143 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,370]\u001b[0m Trial 86 finished with value: 0.6735238095238094 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 9}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,397]\u001b[0m Trial 87 finished with value: 0.6242857142857143 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 7}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,424]\u001b[0m Trial 88 finished with value: 0.6624761904761904 and parameters: {'max_depth': 4, 'max_features': 16, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,453]\u001b[0m Trial 89 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 7}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,479]\u001b[0m Trial 90 finished with value: 0.6171428571428571 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 4}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,507]\u001b[0m Trial 91 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 5}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,535]\u001b[0m Trial 92 finished with value: 0.6734285714285713 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 10}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,572]\u001b[0m Trial 93 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,602]\u001b[0m Trial 94 finished with value: 0.6401904761904761 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 11}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,634]\u001b[0m Trial 95 finished with value: 0.6778095238095239 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 6}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,662]\u001b[0m Trial 96 finished with value: 0.6763809523809523 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 4}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,691]\u001b[0m Trial 97 finished with value: 0.6305714285714286 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 18}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,719]\u001b[0m Trial 98 finished with value: 0.6667619047619047 and parameters: {'max_depth': 5, 'max_features': 10, 'min_samples_split': 3}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 16:10:38,749]\u001b[0m Trial 99 finished with value: 0.6465714285714285 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 8}. Best is trial 34 with value: 0.6795238095238095.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth',3,5,1),\n",
    "        'max_features' : trial.suggest_int(\"max_features\",10,20,1),\n",
    "        'min_samples_split':trial.suggest_int('min_samples_split',2,25,1)\n",
    "    }\n",
    "    model = DecisionTreeClassifier(**param,random_state=1)\n",
    "\n",
    " \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=12, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    " \n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c14f715a-25ca-4c4c-9e5e-01c8ad9f4ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'max_depth': 3, 'max_features': 16, 'min_samples_split': 4}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf =DecisionTreeClassifier(max_depth = study.best_params['max_depth']\n",
    "              ,max_features = study.best_params['max_features']\n",
    "              #,n_estimators = study.best_params['n_estimators']\n",
    "              #,learning_rate = study.best_params['learning_rate']\n",
    "              ,min_samples_split= study.best_params['min_samples_split']\n",
    "              ,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "00e42945-4207-4734-aab2-2cf0b193ec0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.679524</td>\n",
       "      <td>0.015475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.864461</td>\n",
       "      <td>0.006715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.716752</td>\n",
       "      <td>0.015407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.688970</td>\n",
       "      <td>0.016964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.776786</td>\n",
       "      <td>0.023767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.687844</td>\n",
       "      <td>0.018509</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.679524  0.015475\n",
       "Accuracy_train  0.864461  0.006715\n",
       "F1 Score        0.716752  0.015407\n",
       "Precision       0.688970  0.016964\n",
       "Recall          0.776786  0.023767\n",
       "Roc_auc         0.687844  0.018509"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model3\n",
    "Model3=Model_results(clf,X,y,Cv_model)\n",
    "Model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208fbfd4-b156-43fb-b7b8-5f38ee0e24ef",
   "metadata": {},
   "source": [
    "# MLREM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "45019f5e-5468-4d1b-a4de-8b8c86e33654",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data= pd.read_csv(\"./Results/col.csv\",header=0,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f6d91ba6-9049-4060-818a-8cf59d7a47b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>D/Dtr09</th>\n",
       "      <th>ZM1MulPer</th>\n",
       "      <th>ECC</th>\n",
       "      <th>CENT</th>\n",
       "      <th>SMTI</th>\n",
       "      <th>SMTIV</th>\n",
       "      <th>GMTIV</th>\n",
       "      <th>Wap</th>\n",
       "      <th>IDMT</th>\n",
       "      <th>...</th>\n",
       "      <th>ATSC5s</th>\n",
       "      <th>P_VSA_MR_3</th>\n",
       "      <th>P_VSA_ppp_ar</th>\n",
       "      <th>P_VSA_ppp_con</th>\n",
       "      <th>P_VSA_charge_2</th>\n",
       "      <th>SM15_EA(ed)</th>\n",
       "      <th>T(O..Br)</th>\n",
       "      <th>TPSA(Tot)</th>\n",
       "      <th>SAdon</th>\n",
       "      <th>Vx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ma_2019_A</th>\n",
       "      <td>267.28</td>\n",
       "      <td>66.871159</td>\n",
       "      <td>348.869542</td>\n",
       "      <td>135.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>2886.0</td>\n",
       "      <td>5576.0</td>\n",
       "      <td>10547.0</td>\n",
       "      <td>5729.0</td>\n",
       "      <td>4739.692713</td>\n",
       "      <td>...</td>\n",
       "      <td>90.763661</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>75.680233</td>\n",
       "      <td>63.202194</td>\n",
       "      <td>1.899093</td>\n",
       "      <td>36.892542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.54</td>\n",
       "      <td>160.947217</td>\n",
       "      <td>291.295681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_U</th>\n",
       "      <td>244.23</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>340.039426</td>\n",
       "      <td>118.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>4432.0</td>\n",
       "      <td>8734.0</td>\n",
       "      <td>2194.0</td>\n",
       "      <td>3416.557603</td>\n",
       "      <td>...</td>\n",
       "      <td>139.839496</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.373245</td>\n",
       "      <td>46.279992</td>\n",
       "      <td>36.205320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>124.78</td>\n",
       "      <td>146.060780</td>\n",
       "      <td>262.840532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_C</th>\n",
       "      <td>243.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>319.988367</td>\n",
       "      <td>118.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>4240.0</td>\n",
       "      <td>7972.0</td>\n",
       "      <td>2194.0</td>\n",
       "      <td>3416.557603</td>\n",
       "      <td>...</td>\n",
       "      <td>137.031903</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.057867</td>\n",
       "      <td>3.124314</td>\n",
       "      <td>36.205320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.83</td>\n",
       "      <td>160.947217</td>\n",
       "      <td>269.667774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_G</th>\n",
       "      <td>283.28</td>\n",
       "      <td>71.547747</td>\n",
       "      <td>387.546658</td>\n",
       "      <td>144.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>6542.0</td>\n",
       "      <td>12768.0</td>\n",
       "      <td>6578.0</td>\n",
       "      <td>5547.544286</td>\n",
       "      <td>...</td>\n",
       "      <td>117.471961</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>32.387883</td>\n",
       "      <td>80.922082</td>\n",
       "      <td>45.054770</td>\n",
       "      <td>36.939118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.51</td>\n",
       "      <td>178.957968</td>\n",
       "      <td>301.046512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_dA</th>\n",
       "      <td>251.28</td>\n",
       "      <td>63.146800</td>\n",
       "      <td>315.599992</td>\n",
       "      <td>128.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>2584.0</td>\n",
       "      <td>4822.0</td>\n",
       "      <td>8812.0</td>\n",
       "      <td>5068.0</td>\n",
       "      <td>4099.715332</td>\n",
       "      <td>...</td>\n",
       "      <td>52.221046</td>\n",
       "      <td>96.366574</td>\n",
       "      <td>75.680233</td>\n",
       "      <td>63.202194</td>\n",
       "      <td>1.899093</td>\n",
       "      <td>36.335611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.31</td>\n",
       "      <td>118.263874</td>\n",
       "      <td>281.544850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_ArabinoC</th>\n",
       "      <td>243.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>319.988367</td>\n",
       "      <td>118.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2084.0</td>\n",
       "      <td>4240.0</td>\n",
       "      <td>7972.0</td>\n",
       "      <td>2194.0</td>\n",
       "      <td>3416.557603</td>\n",
       "      <td>...</td>\n",
       "      <td>137.031903</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.057867</td>\n",
       "      <td>3.124314</td>\n",
       "      <td>36.205320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.83</td>\n",
       "      <td>160.947217</td>\n",
       "      <td>269.667774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tang_2019_DideoxyC</th>\n",
       "      <td>211.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>253.494297</td>\n",
       "      <td>103.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>2963.0</td>\n",
       "      <td>5062.0</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>2398.199373</td>\n",
       "      <td>...</td>\n",
       "      <td>59.519699</td>\n",
       "      <td>53.683231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>67.057867</td>\n",
       "      <td>3.124314</td>\n",
       "      <td>34.619300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.37</td>\n",
       "      <td>75.580531</td>\n",
       "      <td>250.166113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peters_2014_3</th>\n",
       "      <td>268.26</td>\n",
       "      <td>66.871159</td>\n",
       "      <td>369.000658</td>\n",
       "      <td>135.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>2886.0</td>\n",
       "      <td>5763.0</td>\n",
       "      <td>11275.0</td>\n",
       "      <td>5729.0</td>\n",
       "      <td>4739.692713</td>\n",
       "      <td>...</td>\n",
       "      <td>100.945467</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>32.387883</td>\n",
       "      <td>87.641582</td>\n",
       "      <td>27.044020</td>\n",
       "      <td>36.892542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.49</td>\n",
       "      <td>146.060780</td>\n",
       "      <td>284.468439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plank_2016_2</th>\n",
       "      <td>393.17</td>\n",
       "      <td>71.547747</td>\n",
       "      <td>412.608258</td>\n",
       "      <td>144.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>6760.0</td>\n",
       "      <td>13668.0</td>\n",
       "      <td>6578.0</td>\n",
       "      <td>5547.544286</td>\n",
       "      <td>...</td>\n",
       "      <td>73.852917</td>\n",
       "      <td>96.366574</td>\n",
       "      <td>32.387883</td>\n",
       "      <td>80.922082</td>\n",
       "      <td>45.054770</td>\n",
       "      <td>36.939118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>139.28</td>\n",
       "      <td>136.274624</td>\n",
       "      <td>334.202658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Du2021_L_G</th>\n",
       "      <td>283.28</td>\n",
       "      <td>71.547747</td>\n",
       "      <td>387.546658</td>\n",
       "      <td>144.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>6542.0</td>\n",
       "      <td>12768.0</td>\n",
       "      <td>6578.0</td>\n",
       "      <td>5547.544286</td>\n",
       "      <td>...</td>\n",
       "      <td>117.471961</td>\n",
       "      <td>139.049917</td>\n",
       "      <td>32.387883</td>\n",
       "      <td>80.922082</td>\n",
       "      <td>45.054770</td>\n",
       "      <td>36.939118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.51</td>\n",
       "      <td>178.957968</td>\n",
       "      <td>301.046512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        MW    D/Dtr09   ZM1MulPer    ECC   CENT    SMTI  \\\n",
       "ID                                                                        \n",
       "Ma_2019_A           267.28  66.871159  348.869542  135.0  383.0  2886.0   \n",
       "Ma_2019_U           244.23   0.000000  340.039426  118.0  295.0  2084.0   \n",
       "Ma_2019_C           243.25   0.000000  319.988367  118.0  295.0  2084.0   \n",
       "Ma_2019_G           283.28  71.547747  387.546658  144.0  446.0  3270.0   \n",
       "Ma_2019_dA          251.28  63.146800  315.599992  128.0  336.0  2584.0   \n",
       "...                    ...        ...         ...    ...    ...     ...   \n",
       "Tang_2019_ArabinoC  243.25   0.000000  319.988367  118.0  295.0  2084.0   \n",
       "Tang_2019_DideoxyC  211.25   0.000000  253.494297  103.0  213.0  1585.0   \n",
       "Peters_2014_3       268.26  66.871159  369.000658  135.0  383.0  2886.0   \n",
       "Plank_2016_2        393.17  71.547747  412.608258  144.0  446.0  3270.0   \n",
       "Du2021_L_G          283.28  71.547747  387.546658  144.0  446.0  3270.0   \n",
       "\n",
       "                     SMTIV    GMTIV     Wap         IDMT  ...      ATSC5s  \\\n",
       "ID                                                        ...               \n",
       "Ma_2019_A           5576.0  10547.0  5729.0  4739.692713  ...   90.763661   \n",
       "Ma_2019_U           4432.0   8734.0  2194.0  3416.557603  ...  139.839496   \n",
       "Ma_2019_C           4240.0   7972.0  2194.0  3416.557603  ...  137.031903   \n",
       "Ma_2019_G           6542.0  12768.0  6578.0  5547.544286  ...  117.471961   \n",
       "Ma_2019_dA          4822.0   8812.0  5068.0  4099.715332  ...   52.221046   \n",
       "...                    ...      ...     ...          ...  ...         ...   \n",
       "Tang_2019_ArabinoC  4240.0   7972.0  2194.0  3416.557603  ...  137.031903   \n",
       "Tang_2019_DideoxyC  2963.0   5062.0  1633.0  2398.199373  ...   59.519699   \n",
       "Peters_2014_3       5763.0  11275.0  5729.0  4739.692713  ...  100.945467   \n",
       "Plank_2016_2        6760.0  13668.0  6578.0  5547.544286  ...   73.852917   \n",
       "Du2021_L_G          6542.0  12768.0  6578.0  5547.544286  ...  117.471961   \n",
       "\n",
       "                    P_VSA_MR_3  P_VSA_ppp_ar  P_VSA_ppp_con  P_VSA_charge_2  \\\n",
       "ID                                                                            \n",
       "Ma_2019_A           139.049917     75.680233      63.202194        1.899093   \n",
       "Ma_2019_U           139.049917      0.000000      48.373245       46.279992   \n",
       "Ma_2019_C           139.049917      0.000000      67.057867        3.124314   \n",
       "Ma_2019_G           139.049917     32.387883      80.922082       45.054770   \n",
       "Ma_2019_dA           96.366574     75.680233      63.202194        1.899093   \n",
       "...                        ...           ...            ...             ...   \n",
       "Tang_2019_ArabinoC  139.049917      0.000000      67.057867        3.124314   \n",
       "Tang_2019_DideoxyC   53.683231      0.000000      67.057867        3.124314   \n",
       "Peters_2014_3       139.049917     32.387883      87.641582       27.044020   \n",
       "Plank_2016_2         96.366574     32.387883      80.922082       45.054770   \n",
       "Du2021_L_G          139.049917     32.387883      80.922082       45.054770   \n",
       "\n",
       "                    SM15_EA(ed)  T(O..Br)  TPSA(Tot)       SAdon          Vx  \n",
       "ID                                                                            \n",
       "Ma_2019_A             36.892542       0.0     139.54  160.947217  291.295681  \n",
       "Ma_2019_U             36.205320       0.0     124.78  146.060780  262.840532  \n",
       "Ma_2019_C             36.205320       0.0     130.83  160.947217  269.667774  \n",
       "Ma_2019_G             36.939118       0.0     159.51  178.957968  301.046512  \n",
       "Ma_2019_dA            36.335611       0.0     119.31  118.263874  281.544850  \n",
       "...                         ...       ...        ...         ...         ...  \n",
       "Tang_2019_ArabinoC    36.205320       0.0     130.83  160.947217  269.667774  \n",
       "Tang_2019_DideoxyC    34.619300       0.0      90.37   75.580531  250.166113  \n",
       "Peters_2014_3         36.892542       0.0     133.49  146.060780  284.468439  \n",
       "Plank_2016_2          36.939118       0.0     139.28  136.274624  334.202658  \n",
       "Du2021_L_G            36.939118       0.0     159.51  178.957968  301.046512  \n",
       "\n",
       "[71 rows x 28 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MRLEM_data=X_NAomit_data[col_data.index]\n",
    "MRLEM_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6d8bf42e-2223-4c7b-9bd2-7c2787032802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>D/Dtr09</th>\n",
       "      <th>ZM1MulPer</th>\n",
       "      <th>ECC</th>\n",
       "      <th>CENT</th>\n",
       "      <th>SMTI</th>\n",
       "      <th>SMTIV</th>\n",
       "      <th>GMTIV</th>\n",
       "      <th>Wap</th>\n",
       "      <th>IDMT</th>\n",
       "      <th>...</th>\n",
       "      <th>ATSC5s</th>\n",
       "      <th>P_VSA_MR_3</th>\n",
       "      <th>P_VSA_ppp_ar</th>\n",
       "      <th>P_VSA_ppp_con</th>\n",
       "      <th>P_VSA_charge_2</th>\n",
       "      <th>SM15_EA(ed)</th>\n",
       "      <th>T(O..Br)</th>\n",
       "      <th>TPSA(Tot)</th>\n",
       "      <th>SAdon</th>\n",
       "      <th>Vx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ma_2019_A</th>\n",
       "      <td>0.128645</td>\n",
       "      <td>0.181294</td>\n",
       "      <td>0.193207</td>\n",
       "      <td>0.036281</td>\n",
       "      <td>0.026667</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.037803</td>\n",
       "      <td>0.044265</td>\n",
       "      <td>0.024087</td>\n",
       "      <td>0.024617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176802</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.285905</td>\n",
       "      <td>0.104723</td>\n",
       "      <td>0.015761</td>\n",
       "      <td>0.446054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.472652</td>\n",
       "      <td>0.769847</td>\n",
       "      <td>0.069993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_U</th>\n",
       "      <td>0.075722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175319</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.021252</td>\n",
       "      <td>0.029633</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401921</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.384100</td>\n",
       "      <td>0.311208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.330770</td>\n",
       "      <td>0.665700</td>\n",
       "      <td>0.021569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_C</th>\n",
       "      <td>0.073472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134701</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.011819</td>\n",
       "      <td>0.018475</td>\n",
       "      <td>0.023484</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389042</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.123842</td>\n",
       "      <td>0.025930</td>\n",
       "      <td>0.311208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.388926</td>\n",
       "      <td>0.769847</td>\n",
       "      <td>0.033187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_G</th>\n",
       "      <td>0.165381</td>\n",
       "      <td>0.193972</td>\n",
       "      <td>0.271557</td>\n",
       "      <td>0.046485</td>\n",
       "      <td>0.036549</td>\n",
       "      <td>0.039910</td>\n",
       "      <td>0.051778</td>\n",
       "      <td>0.062188</td>\n",
       "      <td>0.029080</td>\n",
       "      <td>0.033111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299317</td>\n",
       "      <td>0.999046</td>\n",
       "      <td>0.122355</td>\n",
       "      <td>0.192589</td>\n",
       "      <td>0.373931</td>\n",
       "      <td>0.455193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.664616</td>\n",
       "      <td>0.895853</td>\n",
       "      <td>0.086587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ma_2019_dA</th>\n",
       "      <td>0.091909</td>\n",
       "      <td>0.171197</td>\n",
       "      <td>0.125811</td>\n",
       "      <td>0.028345</td>\n",
       "      <td>0.019294</td>\n",
       "      <td>0.023662</td>\n",
       "      <td>0.026894</td>\n",
       "      <td>0.030263</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.017889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666030</td>\n",
       "      <td>0.285905</td>\n",
       "      <td>0.104723</td>\n",
       "      <td>0.015761</td>\n",
       "      <td>0.336774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.278189</td>\n",
       "      <td>0.471230</td>\n",
       "      <td>0.053399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  MW   D/Dtr09  ZM1MulPer       ECC      CENT      SMTI  \\\n",
       "ID                                                                        \n",
       "Ma_2019_A   0.128645  0.181294   0.193207  0.036281  0.026667  0.030815   \n",
       "Ma_2019_U   0.075722  0.000000   0.175319  0.017007  0.012863  0.011819   \n",
       "Ma_2019_C   0.073472  0.000000   0.134701  0.017007  0.012863  0.011819   \n",
       "Ma_2019_G   0.165381  0.193972   0.271557  0.046485  0.036549  0.039910   \n",
       "Ma_2019_dA  0.091909  0.171197   0.125811  0.028345  0.019294  0.023662   \n",
       "\n",
       "               SMTIV     GMTIV       Wap      IDMT  ...    ATSC5s  P_VSA_MR_3  \\\n",
       "ID                                                  ...                         \n",
       "Ma_2019_A   0.037803  0.044265  0.024087  0.024617  ...  0.176802    0.999046   \n",
       "Ma_2019_U   0.021252  0.029633  0.003299  0.010706  ...  0.401921    0.999046   \n",
       "Ma_2019_C   0.018475  0.023484  0.003299  0.010706  ...  0.389042    0.999046   \n",
       "Ma_2019_G   0.051778  0.062188  0.029080  0.033111  ...  0.299317    0.999046   \n",
       "Ma_2019_dA  0.026894  0.030263  0.020200  0.017889  ...  0.000000    0.666030   \n",
       "\n",
       "            P_VSA_ppp_ar  P_VSA_ppp_con  P_VSA_charge_2  SM15_EA(ed)  \\\n",
       "ID                                                                     \n",
       "Ma_2019_A       0.285905       0.104723        0.015761     0.446054   \n",
       "Ma_2019_U       0.000000       0.031193        0.384100     0.311208   \n",
       "Ma_2019_C       0.000000       0.123842        0.025930     0.311208   \n",
       "Ma_2019_G       0.122355       0.192589        0.373931     0.455193   \n",
       "Ma_2019_dA      0.285905       0.104723        0.015761     0.336774   \n",
       "\n",
       "            T(O..Br)  TPSA(Tot)     SAdon        Vx  \n",
       "ID                                                   \n",
       "Ma_2019_A        0.0   0.472652  0.769847  0.069993  \n",
       "Ma_2019_U        0.0   0.330770  0.665700  0.021569  \n",
       "Ma_2019_C        0.0   0.388926  0.769847  0.033187  \n",
       "Ma_2019_G        0.0   0.664616  0.895853  0.086587  \n",
       "Ma_2019_dA       0.0   0.278189  0.471230  0.053399  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scale data\n",
    "Scaler = preprocessing.MinMaxScaler() #StandardScaler\n",
    "Transformer =Scaler.fit(MRLEM_data)\n",
    "X_scaled_data=Transformer.transform(MRLEM_data)\n",
    "X_scaled_data =pd.DataFrame(X_scaled_data)\n",
    "X_scaled_data.columns=MRLEM_data.columns\n",
    "X_scaled_data.index=Raw_data.index\n",
    "X_scaled_data.to_csv(\"./Original data/MRLEM_data_X_scaled_data.csv\",sep=',',header=1,index=1)\n",
    "joblib.dump(Transformer, './Models/MRLEM_data_Scaler_transformer.pkl')\n",
    "\n",
    "X_scaled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c5eb8b75-76bc-4fcb-aad5-6762048df1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model_results(Model_clf,X_test,y,Cv_model):\n",
    "    Model_scores= cross_validate(estimator=Model_clf, X=X_test, y=y, cv=Cv_model,scoring=( 'accuracy','f1','precision','recall','roc_auc'), return_train_score=True)\n",
    "    Model_score= cross_validate(estimator=Model_clf, X=X_test, y=y, cv=Cv_model,scoring=( 'accuracy','f1','precision','recall','roc_auc'), return_train_score=False)\n",
    "#Accuracy\n",
    "    Model_Accuracy_test_mean=Model_scores['test_accuracy'].mean()\n",
    "    Model_Accuracy_test_se=(Model_scores['test_accuracy'].std()/math.sqrt(len(Model_scores['test_accuracy']))) \n",
    "    Model_Accuracy_train_mean=Model_scores['train_accuracy'].mean()\n",
    "    Model_Accuracy_train_se=(Model_scores['train_accuracy'].std()/math.sqrt(len(Model_scores['train_accuracy']))) \n",
    "#f1\n",
    "    Model_f1_mean=Model_score['test_f1'].mean()\n",
    "    Model_f1_se=(Model_score['test_f1'].std()/math.sqrt(len(Model_score['test_f1']))) \n",
    "#precision\n",
    "    Model_precision_mean=Model_score['test_precision'].mean()\n",
    "    Model_precision_se=(Model_score['test_precision'].std()/math.sqrt(len(Model_score['test_precision']))) \n",
    "#recall\n",
    "    Model_recall_mean=Model_score['test_recall'].mean()\n",
    "    Model_recall_se=(Model_score['test_recall'].std()/math.sqrt(len(Model_score['test_recall']))) \n",
    "#roc_auc\n",
    "    Model_roc_auc_mean=Model_score['test_roc_auc'].mean()\n",
    "    Model_roc_auc_se=(Model_score['test_roc_auc'].std()/math.sqrt(len(Model_score['test_roc_auc']))) \n",
    "    Model = {'Mean':[Model_Accuracy_test_mean,Model_Accuracy_train_mean,Model_f1_mean,Model_precision_mean,Model_recall_mean,Model_roc_auc_mean],\n",
    "        'Se':[Model_Accuracy_test_se,Model_Accuracy_train_se,Model_f1_se,Model_precision_se,Model_recall_se,Model_roc_auc_se]}\n",
    "    Model = pd.DataFrame(Model, index=['Accuracy_test','Accuracy_train','F1 Score','Precision','Recall','Roc_auc'])\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "43024221-dcb1-4c20-a862-8ba15cc4978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the KFold \n",
    "Cv_optuna= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "Cv_model= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "Cv_RFECV= RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ef2d9106-5c7e-4daa-89d3-1c228a5ac78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data pre-processing of models\n",
    "X=np.array(X_scaled_data)\n",
    "y=Raw_data['Hydrogel-forming ability'].values\n",
    "clf=LogisticRegression(solver='liblinear',random_state=0,dual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff716df6-f48a-4890-819b-cca04c92e57c",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "96eeb954-8032-43c2-afb2-e47f675b2d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.629333</td>\n",
       "      <td>0.015032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.729981</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.697627</td>\n",
       "      <td>0.013235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.620752</td>\n",
       "      <td>0.011961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.811071</td>\n",
       "      <td>0.020893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.664796</td>\n",
       "      <td>0.020543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.629333  0.015032\n",
       "Accuracy_train  0.729981  0.004700\n",
       "F1 Score        0.697627  0.013235\n",
       "Precision       0.620752  0.011961\n",
       "Recall          0.811071  0.020893\n",
       "Roc_auc         0.664796  0.020543"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "662dc3e0-7ca9-41f6-ad45-99db640a18b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-07-16 17:40:43,976]\u001b[0m A new study created in memory with name: no-name-82a5f1bf-68ee-4d2c-9623-0b6e59581927\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,316]\u001b[0m Trial 0 finished with value: 0.6096190476190476 and parameters: {'logreg_c': 0.3177840006884068, 'l1_ratio': 0.7482920440979423, 'max_iter': 100}. Best is trial 0 with value: 0.6096190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,339]\u001b[0m Trial 1 finished with value: 0.6195238095238095 and parameters: {'logreg_c': 0.0651621545821569, 'l1_ratio': 0.23208030173540176, 'max_iter': 275}. Best is trial 1 with value: 0.6195238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,362]\u001b[0m Trial 2 finished with value: 0.570095238095238 and parameters: {'logreg_c': 0.013108749615263334, 'l1_ratio': 0.411004654338743, 'max_iter': 854}. Best is trial 1 with value: 0.6195238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,385]\u001b[0m Trial 3 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.7096232052870346, 'l1_ratio': 0.4772750629629653, 'max_iter': 1402}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,408]\u001b[0m Trial 4 finished with value: 0.574095238095238 and parameters: {'logreg_c': 0.016854407828169382, 'l1_ratio': 0.8903056927518509, 'max_iter': 152}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,434]\u001b[0m Trial 5 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 10.539137268289434, 'l1_ratio': 0.4755743221304143, 'max_iter': 1162}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,459]\u001b[0m Trial 6 finished with value: 0.5672380952380952 and parameters: {'logreg_c': 0.006955392321661603, 'l1_ratio': 0.2782913401763909, 'max_iter': 1622}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,484]\u001b[0m Trial 7 finished with value: 0.6282857142857143 and parameters: {'logreg_c': 645.0144652189372, 'l1_ratio': 0.38208176034331853, 'max_iter': 1416}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,509]\u001b[0m Trial 8 finished with value: 0.6093333333333334 and parameters: {'logreg_c': 181.27374779133626, 'l1_ratio': 0.9051459971534626, 'max_iter': 261}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,532]\u001b[0m Trial 9 finished with value: 0.5601904761904761 and parameters: {'logreg_c': 0.001715255021408637, 'l1_ratio': 0.25284737760811204, 'max_iter': 1769}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,559]\u001b[0m Trial 10 finished with value: 0.6153333333333333 and parameters: {'logreg_c': 9.589685409552947, 'l1_ratio': 0.6553689413187243, 'max_iter': 845}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,586]\u001b[0m Trial 11 finished with value: 0.6268571428571428 and parameters: {'logreg_c': 843.2126062012233, 'l1_ratio': 0.5380831473609496, 'max_iter': 1376}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,613]\u001b[0m Trial 12 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 7.624466107886496, 'l1_ratio': 0.378272352651409, 'max_iter': 1494}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,640]\u001b[0m Trial 13 finished with value: 0.6162857142857143 and parameters: {'logreg_c': 51.786339691986214, 'l1_ratio': 0.6316370562712422, 'max_iter': 1128}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,665]\u001b[0m Trial 14 finished with value: 0.627904761904762 and parameters: {'logreg_c': 0.8529452363532304, 'l1_ratio': 0.13738672304214128, 'max_iter': 1780}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,692]\u001b[0m Trial 15 finished with value: 0.6281904761904762 and parameters: {'logreg_c': 763.5588094994378, 'l1_ratio': 0.3689322680983461, 'max_iter': 1971}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,719]\u001b[0m Trial 16 finished with value: 0.6195238095238096 and parameters: {'logreg_c': 0.13707192890174966, 'l1_ratio': 0.5845266407991517, 'max_iter': 1309}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,747]\u001b[0m Trial 17 finished with value: 0.6120000000000001 and parameters: {'logreg_c': 76.37303329769077, 'l1_ratio': 0.7683639981317943, 'max_iter': 716}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,774]\u001b[0m Trial 18 finished with value: 0.6252380952380953 and parameters: {'logreg_c': 2.932001936719566, 'l1_ratio': 0.10084185856253824, 'max_iter': 553}. Best is trial 3 with value: 0.6294285714285714.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,801]\u001b[0m Trial 19 finished with value: 0.6307619047619046 and parameters: {'logreg_c': 1.3160447446956969, 'l1_ratio': 0.4671561613242705, 'max_iter': 1312}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,829]\u001b[0m Trial 20 finished with value: 0.6293333333333333 and parameters: {'logreg_c': 1.4651691779773963, 'l1_ratio': 0.4935134748265029, 'max_iter': 983}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,858]\u001b[0m Trial 21 finished with value: 0.6237142857142858 and parameters: {'logreg_c': 0.8287025892084571, 'l1_ratio': 0.5199862466969595, 'max_iter': 1054}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,886]\u001b[0m Trial 22 finished with value: 0.6238095238095238 and parameters: {'logreg_c': 3.1654611271505058, 'l1_ratio': 0.4597028916076701, 'max_iter': 1266}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,913]\u001b[0m Trial 23 finished with value: 0.6166666666666667 and parameters: {'logreg_c': 0.1535200845989714, 'l1_ratio': 0.6782814588832947, 'max_iter': 981}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,939]\u001b[0m Trial 24 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.8873063599506599, 'l1_ratio': 0.326102166138698, 'max_iter': 1522}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,966]\u001b[0m Trial 25 finished with value: 0.6180952380952381 and parameters: {'logreg_c': 0.41516225975438653, 'l1_ratio': 0.2979284535359352, 'max_iter': 1547}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:45,996]\u001b[0m Trial 26 finished with value: 0.6191428571428571 and parameters: {'logreg_c': 17.953162228525414, 'l1_ratio': 0.31727555023219167, 'max_iter': 1687}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,024]\u001b[0m Trial 27 finished with value: 0.6266666666666666 and parameters: {'logreg_c': 2.602409837373429, 'l1_ratio': 0.19868034857393024, 'max_iter': 1919}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,053]\u001b[0m Trial 28 finished with value: 0.6195238095238095 and parameters: {'logreg_c': 0.07229078967635784, 'l1_ratio': 0.42868388614084174, 'max_iter': 1511}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,083]\u001b[0m Trial 29 finished with value: 0.6194285714285716 and parameters: {'logreg_c': 0.3860177138367054, 'l1_ratio': 0.8158505713189557, 'max_iter': 1211}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,114]\u001b[0m Trial 30 finished with value: 0.6168571428571429 and parameters: {'logreg_c': 4.8505444157280655, 'l1_ratio': 0.989090737470458, 'max_iter': 1389}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,141]\u001b[0m Trial 31 finished with value: 0.6293333333333334 and parameters: {'logreg_c': 1.2065391735314397, 'l1_ratio': 0.4963584342131737, 'max_iter': 1011}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,168]\u001b[0m Trial 32 finished with value: 0.6162857142857143 and parameters: {'logreg_c': 28.72272514710678, 'l1_ratio': 0.581199409657952, 'max_iter': 582}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,195]\u001b[0m Trial 33 finished with value: 0.6293333333333333 and parameters: {'logreg_c': 1.3696220900607625, 'l1_ratio': 0.42558482659301816, 'max_iter': 1297}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,222]\u001b[0m Trial 34 finished with value: 0.6067619047619047 and parameters: {'logreg_c': 0.2875637249472787, 'l1_ratio': 0.3482207466346646, 'max_iter': 1109}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,250]\u001b[0m Trial 35 finished with value: 0.6009523809523809 and parameters: {'logreg_c': 0.03576221722627603, 'l1_ratio': 0.5765441371212959, 'max_iter': 870}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,278]\u001b[0m Trial 36 finished with value: 0.6209523809523809 and parameters: {'logreg_c': 0.6626791637945557, 'l1_ratio': 0.20185400717777569, 'max_iter': 1676}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,304]\u001b[0m Trial 37 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.9120072183363928, 'l1_ratio': 0.4733809188346099, 'max_iter': 1467}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,332]\u001b[0m Trial 38 finished with value: 0.6177142857142858 and parameters: {'logreg_c': 19.34210830669201, 'l1_ratio': 0.44990617999528343, 'max_iter': 1557}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,360]\u001b[0m Trial 39 finished with value: 0.6139047619047618 and parameters: {'logreg_c': 0.22059945415572177, 'l1_ratio': 0.3267785933880323, 'max_iter': 1424}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,385]\u001b[0m Trial 40 finished with value: 0.6139047619047618 and parameters: {'logreg_c': 5.702688279032213, 'l1_ratio': 0.706870700092175, 'max_iter': 1216}. Best is trial 19 with value: 0.6307619047619046.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,413]\u001b[0m Trial 41 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 2.1083445203193767, 'l1_ratio': 0.5013573618384977, 'max_iter': 1814}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,440]\u001b[0m Trial 42 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.8778046360697456, 'l1_ratio': 0.39812276354777576, 'max_iter': 1899}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,466]\u001b[0m Trial 43 finished with value: 0.6196190476190475 and parameters: {'logreg_c': 4.005595097886426, 'l1_ratio': 0.40683807642605113, 'max_iter': 1851}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,493]\u001b[0m Trial 44 finished with value: 0.6153333333333333 and parameters: {'logreg_c': 10.51995398501394, 'l1_ratio': 0.5242462748652383, 'max_iter': 1635}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,521]\u001b[0m Trial 45 finished with value: 0.6166666666666667 and parameters: {'logreg_c': 0.43649305773415953, 'l1_ratio': 0.62669435281089, 'max_iter': 1803}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,548]\u001b[0m Trial 46 finished with value: 0.6195238095238095 and parameters: {'logreg_c': 0.07187646725784044, 'l1_ratio': 0.281967448657918, 'max_iter': 1917}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,577]\u001b[0m Trial 47 finished with value: 0.627904761904762 and parameters: {'logreg_c': 2.0310237754182316, 'l1_ratio': 0.39414395429506727, 'max_iter': 1726}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,604]\u001b[0m Trial 48 finished with value: 0.620952380952381 and parameters: {'logreg_c': 0.6082079556285477, 'l1_ratio': 0.5660058818783812, 'max_iter': 1618}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,632]\u001b[0m Trial 49 finished with value: 0.6108571428571429 and parameters: {'logreg_c': 240.59569344033093, 'l1_ratio': 0.24451803344912826, 'max_iter': 1985}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,659]\u001b[0m Trial 50 finished with value: 0.5643809523809523 and parameters: {'logreg_c': 0.003166482546125131, 'l1_ratio': 0.4598078240788601, 'max_iter': 1365}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,686]\u001b[0m Trial 51 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 7.943065063340901, 'l1_ratio': 0.3551537545361903, 'max_iter': 1474}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,712]\u001b[0m Trial 52 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.8518389019522237, 'l1_ratio': 0.49224974644280434, 'max_iter': 1844}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,740]\u001b[0m Trial 53 finished with value: 0.627904761904762 and parameters: {'logreg_c': 1.3572714107165977, 'l1_ratio': 0.49277895678099476, 'max_iter': 1458}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,768]\u001b[0m Trial 54 finished with value: 0.6210476190476191 and parameters: {'logreg_c': 3.7115076962149858, 'l1_ratio': 0.6102000361609092, 'max_iter': 1590}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,797]\u001b[0m Trial 55 finished with value: 0.6264761904761904 and parameters: {'logreg_c': 0.8911994905571492, 'l1_ratio': 0.3993140649769618, 'max_iter': 1883}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,823]\u001b[0m Trial 56 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 2.2506604255090674, 'l1_ratio': 0.5420213510321465, 'max_iter': 1719}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,850]\u001b[0m Trial 57 finished with value: 0.6191428571428571 and parameters: {'logreg_c': 19.197800367683012, 'l1_ratio': 0.4652438210429772, 'max_iter': 1762}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,889]\u001b[0m Trial 58 finished with value: 0.6195238095238096 and parameters: {'logreg_c': 0.17391612699617703, 'l1_ratio': 0.5464710550797393, 'max_iter': 1365}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,917]\u001b[0m Trial 59 finished with value: 0.613904761904762 and parameters: {'logreg_c': 6.575270743844423, 'l1_ratio': 0.5466415698694522, 'max_iter': 1686}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,945]\u001b[0m Trial 60 finished with value: 0.6163809523809525 and parameters: {'logreg_c': 54.54049186437329, 'l1_ratio': 0.4325278919043865, 'max_iter': 1996}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:46,973]\u001b[0m Trial 61 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 2.096362271932597, 'l1_ratio': 0.5160078527964562, 'max_iter': 1556}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,002]\u001b[0m Trial 62 finished with value: 0.6266666666666666 and parameters: {'logreg_c': 2.538893199184801, 'l1_ratio': 0.504806699044337, 'max_iter': 1744}. Best is trial 41 with value: 0.6307619047619047.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,031]\u001b[0m Trial 63 finished with value: 0.6308571428571428 and parameters: {'logreg_c': 1.6491171164828378, 'l1_ratio': 0.37983614680640526, 'max_iter': 1831}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,056]\u001b[0m Trial 64 finished with value: 0.6207619047619048 and parameters: {'logreg_c': 0.5680946103743453, 'l1_ratio': 0.6124516385720995, 'max_iter': 1804}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,086]\u001b[0m Trial 65 finished with value: 0.6194285714285714 and parameters: {'logreg_c': 12.835933863876129, 'l1_ratio': 0.3307896654069705, 'max_iter': 1545}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,116]\u001b[0m Trial 66 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 4.284809135627299, 'l1_ratio': 0.49198373373937326, 'max_iter': 1318}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,144]\u001b[0m Trial 67 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 0.9489377554918126, 'l1_ratio': 0.37627684812340034, 'max_iter': 1225}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,173]\u001b[0m Trial 68 finished with value: 0.6292380952380953 and parameters: {'logreg_c': 1.0538708885678392, 'l1_ratio': 0.6781276320502876, 'max_iter': 1209}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,201]\u001b[0m Trial 69 finished with value: 0.6067619047619047 and parameters: {'logreg_c': 0.29155306833483985, 'l1_ratio': 0.36263117743566936, 'max_iter': 1660}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,227]\u001b[0m Trial 70 finished with value: 0.6251428571428571 and parameters: {'logreg_c': 0.8333203040302765, 'l1_ratio': 0.43197726798313846, 'max_iter': 1169}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,253]\u001b[0m Trial 71 finished with value: 0.6224761904761905 and parameters: {'logreg_c': 3.2825957984619203, 'l1_ratio': 0.52930545567372, 'max_iter': 1902}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,282]\u001b[0m Trial 72 finished with value: 0.6294285714285714 and parameters: {'logreg_c': 1.707563442346071, 'l1_ratio': 0.2692265008939825, 'max_iter': 1804}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,309]\u001b[0m Trial 73 finished with value: 0.627904761904762 and parameters: {'logreg_c': 1.244565542128993, 'l1_ratio': 0.22628313375302067, 'max_iter': 1283}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,335]\u001b[0m Trial 74 finished with value: 0.617904761904762 and parameters: {'logreg_c': 0.5423233219501331, 'l1_ratio': 0.37441966805804505, 'max_iter': 1429}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,362]\u001b[0m Trial 75 finished with value: 0.6238095238095238 and parameters: {'logreg_c': 2.6428530082211377, 'l1_ratio': 0.2853040906189891, 'max_iter': 1736}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,388]\u001b[0m Trial 76 finished with value: 0.6209523809523809 and parameters: {'logreg_c': 0.10939627711828923, 'l1_ratio': 0.4735312789321377, 'max_iter': 1846}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,415]\u001b[0m Trial 77 finished with value: 0.6264761904761904 and parameters: {'logreg_c': 0.8745518604877525, 'l1_ratio': 0.442590140651025, 'max_iter': 1078}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,442]\u001b[0m Trial 78 finished with value: 0.6154285714285713 and parameters: {'logreg_c': 5.330321809025692, 'l1_ratio': 0.4137370416803211, 'max_iter': 1943}. Best is trial 63 with value: 0.6308571428571428.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,467]\u001b[0m Trial 79 finished with value: 0.6321904761904762 and parameters: {'logreg_c': 1.5684558195868687, 'l1_ratio': 0.51063148634346, 'max_iter': 1863}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,494]\u001b[0m Trial 80 finished with value: 0.6194285714285716 and parameters: {'logreg_c': 0.37976816840820854, 'l1_ratio': 0.34025799616210806, 'max_iter': 1579}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,521]\u001b[0m Trial 81 finished with value: 0.6308571428571428 and parameters: {'logreg_c': 1.5835184104709785, 'l1_ratio': 0.5016015552438089, 'max_iter': 1871}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,549]\u001b[0m Trial 82 finished with value: 0.6308571428571428 and parameters: {'logreg_c': 1.6594874201724938, 'l1_ratio': 0.3063290111973975, 'max_iter': 1960}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,576]\u001b[0m Trial 83 finished with value: 0.6307619047619046 and parameters: {'logreg_c': 1.343979148614231, 'l1_ratio': 0.5176219420701993, 'max_iter': 1967}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,604]\u001b[0m Trial 84 finished with value: 0.6223809523809524 and parameters: {'logreg_c': 0.7187697779331849, 'l1_ratio': 0.5672396458148798, 'max_iter': 1939}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,630]\u001b[0m Trial 85 finished with value: 0.6293333333333334 and parameters: {'logreg_c': 1.2029830464414577, 'l1_ratio': 0.5185295899090988, 'max_iter': 1859}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,656]\u001b[0m Trial 86 finished with value: 0.6266666666666666 and parameters: {'logreg_c': 2.9061910777908393, 'l1_ratio': 0.3132992841051993, 'max_iter': 1949}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,682]\u001b[0m Trial 87 finished with value: 0.6096190476190476 and parameters: {'logreg_c': 0.2725593715364528, 'l1_ratio': 0.3860887328318272, 'max_iter': 1796}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,710]\u001b[0m Trial 88 finished with value: 0.6165714285714287 and parameters: {'logreg_c': 0.4912865251268892, 'l1_ratio': 0.5945013797755109, 'max_iter': 2000}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,736]\u001b[0m Trial 89 finished with value: 0.6153333333333333 and parameters: {'logreg_c': 8.240833064906228, 'l1_ratio': 0.5139563256443173, 'max_iter': 162}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,763]\u001b[0m Trial 90 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 1.4731369609639198, 'l1_ratio': 0.4554672724433561, 'max_iter': 1883}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,791]\u001b[0m Trial 91 finished with value: 0.627904761904762 and parameters: {'logreg_c': 1.2362306827283187, 'l1_ratio': 0.45224194854848165, 'max_iter': 1869}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,818]\u001b[0m Trial 92 finished with value: 0.628 and parameters: {'logreg_c': 1.8218108234980488, 'l1_ratio': 0.47339867982647943, 'max_iter': 1823}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,843]\u001b[0m Trial 93 finished with value: 0.6167619047619047 and parameters: {'logreg_c': 4.271962308570505, 'l1_ratio': 0.5609556013046808, 'max_iter': 1914}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,870]\u001b[0m Trial 94 finished with value: 0.6307619047619047 and parameters: {'logreg_c': 0.9774627856036887, 'l1_ratio': 0.42302130014987804, 'max_iter': 1700}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,896]\u001b[0m Trial 95 finished with value: 0.6222857142857143 and parameters: {'logreg_c': 0.8082919920657609, 'l1_ratio': 0.41564559443826843, 'max_iter': 1765}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,923]\u001b[0m Trial 96 finished with value: 0.6308571428571428 and parameters: {'logreg_c': 1.576347619069216, 'l1_ratio': 0.37665937232889934, 'max_iter': 1705}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,949]\u001b[0m Trial 97 finished with value: 0.6252380952380953 and parameters: {'logreg_c': 3.119735317205072, 'l1_ratio': 0.3724348739191003, 'max_iter': 1701}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:47,978]\u001b[0m Trial 98 finished with value: 0.613904761904762 and parameters: {'logreg_c': 6.204154164274675, 'l1_ratio': 0.3480031298152046, 'max_iter': 1631}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:48,006]\u001b[0m Trial 99 finished with value: 0.6195238095238096 and parameters: {'logreg_c': 0.6240590821298014, 'l1_ratio': 0.39483273607916664, 'max_iter': 1765}. Best is trial 79 with value: 0.6321904761904762.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    logreg_c = trial.suggest_float(\"logreg_c\", 1e-3,  1e3, log=True)\n",
    "    l1_ratio = trial.suggest_float(\"l1_ratio\",0.1,1,log=False) \n",
    "    #penalty = trial.suggest_categorical(\"penalty\",['l1','l2'])\n",
    "    max_iter = trial.suggest_int(\"max_iter\", 100,2000)\n",
    "    model =LogisticRegression(C=logreg_c,\n",
    "                              max_iter=max_iter,\n",
    "                              l1_ratio=l1_ratio,\n",
    "                              solver='liblinear',random_state=1)\n",
    "    \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=1))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "36a76651-b007-4715-a58f-8776deab4b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'logreg_c': 1.5684558195868687, 'l1_ratio': 0.51063148634346, 'max_iter': 1863}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=LogisticRegression(C=study.best_params['logreg_c'],\n",
    "                              max_iter=study.best_params['max_iter'],\n",
    "                              l1_ratio=study.best_params['l1_ratio'],\n",
    "                              solver='liblinear',\n",
    "                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b3ae0d1b-badc-4865-802b-396e3ee1b6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.632190</td>\n",
       "      <td>0.014648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.735909</td>\n",
       "      <td>0.004541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.694496</td>\n",
       "      <td>0.013341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.627650</td>\n",
       "      <td>0.012121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.794643</td>\n",
       "      <td>0.021058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.673325</td>\n",
       "      <td>0.020325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.632190  0.014648\n",
       "Accuracy_train  0.735909  0.004541\n",
       "F1 Score        0.694496  0.013341\n",
       "Precision       0.627650  0.012121\n",
       "Recall          0.794643  0.021058\n",
       "Roc_auc         0.673325  0.020325"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model3\n",
    "Model3=Model_results(clf,X,y,Cv_model)\n",
    "Model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892cd0c1-3914-4d08-a63a-550ffd5f60fb",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "570a3fc5-5ce1-4bd8-a18a-b9209126a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=RandomForestClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4448a0ec-3f59-45ad-be0b-3713b3c0072b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.676381</td>\n",
       "      <td>0.014691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.711877</td>\n",
       "      <td>0.013423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.688878</td>\n",
       "      <td>0.014746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.757857</td>\n",
       "      <td>0.020561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.756424</td>\n",
       "      <td>0.017012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.676381  0.014691\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.711877  0.013423\n",
       "Precision       0.688878  0.014746\n",
       "Recall          0.757857  0.020561\n",
       "Roc_auc         0.756424  0.017012"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d0f9ce84-3537-484b-9862-bf7cefca94b8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-07-16 17:40:54,177]\u001b[0m A new study created in memory with name: no-name-13ca66c2-302a-456e-9d41-3aab2206d144\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:57,313]\u001b[0m Trial 0 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 594, 'max_depth': 16, 'max_features': 20, 'min_impurity_decrease': 2.724415914984484}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:40:59,869]\u001b[0m Trial 1 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 481, 'max_depth': 15, 'max_features': 16, 'min_impurity_decrease': 4.4588650039103985}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:04,828]\u001b[0m Trial 2 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 968, 'max_depth': 11, 'max_features': 25, 'min_impurity_decrease': 2.644474598764522}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:07,925]\u001b[0m Trial 3 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 611, 'max_depth': 19, 'max_features': 6, 'min_impurity_decrease': 0.43564649850770354}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:08,562]\u001b[0m Trial 4 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 118, 'max_depth': 18, 'max_features': 25, 'min_impurity_decrease': 4.3500607412340955}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:13,530]\u001b[0m Trial 5 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 981, 'max_depth': 17, 'max_features': 16, 'min_impurity_decrease': 3.902645881432277}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:14,672]\u001b[0m Trial 6 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 206, 'max_depth': 15, 'max_features': 8, 'min_impurity_decrease': 4.7233445852479194}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:17,597]\u001b[0m Trial 7 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 570, 'max_depth': 11, 'max_features': 11, 'min_impurity_decrease': 3.871168447171083}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:20,220]\u001b[0m Trial 8 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 510, 'max_depth': 14, 'max_features': 5, 'min_impurity_decrease': 3.0881774853793855}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:23,605]\u001b[0m Trial 9 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 651, 'max_depth': 14, 'max_features': 29, 'min_impurity_decrease': 3.409101495517417}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:27,622]\u001b[0m Trial 10 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 790, 'max_depth': 7, 'max_features': 21, 'min_impurity_decrease': 1.5046577569438309}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:29,638]\u001b[0m Trial 11 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 397, 'max_depth': 20, 'max_features': 16, 'min_impurity_decrease': 1.9887909510549393}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:31,519]\u001b[0m Trial 12 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 353, 'max_depth': 16, 'max_features': 20, 'min_impurity_decrease': 0.7029374955194359}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:33,638]\u001b[0m Trial 13 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 411, 'max_depth': 12, 'max_features': 12, 'min_impurity_decrease': 4.931175650908394}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:37,437]\u001b[0m Trial 14 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 743, 'max_depth': 7, 'max_features': 19, 'min_impurity_decrease': 1.5253121430442067}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:41,288]\u001b[0m Trial 15 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 757, 'max_depth': 13, 'max_features': 12, 'min_impurity_decrease': 2.668111844473928}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:42,835]\u001b[0m Trial 16 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 286, 'max_depth': 9, 'max_features': 24, 'min_impurity_decrease': 3.4814210246729473}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:47,177]\u001b[0m Trial 17 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 807, 'max_depth': 5, 'max_features': 19, 'min_impurity_decrease': 1.4307627258174422}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:50,951]\u001b[0m Trial 18 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 714, 'max_depth': 13, 'max_features': 12, 'min_impurity_decrease': 2.574069984147719}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:52,482]\u001b[0m Trial 19 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 291, 'max_depth': 8, 'max_features': 24, 'min_impurity_decrease': 3.3144569973486893}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:41:56,965]\u001b[0m Trial 20 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 871, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 1.0528796896599464}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:00,407]\u001b[0m Trial 21 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 670, 'max_depth': 10, 'max_features': 14, 'min_impurity_decrease': 2.1272690676619246}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:03,044]\u001b[0m Trial 22 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 502, 'max_depth': 9, 'max_features': 23, 'min_impurity_decrease': 3.0212243946132653}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:07,534]\u001b[0m Trial 23 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 871, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.8924229302747374}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:11,034]\u001b[0m Trial 24 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 673, 'max_depth': 11, 'max_features': 28, 'min_impurity_decrease': 2.183958261339378}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:13,907]\u001b[0m Trial 25 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 548, 'max_depth': 9, 'max_features': 22, 'min_impurity_decrease': 2.0762642939740097}. Best is trial 0 with value: 0.5352380952380952.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:18,868]\u001b[0m Trial 26 finished with value: 0.6323809523809524 and parameters: {'n_estimators': 859, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.05016303754931206}. Best is trial 26 with value: 0.6323809523809524.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:24,368]\u001b[0m Trial 27 finished with value: 0.672 and parameters: {'n_estimators': 889, 'max_depth': 6, 'max_features': 27, 'min_impurity_decrease': 0.008573433661244079}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:29,761]\u001b[0m Trial 28 finished with value: 0.6322857142857143 and parameters: {'n_estimators': 901, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.04449057244021093}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:35,135]\u001b[0m Trial 29 finished with value: 0.6254285714285714 and parameters: {'n_estimators': 918, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.06568686852538762}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:39,537]\u001b[0m Trial 30 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 855, 'max_depth': 6, 'max_features': 26, 'min_impurity_decrease': 0.3179551086113602}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:44,446]\u001b[0m Trial 31 finished with value: 0.597047619047619 and parameters: {'n_estimators': 937, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.18490386200494244}. Best is trial 27 with value: 0.672.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:49,985]\u001b[0m Trial 32 finished with value: 0.6747619047619048 and parameters: {'n_estimators': 913, 'max_depth': 8, 'max_features': 27, 'min_impurity_decrease': 0.004905793398417582}. Best is trial 32 with value: 0.6747619047619048.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:42:55,588]\u001b[0m Trial 33 finished with value: 0.6761904761904762 and parameters: {'n_estimators': 917, 'max_depth': 6, 'max_features': 30, 'min_impurity_decrease': 0.0020653183224317627}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:00,633]\u001b[0m Trial 34 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 989, 'max_depth': 5, 'max_features': 29, 'min_impurity_decrease': 0.6299600959488392}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:04,811]\u001b[0m Trial 35 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 815, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.4674586648441808}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:09,753]\u001b[0m Trial 36 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 961, 'max_depth': 6, 'max_features': 25, 'min_impurity_decrease': 1.1357603095945366}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:14,095]\u001b[0m Trial 37 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 840, 'max_depth': 8, 'max_features': 26, 'min_impurity_decrease': 0.4114799952729883}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:19,257]\u001b[0m Trial 38 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 998, 'max_depth': 10, 'max_features': 23, 'min_impurity_decrease': 0.7115028745023322}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:24,592]\u001b[0m Trial 39 finished with value: 0.6297142857142857 and parameters: {'n_estimators': 903, 'max_depth': 6, 'max_features': 28, 'min_impurity_decrease': 0.06208220513260173}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:28,485]\u001b[0m Trial 40 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 758, 'max_depth': 5, 'max_features': 25, 'min_impurity_decrease': 1.2530333907876288}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:33,854]\u001b[0m Trial 41 finished with value: 0.6322857142857143 and parameters: {'n_estimators': 907, 'max_depth': 8, 'max_features': 27, 'min_impurity_decrease': 0.05286677087361633}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:38,655]\u001b[0m Trial 42 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 942, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.5113387488083093}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:43,322]\u001b[0m Trial 43 finished with value: 0.5421904761904762 and parameters: {'n_estimators': 888, 'max_depth': 10, 'max_features': 28, 'min_impurity_decrease': 0.22488874368369016}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:47,579]\u001b[0m Trial 44 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 829, 'max_depth': 6, 'max_features': 26, 'min_impurity_decrease': 0.9284394996974525}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:53,283]\u001b[0m Trial 45 finished with value: 0.6718095238095237 and parameters: {'n_estimators': 948, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.0222031983719142}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:43:58,163]\u001b[0m Trial 46 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 949, 'max_depth': 9, 'max_features': 29, 'min_impurity_decrease': 0.3420653743587697}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:02,127]\u001b[0m Trial 47 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 785, 'max_depth': 7, 'max_features': 9, 'min_impurity_decrease': 0.6816047837540026}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:05,387]\u001b[0m Trial 48 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 622, 'max_depth': 6, 'max_features': 24, 'min_impurity_decrease': 0.8511767213202808}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:05,977]\u001b[0m Trial 49 finished with value: 0.5309523809523808 and parameters: {'n_estimators': 107, 'max_depth': 10, 'max_features': 21, 'min_impurity_decrease': 0.26320721389361473}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:09,667]\u001b[0m Trial 50 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 708, 'max_depth': 12, 'max_features': 28, 'min_impurity_decrease': 1.8082382735708258}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:15,452]\u001b[0m Trial 51 finished with value: 0.676095238095238 and parameters: {'n_estimators': 962, 'max_depth': 7, 'max_features': 27, 'min_impurity_decrease': 0.02106459317315383}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:20,313]\u001b[0m Trial 52 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 955, 'max_depth': 7, 'max_features': 26, 'min_impurity_decrease': 0.49937746742507}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:25,489]\u001b[0m Trial 53 finished with value: 0.5323809523809523 and parameters: {'n_estimators': 989, 'max_depth': 5, 'max_features': 30, 'min_impurity_decrease': 0.24933984959020736}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:30,692]\u001b[0m Trial 54 finished with value: 0.671904761904762 and parameters: {'n_estimators': 850, 'max_depth': 8, 'max_features': 29, 'min_impurity_decrease': 0.00020460322913867096}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:35,539]\u001b[0m Trial 55 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 926, 'max_depth': 8, 'max_features': 29, 'min_impurity_decrease': 0.5829495913183985}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:39,659]\u001b[0m Trial 56 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 807, 'max_depth': 9, 'max_features': 25, 'min_impurity_decrease': 4.063447563238093}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:45,036]\u001b[0m Trial 57 finished with value: 0.6761904761904762 and parameters: {'n_estimators': 880, 'max_depth': 11, 'max_features': 28, 'min_impurity_decrease': 0.007051219363487071}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:45,946]\u001b[0m Trial 58 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 167, 'max_depth': 11, 'max_features': 29, 'min_impurity_decrease': 0.798147610103549}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:49,921]\u001b[0m Trial 59 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 770, 'max_depth': 18, 'max_features': 30, 'min_impurity_decrease': 0.382341636539112}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:54,285]\u001b[0m Trial 60 finished with value: 0.5309523809523808 and parameters: {'n_estimators': 843, 'max_depth': 16, 'max_features': 17, 'min_impurity_decrease': 0.2654469511283595}. Best is trial 33 with value: 0.6761904761904762.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:44:59,599]\u001b[0m Trial 61 finished with value: 0.6776190476190476 and parameters: {'n_estimators': 879, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.008123194334826785}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:04,289]\u001b[0m Trial 62 finished with value: 0.5956190476190477 and parameters: {'n_estimators': 876, 'max_depth': 12, 'max_features': 28, 'min_impurity_decrease': 0.18049512848250882}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:08,893]\u001b[0m Trial 63 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 889, 'max_depth': 8, 'max_features': 26, 'min_impurity_decrease': 0.5086713670369812}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:12,758]\u001b[0m Trial 64 finished with value: 0.5983809523809525 and parameters: {'n_estimators': 736, 'max_depth': 14, 'max_features': 23, 'min_impurity_decrease': 0.17773926474715257}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:17,769]\u001b[0m Trial 65 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 969, 'max_depth': 10, 'max_features': 29, 'min_impurity_decrease': 0.33631758132430556}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:22,805]\u001b[0m Trial 66 finished with value: 0.6165714285714285 and parameters: {'n_estimators': 925, 'max_depth': 11, 'max_features': 27, 'min_impurity_decrease': 0.1425798717417805}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:27,280]\u001b[0m Trial 67 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 862, 'max_depth': 6, 'max_features': 24, 'min_impurity_decrease': 2.868193262665083}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:31,564]\u001b[0m Trial 68 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 797, 'max_depth': 9, 'max_features': 25, 'min_impurity_decrease': 0.994325851568861}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:36,567]\u001b[0m Trial 69 finished with value: 0.6535238095238095 and parameters: {'n_estimators': 828, 'max_depth': 7, 'max_features': 30, 'min_impurity_decrease': 0.031484934359399815}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:41,553]\u001b[0m Trial 70 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 974, 'max_depth': 8, 'max_features': 15, 'min_impurity_decrease': 0.634550934357082}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:46,399]\u001b[0m Trial 71 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 931, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.38649297888281287}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:51,898]\u001b[0m Trial 72 finished with value: 0.6577142857142857 and parameters: {'n_estimators': 899, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.030118637469662382}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:45:57,861]\u001b[0m Trial 73 finished with value: 0.6464761904761906 and parameters: {'n_estimators': 997, 'max_depth': 8, 'max_features': 27, 'min_impurity_decrease': 0.03301911564319314}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:02,468]\u001b[0m Trial 74 finished with value: 0.5832380952380952 and parameters: {'n_estimators': 869, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.19804900719497173}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:08,343]\u001b[0m Trial 75 finished with value: 0.6733333333333333 and parameters: {'n_estimators': 960, 'max_depth': 11, 'max_features': 27, 'min_impurity_decrease': 0.0033010724541531968}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:13,082]\u001b[0m Trial 76 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 913, 'max_depth': 11, 'max_features': 26, 'min_impurity_decrease': 0.42939915296424785}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:17,580]\u001b[0m Trial 77 finished with value: 0.5941904761904763 and parameters: {'n_estimators': 841, 'max_depth': 13, 'max_features': 27, 'min_impurity_decrease': 0.16676348958431253}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:19,935]\u001b[0m Trial 78 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 457, 'max_depth': 6, 'max_features': 29, 'min_impurity_decrease': 0.7933694921818996}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:24,947]\u001b[0m Trial 79 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 965, 'max_depth': 12, 'max_features': 26, 'min_impurity_decrease': 0.5862286818332793}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:29,590]\u001b[0m Trial 80 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 894, 'max_depth': 10, 'max_features': 30, 'min_impurity_decrease': 0.3144294030115981}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:35,320]\u001b[0m Trial 81 finished with value: 0.6733333333333335 and parameters: {'n_estimators': 934, 'max_depth': 10, 'max_features': 28, 'min_impurity_decrease': 0.006323452295319941}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:40,428]\u001b[0m Trial 82 finished with value: 0.6165714285714285 and parameters: {'n_estimators': 927, 'max_depth': 10, 'max_features': 28, 'min_impurity_decrease': 0.14545468932474404}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:45,518]\u001b[0m Trial 83 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 975, 'max_depth': 5, 'max_features': 27, 'min_impurity_decrease': 0.33087900745123566}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:50,243]\u001b[0m Trial 84 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 874, 'max_depth': 10, 'max_features': 28, 'min_impurity_decrease': 2.387601930957447}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:46:56,151]\u001b[0m Trial 85 finished with value: 0.6748571428571429 and parameters: {'n_estimators': 952, 'max_depth': 11, 'max_features': 25, 'min_impurity_decrease': 0.0007392099609671288}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:00,990]\u001b[0m Trial 86 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 947, 'max_depth': 11, 'max_features': 26, 'min_impurity_decrease': 0.48146683809049023}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:06,090]\u001b[0m Trial 87 finished with value: 0.6207619047619047 and parameters: {'n_estimators': 915, 'max_depth': 13, 'max_features': 24, 'min_impurity_decrease': 0.13757031221334323}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:11,224]\u001b[0m Trial 88 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 998, 'max_depth': 12, 'max_features': 25, 'min_impurity_decrease': 0.30676748165353357}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:16,153]\u001b[0m Trial 89 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 956, 'max_depth': 11, 'max_features': 27, 'min_impurity_decrease': 4.576798205075603}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:17,734]\u001b[0m Trial 90 finished with value: 0.620952380952381 and parameters: {'n_estimators': 289, 'max_depth': 12, 'max_features': 7, 'min_impurity_decrease': 0.1288307028354426}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:22,940]\u001b[0m Trial 91 finished with value: 0.6761904761904762 and parameters: {'n_estimators': 852, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.007725264887275658}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:27,531]\u001b[0m Trial 92 finished with value: 0.5295238095238095 and parameters: {'n_estimators': 889, 'max_depth': 11, 'max_features': 30, 'min_impurity_decrease': 0.2520599220602134}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:32,507]\u001b[0m Trial 93 finished with value: 0.6732380952380952 and parameters: {'n_estimators': 822, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.02259369678524981}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:36,891]\u001b[0m Trial 94 finished with value: 0.617904761904762 and parameters: {'n_estimators': 812, 'max_depth': 7, 'max_features': 29, 'min_impurity_decrease': 0.14831488771027054}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:42,605]\u001b[0m Trial 95 finished with value: 0.6761904761904762 and parameters: {'n_estimators': 940, 'max_depth': 7, 'max_features': 28, 'min_impurity_decrease': 0.005823654890190223}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:47,291]\u001b[0m Trial 96 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 938, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.4235083475944678}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:52,253]\u001b[0m Trial 97 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 978, 'max_depth': 10, 'max_features': 27, 'min_impurity_decrease': 0.5367328068166792}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:47:57,816]\u001b[0m Trial 98 finished with value: 0.6733333333333335 and parameters: {'n_estimators': 919, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.0005874942435908203}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:48:02,466]\u001b[0m Trial 99 finished with value: 0.5352380952380952 and parameters: {'n_estimators': 909, 'max_depth': 8, 'max_features': 30, 'min_impurity_decrease': 0.719689906318086}. Best is trial 61 with value: 0.6776190476190476.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\",100,1000,1) \n",
    "    max_depth = trial.suggest_int(\"max_depth\",5,20,1)\n",
    "    max_features = trial.suggest_int(\"max_features\",5,30,1)\n",
    "    min_impurity_decrease = trial.suggest_float(\"min_impurity_decrease\",0,5,log=False) \n",
    "    model = RandomForestClassifier(n_estimators = n_estimators\n",
    "              ,max_depth = max_depth\n",
    "              ,max_features = max_features\n",
    "              ,min_impurity_decrease = min_impurity_decrease\n",
    "              ,random_state=1\n",
    "              ,verbose=False\n",
    "              ,n_jobs=8)\n",
    "\n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    " \n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9b88d250-7567-4062-b7d5-0a9638d732c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'n_estimators': 879, 'max_depth': 9, 'max_features': 28, 'min_impurity_decrease': 0.008123194334826785}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=RandomForestClassifier(n_estimators = study.best_params['n_estimators']\n",
    "              ,max_depth = study.best_params['max_depth']\n",
    "              ,max_features = study.best_params['max_features']\n",
    "              ,min_impurity_decrease = study.best_params['min_impurity_decrease']\n",
    "              ,random_state=0\n",
    "              ,verbose=False\n",
    "              ,n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a4281260-9ed3-4ed2-93c1-ed1888b7625f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.670571</td>\n",
       "      <td>0.013460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.974643</td>\n",
       "      <td>0.001874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.703054</td>\n",
       "      <td>0.013994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.681364</td>\n",
       "      <td>0.014131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.749643</td>\n",
       "      <td>0.022551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.752985</td>\n",
       "      <td>0.015938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.670571  0.013460\n",
       "Accuracy_train  0.974643  0.001874\n",
       "F1 Score        0.703054  0.013994\n",
       "Precision       0.681364  0.014131\n",
       "Recall          0.749643  0.022551\n",
       "Roc_auc         0.752985  0.015938"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model3\n",
    "Model3=Model_results(clf,X,y,Cv_model)\n",
    "Model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de27d5ec-8039-4cfd-8629-ad30d87ca90f",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "19aa8297-4182-4dac-8857-008a9ad45f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=xgb.XGBClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "66336653-640b-407f-8c25-80c9e29b3651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.662476</td>\n",
       "      <td>0.016918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.693483</td>\n",
       "      <td>0.017356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.671509</td>\n",
       "      <td>0.015551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.737143</td>\n",
       "      <td>0.025112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.749660</td>\n",
       "      <td>0.017686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.662476  0.016918\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.693483  0.017356\n",
       "Precision       0.671509  0.015551\n",
       "Recall          0.737143  0.025112\n",
       "Roc_auc         0.749660  0.017686"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "2184b863-0c03-4641-8e98-2b00c0fd5878",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-07-16 17:48:46,357]\u001b[0m A new study created in memory with name: no-name-29efda52-4a76-48f5-9e1c-e9e627c45501\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:47,949]\u001b[0m Trial 0 finished with value: 0.6608571428571429 and parameters: {'lambda': 0.15676677195506075, 'alpha': 0.7257005721594281, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.0801, 'n_estimators': 664}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:48,814]\u001b[0m Trial 1 finished with value: 0.6323809523809524 and parameters: {'lambda': 0.0562793204741517, 'alpha': 3.6905577292137624, 'colsample_bytree': 1.0, 'subsample': 0.6000000000000001, 'learning_rate': 0.1551, 'n_estimators': 552}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:49,949]\u001b[0m Trial 2 finished with value: 0.5408571428571428 and parameters: {'lambda': 0.18714500686240676, 'alpha': 5.039489598671215, 'colsample_bytree': 0.3, 'subsample': 0.4, 'learning_rate': 0.0001, 'n_estimators': 841}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:51,182]\u001b[0m Trial 3 finished with value: 0.6392380952380953 and parameters: {'lambda': 1.2960656597279736, 'alpha': 3.0202896401586674, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.0901, 'n_estimators': 792}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:52,085]\u001b[0m Trial 4 finished with value: 0.6553333333333334 and parameters: {'lambda': 0.0029723346443356552, 'alpha': 0.3628140404024381, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.10010000000000001, 'n_estimators': 444}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:53,846]\u001b[0m Trial 5 finished with value: 0.638 and parameters: {'lambda': 0.011434638743472197, 'alpha': 1.2500712230836255, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.7000000000000001, 'learning_rate': 0.0001, 'n_estimators': 637}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:54,887]\u001b[0m Trial 6 finished with value: 0.6510476190476191 and parameters: {'lambda': 0.2807908107885728, 'alpha': 0.2935864364395359, 'colsample_bytree': 1.0, 'subsample': 0.8, 'learning_rate': 0.07010000000000001, 'n_estimators': 465}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:55,414]\u001b[0m Trial 7 finished with value: 0.6508571428571429 and parameters: {'lambda': 0.6173405204074314, 'alpha': 0.0017414134181586202, 'colsample_bytree': 0.8, 'subsample': 0.8, 'learning_rate': 0.040100000000000004, 'n_estimators': 172}. Best is trial 0 with value: 0.6608571428571429.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:55,761]\u001b[0m Trial 8 finished with value: 0.6779047619047618 and parameters: {'lambda': 0.01826894228153233, 'alpha': 0.028499883436971588, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 147}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:56,370]\u001b[0m Trial 9 finished with value: 0.667904761904762 and parameters: {'lambda': 0.006847105576684045, 'alpha': 0.004418125737902547, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.0901, 'n_estimators': 282}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:56,566]\u001b[0m Trial 10 finished with value: 0.6451428571428572 and parameters: {'lambda': 5.790132527437195, 'alpha': 0.025043968115100592, 'colsample_bytree': 0.5, 'subsample': 0.5, 'learning_rate': 0.1901, 'n_estimators': 57}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:57,125]\u001b[0m Trial 11 finished with value: 0.6690476190476192 and parameters: {'lambda': 0.017123553109627314, 'alpha': 0.013676263870483537, 'colsample_bytree': 0.8, 'subsample': 0.5, 'learning_rate': 0.14509999999999998, 'n_estimators': 277}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:57,740]\u001b[0m Trial 12 finished with value: 0.6763809523809523 and parameters: {'lambda': 0.02491353701899208, 'alpha': 0.023063141329483616, 'colsample_bytree': 0.8, 'subsample': 0.4, 'learning_rate': 0.15009999999999998, 'n_estimators': 319}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:58,326]\u001b[0m Trial 13 finished with value: 0.662 and parameters: {'lambda': 0.04474111800996658, 'alpha': 0.038990832725213885, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.4, 'learning_rate': 0.1951, 'n_estimators': 316}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:58,609]\u001b[0m Trial 14 finished with value: 0.6509523809523808 and parameters: {'lambda': 0.0012140452982167488, 'alpha': 0.06910620324453418, 'colsample_bytree': 0.7, 'subsample': 0.6000000000000001, 'learning_rate': 0.1551, 'n_estimators': 94}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:59,121]\u001b[0m Trial 15 finished with value: 0.6680000000000001 and parameters: {'lambda': 0.027126643489253296, 'alpha': 0.0045017087887461935, 'colsample_bytree': 0.9000000000000001, 'subsample': 1.0, 'learning_rate': 0.1301, 'n_estimators': 204}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:48:59,825]\u001b[0m Trial 16 finished with value: 0.6703809523809525 and parameters: {'lambda': 0.004818440651909064, 'alpha': 0.16888877355169551, 'colsample_bytree': 0.5, 'subsample': 0.6000000000000001, 'learning_rate': 0.1751, 'n_estimators': 358}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:00,284]\u001b[0m Trial 17 finished with value: 0.6649523809523811 and parameters: {'lambda': 0.0010007385532741818, 'alpha': 0.009646273191219181, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.8, 'learning_rate': 0.1201, 'n_estimators': 181}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:00,940]\u001b[0m Trial 18 finished with value: 0.6763809523809524 and parameters: {'lambda': 0.061634227943790754, 'alpha': 0.07305295568841107, 'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.1701, 'n_estimators': 367}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:02,454]\u001b[0m Trial 19 finished with value: 0.6748571428571429 and parameters: {'lambda': 0.08189216606299816, 'alpha': 0.09137793328553549, 'colsample_bytree': 0.5, 'subsample': 0.6000000000000001, 'learning_rate': 0.1751, 'n_estimators': 930}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:03,371]\u001b[0m Trial 20 finished with value: 0.6764761904761906 and parameters: {'lambda': 1.6767154928982846, 'alpha': 0.0016483661900255071, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.1751, 'n_estimators': 429}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:04,460]\u001b[0m Trial 21 finished with value: 0.6525714285714286 and parameters: {'lambda': 7.401604059812908, 'alpha': 0.0010968075467978052, 'colsample_bytree': 0.7, 'subsample': 0.9, 'learning_rate': 0.18009999999999998, 'n_estimators': 421}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:05,610]\u001b[0m Trial 22 finished with value: 0.6694285714285715 and parameters: {'lambda': 2.0928422583512405, 'alpha': 0.0036652235601470915, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.9, 'learning_rate': 0.1701, 'n_estimators': 558}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:06,401]\u001b[0m Trial 23 finished with value: 0.6751428571428572 and parameters: {'lambda': 0.6439279076937869, 'alpha': 0.04386602055339343, 'colsample_bytree': 0.7, 'subsample': 0.7000000000000001, 'learning_rate': 0.1951, 'n_estimators': 391}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:07,373]\u001b[0m Trial 24 finished with value: 0.6764761904761905 and parameters: {'lambda': 0.3883136303907193, 'alpha': 0.010049953917114773, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.1301, 'n_estimators': 496}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:08,803]\u001b[0m Trial 25 finished with value: 0.6653333333333333 and parameters: {'lambda': 2.7802503139996473, 'alpha': 0.008337657822509066, 'colsample_bytree': 0.6000000000000001, 'subsample': 0.8, 'learning_rate': 0.1301, 'n_estimators': 656}. Best is trial 8 with value: 0.6779047619047618.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:10,311]\u001b[0m Trial 26 finished with value: 0.6790476190476192 and parameters: {'lambda': 0.5989769734564016, 'alpha': 0.001754204354028918, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.11510000000000001, 'n_estimators': 725}. Best is trial 26 with value: 0.6790476190476192.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:12,123]\u001b[0m Trial 27 finished with value: 0.6693333333333336 and parameters: {'lambda': 1.2518838419016274, 'alpha': 0.002084110757581769, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.0451, 'n_estimators': 779}. Best is trial 26 with value: 0.6790476190476192.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:14,458]\u001b[0m Trial 28 finished with value: 0.6609523809523811 and parameters: {'lambda': 4.723187635059192, 'alpha': 0.002467585771750723, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.050100000000000006, 'n_estimators': 916}. Best is trial 26 with value: 0.6790476190476192.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:15,764]\u001b[0m Trial 29 finished with value: 0.6861904761904764 and parameters: {'lambda': 0.1357530766063751, 'alpha': 0.00604912334356112, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.11510000000000001, 'n_estimators': 740}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:17,115]\u001b[0m Trial 30 finished with value: 0.6750476190476192 and parameters: {'lambda': 0.18092199214591256, 'alpha': 0.0010057216538091605, 'colsample_bytree': 0.4, 'subsample': 0.7000000000000001, 'learning_rate': 0.1101, 'n_estimators': 709}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:18,393]\u001b[0m Trial 31 finished with value: 0.6751428571428572 and parameters: {'lambda': 0.11119277519798368, 'alpha': 0.006118149756823851, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.11510000000000001, 'n_estimators': 723}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:19,752]\u001b[0m Trial 32 finished with value: 0.6821904761904765 and parameters: {'lambda': 0.6301118583785598, 'alpha': 0.01900920382279658, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.0651, 'n_estimators': 591}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:21,079]\u001b[0m Trial 33 finished with value: 0.6763809523809524 and parameters: {'lambda': 0.6999320996994477, 'alpha': 0.016156842125445648, 'colsample_bytree': 0.3, 'subsample': 0.7000000000000001, 'learning_rate': 0.0601, 'n_estimators': 588}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:23,045]\u001b[0m Trial 34 finished with value: 0.674952380952381 and parameters: {'lambda': 0.3527627967271589, 'alpha': 0.030241715405447074, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.0751, 'n_estimators': 991}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:24,935]\u001b[0m Trial 35 finished with value: 0.6609523809523811 and parameters: {'lambda': 0.14364941444825624, 'alpha': 0.017627889213148507, 'colsample_bytree': 0.3, 'subsample': 0.6000000000000001, 'learning_rate': 0.0201, 'n_estimators': 836}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:26,195]\u001b[0m Trial 36 finished with value: 0.6680000000000001 and parameters: {'lambda': 1.0360787933743876, 'alpha': 0.006610100039382611, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.10010000000000001, 'n_estimators': 590}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:27,225]\u001b[0m Trial 37 finished with value: 0.5927619047619048 and parameters: {'lambda': 0.24008342239429287, 'alpha': 8.861629685772453, 'colsample_bytree': 0.5, 'subsample': 0.7000000000000001, 'learning_rate': 0.08510000000000001, 'n_estimators': 730}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:28,888]\u001b[0m Trial 38 finished with value: 0.6707619047619049 and parameters: {'lambda': 0.037769804089962805, 'alpha': 0.17532934883981124, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.0201, 'n_estimators': 622}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:29,920]\u001b[0m Trial 39 finished with value: 0.677904761904762 and parameters: {'lambda': 0.08654988469845346, 'alpha': 0.0033305961239733462, 'colsample_bytree': 0.3, 'subsample': 0.8, 'learning_rate': 0.1401, 'n_estimators': 535}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:31,044]\u001b[0m Trial 40 finished with value: 0.667904761904762 and parameters: {'lambda': 0.4364559577785702, 'alpha': 0.002986127875870649, 'colsample_bytree': 0.3, 'subsample': 0.8, 'learning_rate': 0.1051, 'n_estimators': 529}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:32,327]\u001b[0m Trial 41 finished with value: 0.6820952380952382 and parameters: {'lambda': 0.012041246302698917, 'alpha': 0.005755556753698326, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1401, 'n_estimators': 679}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:33,630]\u001b[0m Trial 42 finished with value: 0.6821904761904761 and parameters: {'lambda': 0.00976530253589791, 'alpha': 0.004521658633426094, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1401, 'n_estimators': 692}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:35,099]\u001b[0m Trial 43 finished with value: 0.6820952380952382 and parameters: {'lambda': 0.009533938648370993, 'alpha': 0.005142380479852608, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.1201, 'n_estimators': 779}. Best is trial 29 with value: 0.6861904761904764.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:36,659]\u001b[0m Trial 44 finished with value: 0.701714285714286 and parameters: {'lambda': 0.008556178294461857, 'alpha': 0.00559859295117001, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 792}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:37,999]\u001b[0m Trial 45 finished with value: 0.7003809523809525 and parameters: {'lambda': 0.002736395942107596, 'alpha': 0.010967284413723879, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 687}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:39,560]\u001b[0m Trial 46 finished with value: 0.6960952380952382 and parameters: {'lambda': 0.0028166231869091456, 'alpha': 0.009994188044722196, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 830}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:41,229]\u001b[0m Trial 47 finished with value: 0.6947619047619049 and parameters: {'lambda': 0.0019027548784550043, 'alpha': 0.012393652417147363, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.0651, 'n_estimators': 836}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:42,823]\u001b[0m Trial 48 finished with value: 0.6960952380952381 and parameters: {'lambda': 0.0024183815673537484, 'alpha': 0.011729643522042612, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1851, 'n_estimators': 846}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:44,045]\u001b[0m Trial 49 finished with value: 0.6551428571428572 and parameters: {'lambda': 0.0019408046467350051, 'alpha': 1.0433113847404174, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1851, 'n_estimators': 854}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:45,686]\u001b[0m Trial 50 finished with value: 0.7002857142857144 and parameters: {'lambda': 0.0033591891295767654, 'alpha': 0.01382541664678391, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 874}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:47,316]\u001b[0m Trial 51 finished with value: 0.7017142857142857 and parameters: {'lambda': 0.0034229051056133657, 'alpha': 0.012871271791450633, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 866}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:49,008]\u001b[0m Trial 52 finished with value: 0.7001904761904763 and parameters: {'lambda': 0.004225220871455537, 'alpha': 0.011370294688030805, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 890}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:50,704]\u001b[0m Trial 53 finished with value: 0.6960000000000002 and parameters: {'lambda': 0.004103597114144333, 'alpha': 0.04967022533268807, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 920}. Best is trial 44 with value: 0.701714285714286.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:52,405]\u001b[0m Trial 54 finished with value: 0.7031428571428572 and parameters: {'lambda': 0.004043834319918063, 'alpha': 0.029633425764158693, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 894}. Best is trial 54 with value: 0.7031428571428572.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:54,250]\u001b[0m Trial 55 finished with value: 0.6975238095238098 and parameters: {'lambda': 0.004980224075064425, 'alpha': 0.03510787695470061, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 995}. Best is trial 54 with value: 0.7031428571428572.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:55,878]\u001b[0m Trial 56 finished with value: 0.7102857142857144 and parameters: {'lambda': 0.0013221304985698086, 'alpha': 0.02305461805888264, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 884}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:57,644]\u001b[0m Trial 57 finished with value: 0.6793333333333333 and parameters: {'lambda': 0.0012446922418356734, 'alpha': 0.022077479688830965, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 962}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:49:59,285]\u001b[0m Trial 58 finished with value: 0.6791428571428573 and parameters: {'lambda': 0.006622004577595627, 'alpha': 0.1431496104944084, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 885}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:00,820]\u001b[0m Trial 59 finished with value: 0.6751428571428572 and parameters: {'lambda': 0.0014168536815106854, 'alpha': 0.02621704841078774, 'colsample_bytree': 0.5, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 803}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:02,561]\u001b[0m Trial 60 finished with value: 0.677904761904762 and parameters: {'lambda': 0.003453061915516729, 'alpha': 0.06591221885128822, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1701, 'n_estimators': 955}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:04,200]\u001b[0m Trial 61 finished with value: 0.6932380952380953 and parameters: {'lambda': 0.006241233717411143, 'alpha': 0.008377211507259008, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 885}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:05,817]\u001b[0m Trial 62 finished with value: 0.7001904761904763 and parameters: {'lambda': 0.00199673319822298, 'alpha': 0.01284143905578283, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 875}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:07,342]\u001b[0m Trial 63 finished with value: 0.6935238095238097 and parameters: {'lambda': 0.0018104838952691495, 'alpha': 0.014581416070468362, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.18009999999999998, 'n_estimators': 804}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:08,837]\u001b[0m Trial 64 finished with value: 0.7004761904761907 and parameters: {'lambda': 0.0024756254478756406, 'alpha': 0.052568127938742146, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.14509999999999998, 'n_estimators': 764}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:10,244]\u001b[0m Trial 65 finished with value: 0.6792380952380952 and parameters: {'lambda': 0.0028689485895962183, 'alpha': 0.04629073394230276, 'colsample_bytree': 0.9000000000000001, 'subsample': 0.9, 'learning_rate': 0.1351, 'n_estimators': 765}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:11,454]\u001b[0m Trial 66 finished with value: 0.6595238095238096 and parameters: {'lambda': 0.001014578900194637, 'alpha': 0.36043897298369987, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.14509999999999998, 'n_estimators': 761}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:12,955]\u001b[0m Trial 67 finished with value: 0.6751428571428572 and parameters: {'lambda': 0.01737522874540943, 'alpha': 0.05780931059819181, 'colsample_bytree': 1.0, 'subsample': 0.9, 'learning_rate': 0.15009999999999998, 'n_estimators': 815}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:14,743]\u001b[0m Trial 68 finished with value: 0.6863809523809524 and parameters: {'lambda': 0.0015222664201042758, 'alpha': 0.11020953113272494, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1301, 'n_estimators': 936}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:16,396]\u001b[0m Trial 69 finished with value: 0.6778095238095238 and parameters: {'lambda': 0.003426341240967464, 'alpha': 0.03256146187092011, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1751, 'n_estimators': 866}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:17,993]\u001b[0m Trial 70 finished with value: 0.6820952380952382 and parameters: {'lambda': 0.005384501751833538, 'alpha': 0.022922639033061288, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.14509999999999998, 'n_estimators': 905}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:19,797]\u001b[0m Trial 71 finished with value: 0.6932380952380954 and parameters: {'lambda': 0.0038325905699101567, 'alpha': 0.02051598231771764, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 965}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:21,480]\u001b[0m Trial 72 finished with value: 0.7002857142857144 and parameters: {'lambda': 0.0024577069483295065, 'alpha': 0.007109582478627625, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 896}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:22,959]\u001b[0m Trial 73 finished with value: 0.6973333333333335 and parameters: {'lambda': 0.0024282570038449054, 'alpha': 0.08294927790166803, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 752}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:24,419]\u001b[0m Trial 74 finished with value: 0.6973333333333335 and parameters: {'lambda': 0.008296731719590096, 'alpha': 0.0073542298983549645, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1701, 'n_estimators': 787}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:25,946]\u001b[0m Trial 75 finished with value: 0.678952380952381 and parameters: {'lambda': 0.013459544298683093, 'alpha': 0.01606555197132388, 'colsample_bytree': 0.3, 'subsample': 0.5, 'learning_rate': 0.1251, 'n_estimators': 938}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:27,721]\u001b[0m Trial 76 finished with value: 0.6835238095238096 and parameters: {'lambda': 0.002432929541319386, 'alpha': 0.03867844669314933, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 905}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:28,928]\u001b[0m Trial 77 finished with value: 0.6908571428571428 and parameters: {'lambda': 0.0014775448337385007, 'alpha': 0.008379965030471833, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1901, 'n_estimators': 647}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:30,424]\u001b[0m Trial 78 finished with value: 0.6807619047619049 and parameters: {'lambda': 0.006701372418938602, 'alpha': 0.004045620666985065, 'colsample_bytree': 0.4, 'subsample': 0.9, 'learning_rate': 0.14509999999999998, 'n_estimators': 820}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:32,111]\u001b[0m Trial 79 finished with value: 0.6876190476190479 and parameters: {'lambda': 0.003127746609011728, 'alpha': 0.025715495289677936, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.18009999999999998, 'n_estimators': 858}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:33,662]\u001b[0m Trial 80 finished with value: 0.6960952380952383 and parameters: {'lambda': 0.0011720575701298413, 'alpha': 0.016807260672059614, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 797}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:35,325]\u001b[0m Trial 81 finished with value: 0.7044761904761907 and parameters: {'lambda': 0.0020203854446397642, 'alpha': 0.013357193776383159, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 877}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:37,009]\u001b[0m Trial 82 finished with value: 0.698857142857143 and parameters: {'lambda': 0.005019268124724571, 'alpha': 0.009246692653937827, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1701, 'n_estimators': 903}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:38,654]\u001b[0m Trial 83 finished with value: 0.6962857142857143 and parameters: {'lambda': 0.002471192899337615, 'alpha': 0.014195787739054177, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1351, 'n_estimators': 863}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:39,637]\u001b[0m Trial 84 finished with value: 0.6265714285714286 and parameters: {'lambda': 0.0020518609785551635, 'alpha': 2.3474089145676693, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 709}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:41,438]\u001b[0m Trial 85 finished with value: 0.6877142857142857 and parameters: {'lambda': 0.0016715687766010845, 'alpha': 0.007252154293450322, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1401, 'n_estimators': 973}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:43,113]\u001b[0m Trial 86 finished with value: 0.6807619047619048 and parameters: {'lambda': 0.004042017054172613, 'alpha': 0.019549267963618795, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.15009999999999998, 'n_estimators': 938}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:44,686]\u001b[0m Trial 87 finished with value: 0.7031428571428572 and parameters: {'lambda': 0.023473634497896897, 'alpha': 0.005636601005747783, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 848}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:46,345]\u001b[0m Trial 88 finished with value: 0.6862857142857144 and parameters: {'lambda': 0.03357853084274716, 'alpha': 0.004764047018815245, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1751, 'n_estimators': 843}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:47,912]\u001b[0m Trial 89 finished with value: 0.7031428571428572 and parameters: {'lambda': 0.00851847070190272, 'alpha': 0.010913691529586302, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 816}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:49,302]\u001b[0m Trial 90 finished with value: 0.6907619047619049 and parameters: {'lambda': 0.026298523095546197, 'alpha': 0.002337637077580732, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.18009999999999998, 'n_estimators': 767}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:50,870]\u001b[0m Trial 91 finished with value: 0.6889523809523811 and parameters: {'lambda': 0.007679646889123362, 'alpha': 0.003095610150491639, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 817}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:52,443]\u001b[0m Trial 92 finished with value: 0.698857142857143 and parameters: {'lambda': 0.019627147648464925, 'alpha': 0.010308846754474606, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.14509999999999998, 'n_estimators': 789}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:53,905]\u001b[0m Trial 93 finished with value: 0.6989523809523811 and parameters: {'lambda': 0.012104952676441646, 'alpha': 0.03039331547668624, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1601, 'n_estimators': 742}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:54,940]\u001b[0m Trial 94 finished with value: 0.7060000000000001 and parameters: {'lambda': 0.005758744911796952, 'alpha': 0.00592458636159379, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.1551, 'n_estimators': 489}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:56,093]\u001b[0m Trial 95 finished with value: 0.7003809523809525 and parameters: {'lambda': 0.05944768953566761, 'alpha': 0.00537721962184014, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 559}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:57,091]\u001b[0m Trial 96 finished with value: 0.6947619047619048 and parameters: {'lambda': 0.04615810486191638, 'alpha': 0.005041871944751029, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 482}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:58,053]\u001b[0m Trial 97 finished with value: 0.6821904761904762 and parameters: {'lambda': 0.009891164357898056, 'alpha': 0.0036417886687594215, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1701, 'n_estimators': 462}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:50:59,191]\u001b[0m Trial 98 finished with value: 0.6793333333333336 and parameters: {'lambda': 0.07346609827384647, 'alpha': 0.0017648041937854684, 'colsample_bytree': 0.3, 'subsample': 0.9, 'learning_rate': 0.1351, 'n_estimators': 557}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_25928\\2666013998.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
      "E:\\C++\\anaconda\\envs\\hydrogel\\lib\\site-packages\\optuna\\distributions.py:668: UserWarning: The distribution is specified by [0.0001, 0.2] and step=0.005, but the range is not divisible by `step`. It will be replaced by [0.0001, 0.1951].\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2023-07-16 17:51:00,488]\u001b[0m Trial 99 finished with value: 0.6862857142857142 and parameters: {'lambda': 0.005843712147701568, 'alpha': 0.010227730927162207, 'colsample_bytree': 0.4, 'subsample': 1.0, 'learning_rate': 0.1651, 'n_estimators': 681}. Best is trial 56 with value: 0.7102857142857144.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.3,1.0,step=0.1),\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0, step=0.1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.2, step=0.005),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\",50,1000,1)\n",
    "        #'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "    }\n",
    "    model = xgb.XGBClassifier(**param,random_state=1,n_jobs=8)\n",
    "\n",
    " \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    "\n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "61fd4c6f-d6af-4f71-8f40-73e5cb7f1a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'lambda': 0.0013221304985698086, 'alpha': 0.02305461805888264, 'colsample_bytree': 0.3, 'subsample': 1.0, 'learning_rate': 0.15009999999999998, 'n_estimators': 884}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf=xgb.XGBClassifier(alpha = study.best_params['alpha']\n",
    "              ,colsample_bytree = study.best_params['colsample_bytree']\n",
    "              ,subsample = study.best_params['subsample']\n",
    "              ,n_estimators = study.best_params['n_estimators']\n",
    "              ,learning_rate= study.best_params['learning_rate'], n_jobs=8\n",
    "              ,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4b5d7bd3-660b-42d3-9546-2633863cc8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.682000</td>\n",
       "      <td>0.015467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.710276</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.693874</td>\n",
       "      <td>0.013624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.748214</td>\n",
       "      <td>0.023279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.752117</td>\n",
       "      <td>0.018448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.682000  0.015467\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.710276  0.015000\n",
       "Precision       0.693874  0.013624\n",
       "Recall          0.748214  0.023279\n",
       "Roc_auc         0.752117  0.018448"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model3=Model_results(clf,X,y,Cv_model)\n",
    "Model3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a9588-1675-478b-9b86-42e4bd4cd2d7",
   "metadata": {},
   "source": [
    "## DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d4d701d8-2836-4f4f-b14c-6a9bff51ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=DecisionTreeClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ba3465ca-ca5f-4b89-8b35-255abaff9322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.690571</td>\n",
       "      <td>0.015385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.978164</td>\n",
       "      <td>0.001539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.711620</td>\n",
       "      <td>0.015412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.714715</td>\n",
       "      <td>0.016121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.024177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.687674</td>\n",
       "      <td>0.015133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.690571  0.015385\n",
       "Accuracy_train  0.978164  0.001539\n",
       "F1 Score        0.711620  0.015412\n",
       "Precision       0.714715  0.016121\n",
       "Recall          0.735000  0.024177\n",
       "Roc_auc         0.687674  0.015133"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model1 （4175 descriptors）\n",
    "Model1_clf=clf\n",
    "#Model1\n",
    "Model1=Model_results(Model1_clf,X,y,Cv_model)\n",
    "Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ca03e46f-0b64-4d7f-8708-bf4b1c13c24e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-07-16 17:51:09,197]\u001b[0m A new study created in memory with name: no-name-18313ac9-d62d-4820-adb6-1ca64667a923\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,225]\u001b[0m Trial 0 finished with value: 0.6312380952380953 and parameters: {'max_depth': 4, 'max_features': 17, 'min_samples_split': 16}. Best is trial 0 with value: 0.6312380952380953.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,250]\u001b[0m Trial 1 finished with value: 0.5926666666666667 and parameters: {'max_depth': 4, 'max_features': 14, 'min_samples_split': 17}. Best is trial 0 with value: 0.6312380952380953.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,272]\u001b[0m Trial 2 finished with value: 0.6375238095238095 and parameters: {'max_depth': 4, 'max_features': 19, 'min_samples_split': 25}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,296]\u001b[0m Trial 3 finished with value: 0.6208571428571428 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 14}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,318]\u001b[0m Trial 4 finished with value: 0.6296190476190476 and parameters: {'max_depth': 4, 'max_features': 20, 'min_samples_split': 3}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,341]\u001b[0m Trial 5 finished with value: 0.6120952380952381 and parameters: {'max_depth': 3, 'max_features': 10, 'min_samples_split': 21}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,366]\u001b[0m Trial 6 finished with value: 0.6318095238095238 and parameters: {'max_depth': 5, 'max_features': 19, 'min_samples_split': 25}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,389]\u001b[0m Trial 7 finished with value: 0.5998095238095238 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 20}. Best is trial 2 with value: 0.6375238095238095.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,412]\u001b[0m Trial 8 finished with value: 0.6439999999999999 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 5}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,436]\u001b[0m Trial 9 finished with value: 0.6057142857142856 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,465]\u001b[0m Trial 10 finished with value: 0.601047619047619 and parameters: {'max_depth': 3, 'max_features': 12, 'min_samples_split': 2}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,493]\u001b[0m Trial 11 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,521]\u001b[0m Trial 12 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,549]\u001b[0m Trial 13 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,577]\u001b[0m Trial 14 finished with value: 0.6280952380952382 and parameters: {'max_depth': 3, 'max_features': 13, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,606]\u001b[0m Trial 15 finished with value: 0.64 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 5}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,634]\u001b[0m Trial 16 finished with value: 0.64 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 10}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,665]\u001b[0m Trial 17 finished with value: 0.6042857142857144 and parameters: {'max_depth': 3, 'max_features': 20, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,697]\u001b[0m Trial 18 finished with value: 0.6207619047619047 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 12}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,724]\u001b[0m Trial 19 finished with value: 0.6295238095238096 and parameters: {'max_depth': 3, 'max_features': 13, 'min_samples_split': 4}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,753]\u001b[0m Trial 20 finished with value: 0.6385714285714286 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,780]\u001b[0m Trial 21 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,807]\u001b[0m Trial 22 finished with value: 0.6111428571428571 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 13}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,833]\u001b[0m Trial 23 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,858]\u001b[0m Trial 24 finished with value: 0.6253333333333333 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,884]\u001b[0m Trial 25 finished with value: 0.6405714285714285 and parameters: {'max_depth': 4, 'max_features': 16, 'min_samples_split': 10}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,912]\u001b[0m Trial 26 finished with value: 0.6097142857142858 and parameters: {'max_depth': 3, 'max_features': 14, 'min_samples_split': 4}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,938]\u001b[0m Trial 27 finished with value: 0.6129523809523809 and parameters: {'max_depth': 3, 'max_features': 15, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,964]\u001b[0m Trial 28 finished with value: 0.6222857142857143 and parameters: {'max_depth': 4, 'max_features': 18, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:09,990]\u001b[0m Trial 29 finished with value: 0.6396190476190476 and parameters: {'max_depth': 4, 'max_features': 17, 'min_samples_split': 14}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,018]\u001b[0m Trial 30 finished with value: 0.6363809523809524 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 16}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,043]\u001b[0m Trial 31 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,069]\u001b[0m Trial 32 finished with value: 0.6385714285714286 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,095]\u001b[0m Trial 33 finished with value: 0.624 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 2}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,122]\u001b[0m Trial 34 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,149]\u001b[0m Trial 35 finished with value: 0.614 and parameters: {'max_depth': 4, 'max_features': 15, 'min_samples_split': 4}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,175]\u001b[0m Trial 36 finished with value: 0.6111428571428571 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 12}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,201]\u001b[0m Trial 37 finished with value: 0.6011428571428571 and parameters: {'max_depth': 4, 'max_features': 14, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,227]\u001b[0m Trial 38 finished with value: 0.6281904761904762 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 5}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,253]\u001b[0m Trial 39 finished with value: 0.6067619047619047 and parameters: {'max_depth': 4, 'max_features': 10, 'min_samples_split': 16}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,279]\u001b[0m Trial 40 finished with value: 0.6393333333333333 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,305]\u001b[0m Trial 41 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 9}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,332]\u001b[0m Trial 42 finished with value: 0.6368571428571428 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 14}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,357]\u001b[0m Trial 43 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,385]\u001b[0m Trial 44 finished with value: 0.614 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 10}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,411]\u001b[0m Trial 45 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,436]\u001b[0m Trial 46 finished with value: 0.6056190476190477 and parameters: {'max_depth': 3, 'max_features': 20, 'min_samples_split': 5}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,463]\u001b[0m Trial 47 finished with value: 0.64 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 3}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,489]\u001b[0m Trial 48 finished with value: 0.6143809523809524 and parameters: {'max_depth': 3, 'max_features': 15, 'min_samples_split': 8}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,516]\u001b[0m Trial 49 finished with value: 0.6180952380952381 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 21}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,542]\u001b[0m Trial 50 finished with value: 0.6406666666666666 and parameters: {'max_depth': 3, 'max_features': 11, 'min_samples_split': 3}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,570]\u001b[0m Trial 51 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,597]\u001b[0m Trial 52 finished with value: 0.6411428571428571 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 11}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,623]\u001b[0m Trial 53 finished with value: 0.6371428571428572 and parameters: {'max_depth': 3, 'max_features': 16, 'min_samples_split': 12}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,652]\u001b[0m Trial 54 finished with value: 0.6425714285714286 and parameters: {'max_depth': 3, 'max_features': 17, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,678]\u001b[0m Trial 55 finished with value: 0.6211428571428571 and parameters: {'max_depth': 3, 'max_features': 18, 'min_samples_split': 6}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,704]\u001b[0m Trial 56 finished with value: 0.6375238095238095 and parameters: {'max_depth': 3, 'max_features': 19, 'min_samples_split': 24}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,731]\u001b[0m Trial 57 finished with value: 0.6422857142857145 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 7}. Best is trial 8 with value: 0.6439999999999999.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,758]\u001b[0m Trial 58 finished with value: 0.6604761904761905 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 5}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,786]\u001b[0m Trial 59 finished with value: 0.6406666666666666 and parameters: {'max_depth': 5, 'max_features': 15, 'min_samples_split': 6}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,815]\u001b[0m Trial 60 finished with value: 0.6604761904761905 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 5}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,842]\u001b[0m Trial 61 finished with value: 0.6604761904761905 and parameters: {'max_depth': 5, 'max_features': 16, 'min_samples_split': 5}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,871]\u001b[0m Trial 62 finished with value: 0.647904761904762 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 3}. Best is trial 58 with value: 0.6604761904761905.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,899]\u001b[0m Trial 63 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 63 with value: 0.6742857142857143.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,925]\u001b[0m Trial 64 finished with value: 0.6520952380952381 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 2}. Best is trial 63 with value: 0.6742857142857143.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,952]\u001b[0m Trial 65 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 63 with value: 0.6742857142857143.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:10,979]\u001b[0m Trial 66 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,006]\u001b[0m Trial 67 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,033]\u001b[0m Trial 68 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,059]\u001b[0m Trial 69 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,088]\u001b[0m Trial 70 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,116]\u001b[0m Trial 71 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,142]\u001b[0m Trial 72 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,171]\u001b[0m Trial 73 finished with value: 0.6690476190476191 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,198]\u001b[0m Trial 74 finished with value: 0.6761904761904762 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,225]\u001b[0m Trial 75 finished with value: 0.6676190476190478 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,251]\u001b[0m Trial 76 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,278]\u001b[0m Trial 77 finished with value: 0.6690476190476191 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,307]\u001b[0m Trial 78 finished with value: 0.6451428571428572 and parameters: {'max_depth': 5, 'max_features': 11, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,334]\u001b[0m Trial 79 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,360]\u001b[0m Trial 80 finished with value: 0.6676190476190478 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,387]\u001b[0m Trial 81 finished with value: 0.6742857142857143 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 2}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,414]\u001b[0m Trial 82 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,441]\u001b[0m Trial 83 finished with value: 0.647904761904762 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,468]\u001b[0m Trial 84 finished with value: 0.6478095238095238 and parameters: {'max_depth': 5, 'max_features': 11, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,495]\u001b[0m Trial 85 finished with value: 0.6759999999999999 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,522]\u001b[0m Trial 86 finished with value: 0.6676190476190478 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,549]\u001b[0m Trial 87 finished with value: 0.6337142857142858 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 18}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,577]\u001b[0m Trial 88 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,604]\u001b[0m Trial 89 finished with value: 0.647904761904762 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,632]\u001b[0m Trial 90 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,659]\u001b[0m Trial 91 finished with value: 0.6759999999999999 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,685]\u001b[0m Trial 92 finished with value: 0.6759999999999999 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,712]\u001b[0m Trial 93 finished with value: 0.6759999999999999 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,739]\u001b[0m Trial 94 finished with value: 0.6379047619047619 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,768]\u001b[0m Trial 95 finished with value: 0.6676190476190478 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 4}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,795]\u001b[0m Trial 96 finished with value: 0.68 and parameters: {'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,821]\u001b[0m Trial 97 finished with value: 0.6421904761904762 and parameters: {'max_depth': 5, 'max_features': 14, 'min_samples_split': 5}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,849]\u001b[0m Trial 98 finished with value: 0.6634285714285714 and parameters: {'max_depth': 5, 'max_features': 12, 'min_samples_split': 5}. Best is trial 66 with value: 0.68.\u001b[0m\n",
      "\u001b[32m[I 2023-07-16 17:51:11,876]\u001b[0m Trial 99 finished with value: 0.6478095238095238 and parameters: {'max_depth': 5, 'max_features': 11, 'min_samples_split': 3}. Best is trial 66 with value: 0.68.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Tuning hyperparameters\n",
    "#Step 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth',3,5,1),\n",
    "        'max_features' : trial.suggest_int(\"max_features\",10,20,1),\n",
    "        'min_samples_split':trial.suggest_int('min_samples_split',2,25,1)\n",
    "    }\n",
    "    model = DecisionTreeClassifier(**param,random_state=1)\n",
    "\n",
    " \n",
    "# Step 2: Scoring method:\n",
    "    score = model_selection.cross_val_score(model, X, y, n_jobs=8, scoring=\"accuracy\",cv=Cv_optuna)\n",
    "    accuracy = score.mean()\n",
    "    return accuracy\n",
    " \n",
    "# Step 3: Running it\n",
    "study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=0))\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ed59afed-686d-46e1-ba15-3adc82b35741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : \n",
      "{'max_depth': 5, 'max_features': 13, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "# Getting the best parameters:\n",
    "print(f\"The best parameters are : \\n{study.best_params}\")\n",
    "# Setting the best model\n",
    "clf =DecisionTreeClassifier(max_depth = study.best_params['max_depth']\n",
    "              ,max_features = study.best_params['max_features']\n",
    "              #,n_estimators = study.best_params['n_estimators']\n",
    "              #,learning_rate = study.best_params['learning_rate']\n",
    "              ,min_samples_split= study.best_params['min_samples_split']\n",
    "              ,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2446e4e5-91ab-430f-b776-81a0dd6631ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Se</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy_test</th>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.012329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy_train</th>\n",
       "      <td>0.925345</td>\n",
       "      <td>0.005584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.706865</td>\n",
       "      <td>0.012092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.698240</td>\n",
       "      <td>0.013460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.735714</td>\n",
       "      <td>0.020076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Roc_auc</th>\n",
       "      <td>0.690561</td>\n",
       "      <td>0.015281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Mean        Se\n",
       "Accuracy_test   0.680000  0.012329\n",
       "Accuracy_train  0.925345  0.005584\n",
       "F1 Score        0.706865  0.012092\n",
       "Precision       0.698240  0.013460\n",
       "Recall          0.735714  0.020076\n",
       "Roc_auc         0.690561  0.015281"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model3\n",
    "Model3=Model_results(clf,X,y,Cv_model)\n",
    "Model3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrogel",
   "language": "python",
   "name": "hydrogel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
